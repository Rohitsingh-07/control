#Step 1 

# Step 1 → Finalized Architecture Blueprint

Great — I’ll start by finalizing a complete, implementation-ready **architecture blueprint** for your RAG project on 2024 10-K filings. This includes a clear pipeline, component choices, configuration parameters tuned for long financial documents, a novelty suggestion, an ASCII flow diagram, and a compact Python-style pseudo-implementation you can drop into your repo.

---

## 1 — High-level pipeline (one-line)

**Download 10-K PDFs → parse & clean → chunk + metadata → embed & build indexes (dense + sparse) → hybrid retriever → LLM prompt templates → generator (3 LLMs) → grounded answer + provenance + evaluation.**

---

## 2 — ASCII flow diagram

```
[SEC EDGAR 10-K PDFs]
           │
           ▼
   data_ingest/ (scripts)
           │
           ▼
   parsing (pdf -> raw text)  ──> cleaning (remove headers/footers, tables→structured)
           │
           ▼
    chunking (+metadata: company, filing_date, section, page_range)
           │
           ▼
   ┌─────────────────────────────────────────────┐
   │  Indexing:                                  │
   │  - Dense embeddings -> Chroma/FAISS         │
   │  - Sparse index -> BM25 (Whoosh/Elasticsearch)│
   └─────────────────────────────────────────────┘
           │
           ▼
   hybrid retriever (sparse + dense fusion / rerank)
           │
           ▼
   prompt construction (context window selection + chain-of-thought control)
           │
           ▼
   generator: run with Llama-3-8B, Mistral-7B, Phi-3-mini (same prompt)
           │
           ▼
   post-processing: provenance extraction, numeric-checker, hallucination detector
           │
           ▼
   evaluation + logs + report outputs (CSV, figures)
```

---

## 3 — Component choices & rationale

**Data ingestion**

* Source: SEC EDGAR (FY2024 10-Ks for 10–15 Big Tech firms).
* Scripts: `ingest/edgar_downloader.py` — fetch PDFs by CIK or company name.
* Store raw PDFs in `/data/raw_pdfs/` and parsed text in `/data/parsed/`.

**Parsing & cleaning**

* Tooling: `pdfminer.six` or `pdfplumber` + custom regex for financial sections. Convert tables to CSV where possible.
* Normalize: remove multi-line headers/footers, fix encoding, normalize whitespace, preserve section titles (Item 1, Item 7, Risk Factors, MD&A).

**Chunking**

* Strategy: *semantic-aware, section-preserving chunking*.
* Parameters (recommended):

  * `max_chars = 2500` (~1,500–2,500 tokens depending on tokenizer)
  * `overlap_chars = 400`
  * Prefer splitting at logical boundaries (section headings, paragraphs, tables), *not* mid-sentence.
* Metadata per chunk: `company`, `ticker`, `filing_date`, `section`, `start_page`, `end_page`, `chunk_id`, `original_paragraph`.

**Embedding**

* Use an open-source embedding model (HuggingFace or Ollama compatible).
* Save embeddings with dims (e.g., 1024 if using advanced models; adapt to your chosen embedder).
* Persist embedding vectors in Chroma/FAISS (local) — directory `/indexes/dense/`.

**Sparse index**

* BM25 (Whoosh or Elasticsearch if available) for keyword exactness and numeric matches. Directory `/indexes/sparse/`.

**Hybrid Retriever**

* Query flow:

  1. Run BM25 to get candidates (top 50) — ensures exact numeric/phrase matches (e.g., "Net income 2024").
  2. Run vector search (dense) on query embedding — get top 50.
  3. Union candidates, rerank using a lightweight cross-encoder or heuristic (source recency, section priority).
* Reranking features: retrieval score, BM25 score, chunk length, presence of numeric tokens.

**LLM Serving**

* Generative models (three): Llama-3-8B, Mistral-7B, Phi-3-mini.
* Run locally via Ollama or use Hugging Face inference / quantized models on Colab (4-bit or 8-bit) for hardware constraints.
* Standardize inference settings across models: temperature=0.0–0.2 for factual Qs, max_tokens tuned per prompt.

**Prompting & Context Window**

* Use two-stage prompts:

  * **Retriever prompt**: short cue guiding retrieval (in retriever pipeline, if using a re-ranker).
  * **Generator prompt template**:

    1. System instruction: role, constraints (cite sources, numeric verification, answer concisely).
    2. Context: top-K chunks (with chunk metadata labels).
    3. Question.
    4. Answer format: include “Answer”, “Supporting chunks (id:score)”, “Numeric checks”.
* Keep combined token length <= model window (for 8B models ~32k if using long-context variants; otherwise 8k–16k). If limited, perform iterative retrieval + condensation.

**Post-processing & Grounding**

* Produce an answer plus explicit provenance list: chunk_id, company, section, page range, text snippet.
* Run numeric-checker: verify numbers in answer against retrieved chunks (regex match).
* Hallucination detection heuristic: flag when an asserted fact cannot be matched in any retrieved chunk.

**Evaluation**

* Prepare at least 10-20 carefully designed questions (factual, numeric, synthesis, multi-document).
* Metrics: factual accuracy (binary/graded), numerical correctness (exact or within tolerance), hallucination rate, answer completeness, citation precision, ROUGE/BERTScore for fluency (but emphasize human eval for correctness).
* For each Q: produce responses from all three models, and human-assigned labels (ground truth checked against original 10-K).

---

## 4 — Novelty / interesting angle (to score higher)

* **Hybrid retrieval + numeric-awareness**: Combine BM25 for precise numeric and phrase matching with dense retrieval for semantics. Add a small numeric re-ranker that prioritizes chunks containing numbers and tables.
* **Provenance-aware prompt tuning**: force the LLM to list exact chunk ids and quote snippets for any numeric claims. Measure whether forced provenance reduces hallucination.
* **Multi-document synthesis challenge**: create Qs that require synthesizing data across multiple companies (e.g., “Compare BigTech A and B revenue recognition policies in FY2024”), then analyze model reasoning and citation usage.

---

## 5 — Directory layout (recommended)

```
project-root/
├─ data/
│  ├─ raw_pdfs/
│  ├─ parsed/
│  └─ curated_questions.csv
├─ ingest/
│  └─ edgar_downloader.py
├─ parse/
│  └─ pdf_to_text.py
├─ chunking/
│  └─ chunker.py
├─ embeddings/
│  └─ embedder.py
├─ indexes/
│  ├─ dense/   # Chroma/FAISS
│  └─ sparse/  # BM25/Whoosh
├─ retrieval/
│  └─ hybrid_retriever.py
├─ models/
│  └─ runner.py   # unified interface to Llama/Mistral/Phi
├─ prompts/
│  └─ templates.py
├─ eval/
│  └─ evaluate.py
├─ notebooks/
│  └─ analysis.ipynb
├─ reports/
└─ README.md
```

---

## 6 — Key configuration parameters (example)

```yaml
chunker:
  max_chars: 2500
  overlap_chars: 400
  preserve_headings: true

dense_index:
  backend: chroma
  embedding_model: "open-embed-1024"  # replace with chosen model
  embedding_dim: 1024

sparse_index:
  backend: whoosh
  top_k: 50

retriever:
  bm25_topk: 50
  dense_topk: 50
  final_union_topk: 20
  reranker: "cross_encoder"  # optional

generator_defaults:
  temperature: 0.0
  max_tokens: 512
  stop_sequences: ["\n\n"]
```

---

## 7 — Example pseudo-code (core retrieval → generation)

```python
# hybrid_retriever.py (pseudo)
def hybrid_retrieve(query, bm25_index, dense_index, embedder, bm25_topk=50, dense_topk=50, final_k=5):
    bm25_hits = bm25_index.search(query, topk=bm25_topk)        # returns chunks + scores
    q_emb = embedder.embed(query)
    dense_hits = dense_index.search(q_emb, topk=dense_topk)
    candidate_ids = union_ids(bm25_hits, dense_hits)
    candidates = fetch_chunks(candidate_ids)
    reranked = rerank_candidates(candidates, query)  # optionally cross-encoder
    return reranked[:final_k]

# runner.py (pseudo)
def answer_question(query, retriever, model_runner, prompt_template, top_k_context=5):
    ctx_chunks = retriever.hybrid_retrieve(query, top_k=top_k_context)
    prompt = prompt_template.fill(context=ctx_chunks, question=query)
    response = model_runner.generate(prompt)
    provenance = extract_provenance(response, ctx_chunks)
    numeric_check = check_numbers(response, ctx_chunks)
    return {"answer": response, "provenance": provenance, "numeric_ok": numeric_check}
```

---

## 8 — Prompt template (skeleton)

```
SYSTEM: You are an expert Financial NLP assistant. When answering, only use the provided context chunks. Always include explicit provenance for any factual or numeric claim. If you cannot find evidence in the context, say "NOT FOUND" and do NOT hallucinate.

CONTEXT:
[1] (company: Apple, section: Item 8, page: 45-48)
Text: "Net income for 2024 was $X..."

[2] (company: Microsoft, section: Item 7, page: 60-62)
Text: "Revenue increased due to..."

QUESTION:
<<user question>>

RESPONSE FORMAT:
Answer: <concise answer>
Supporting chunks:
 - chunk_id: reason
Numeric checks: <list numbers and cite chunk>
If NOT FOUND -> say "NOT FOUND".
```

---

## 9 — Evaluation plan (detailed)

* **Question types** (10–20 items):

  * *Factual single-document* (e.g., “What was Company X’s net income in 2024?”)
  * *Numeric reasoning* (e.g., “Compute YoY revenue change for Company X between 2023 and 2024.”)
  * *Synthesis multi-document* (e.g., “Which companies changed revenue recognition policies?”)
  * *Ambiguity resolution* (e.g., “Does Company X disclose any litigation risk affecting FY2024?”)
* **Metrics**:

  * `Factual Accuracy` (0/1) by human check vs 10-K text.
  * `Numeric Correctness` (Exact / ± tolerance).
  * `Citation Precision` = (# correct citations / # citations provided).
  * `Hallucination Rate` = fraction of answers containing unsupported claims.
  * `Response Time` (for practical deployment analysis).
* **Experiment table**: CSV rows = (question_id, model, answer_text, provenance_list, factual_label, numeric_label, hallucination_flag, runtime_seconds).

---

## 10 — Logging, reproducibility & artifacts

* **Unit tests** for chunker (ensures headings preserved), embedder (consistent dims), retriever (top-k stability).
* **Seed control** for any randomness during retrieval / reranking.
* Save index snapshots (Chroma/FAISS) and export a small sample for reproducibility.
* Provide `requirements.txt` and a `run_demo.sh` to replicate demos.

---

## 11 — Quick implementation roadmap (next concrete tasks)

1. `ingest/edgar_downloader.py` — fetch and store PDFs.
2. `parse/pdf_to_text.py` — robust text extraction + section segmentation.
3. `chunking/chunker.py` — produce chunk JSONL with metadata.
4. `embeddings/embedder.py` — pick embedding model → embed all chunks → index in Chroma.
5. `indexes/sparse/` — build BM25 index (Whoosh).
6. `retrieval/hybrid_retriever.py` — implement union + rerank.
7. `models/runner.py` — wrapper to call Llama-3/Mistral/Phi (Ollama/HF).
8. `eval/evaluate.py` — run question set across models and produce CSV + plots.
9. Write README + demo notebook.

---

## 12 — Risks & mitigation

* **Huge documents → context overflow**: mitigate with section-preserving chunking and iterative retrieval.
* **Numeric hallucinations**: use BM25 to ensure numeric evidence is retrieved; numeric-checker to verify.
* **Compute limits**: use quantized models (4-bit/8-bit) and limit context tokens, or rely on Hugging Face free inference for experimentation.
* **Comparability across LLMs**: keep prompts and generation settings identical; record seed and temperature.

---

## 13 — Deliverables to include in repo for grading

* `report.pdf` (ACL-style)
* `slides.pdf` (9 slides for proposal + final slide deck)
* `demo_video_unlisted_link.txt`
* `code/` with clear README and usage examples
* `data_sample/` limited parsed 10-Ks and index snapshot
* `eval/` CSV results and Jupyter analysis notebook

---

## 14 — Minimal checklist for me to implement next (I can generate now)

* [ ] Example `chunker.py` implementation
* [ ] Small sample of 3 parsed 10-Ks and generated chunks (JSONL)
* [ ] Example prompt + mocked model outputs for one question
* [ ] Template `evaluate.py` to produce the CSV and basic plots

---

If you want, I can immediately:

* generate `chunker.py` (actual code) and a sample chunk JSONL, **or**
* create your 10–20 domain-specific question set (recommended next), **or**
* produce the prompt templates and a README snippet for the repo.

Which of these should I produce right away? (I can just pick one and produce it — no need to answer if you want me to start on the most valuable: **generate the 10–20 question set** and an example `chunker.py`. )
