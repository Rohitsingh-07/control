## Inference: making predictions (forward propagation)

- An algorithm which lets our neural network to make inferences or make predictions is called forward propagation. We are going to use the handwritten digit recognition example. For simplicity we are going to just distinguish between two digits 0 and 1 for now.
- It's just a binary classification problem where we're going to input an image and classify, is this the digit 0 or the digit 1?
- For this example, we are going to use an 8 x 8 image and this image of a 1 is this grid or matrix of 8 x 8 or 64 pixel intensity values where 255 denotes a bright white pixel and 0 would denote a black pixel.
- Given these 64 input features, we're going to use the neural network with two hidden layers. Where the first hidden layer has 25 neurons or 25 units. 2nd hidden layer has 15 neurons or 15 units. And then finally the output layer or the output unit, what's the chance of this being 1 versus 0?
- The first computation is to go from X to a1, and that's what the first layer of the first hidden layers. It carries out a computation of a[1]. The formula is shown in the image below. Notice that a1 has 25 numbers because the hidden layer has 25 units. Hence, the parameters go from w1 through w25 as well as b1 through b25. We have written x here but we could also write a0 because by convention the activation of layer 0, that is a0 is equal to the input feature value x.
- The next step would be to compute a2. Looking at the 2nd hidden layer, it then carries out the computation where a2 is the function of a1 and it's computed as the safe point activation function applied to w dot product a1 plus the corresponding value of b.
- Layer two has 15 neurons or 15 units which is why the parameter runs from w1 through w15 and b1 through b15.

<img width="1888" height="941" alt="image" src="https://github.com/user-attachments/assets/5e3cc7a9-38d8-4143-afb7-02d9fc68b6bb" />
---
<img width="1896" height="910" alt="image" src="https://github.com/user-attachments/assets/51b27fe5-1af5-43d3-bba1-b38ae9f514a9" />
---

- The final step is then to compute a3 and we do so using a very similar computation. The third layer has just one unit which is why there's just one output here. So, a3 is just a scalar. So finally we can take a[3] and threshold it at 0.5 to come up with a binary classification label.
- We also use f(x) to denote the function computed by the neural network as a function of x. Because this function goes from left to right, you start from x and compute a1, then a2, then a3. This album is also called forward propagation because we're propagating the activations of the neurons. So we're making these computations in the forward direction from left to right.
- This is in contract to a different algorithm called backward propagation or back propagation.

<img width="1919" height="951" alt="image" src="https://github.com/user-attachments/assets/f14959be-ea2a-4936-9f09-6ed113f5c122" />

---

## Lab: Neurons and Layers

#### Packages
**Tensorflow and Keras**  
Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by Fran√ßois Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface. 

```
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Sequential
from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy
from tensorflow.keras.activations import sigmoid
from lab_utils_common import dlc
from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic
plt.style.use('./deeplearning.mplstyle')
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)
```

We'll use an example from Course 1, linear regression on house prices.

```
X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet)
Y_train = np.array([[300.0], [500.0]], dtype=np.float32)       #(price in 1000s of dollars)

fig, ax = plt.subplots(1,1)
ax.scatter(X_train, Y_train, marker='x', c='r', label="Data Points")
ax.legend( fontsize='xx-large')
ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')
ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')
plt.show()
```

##### Regression/Linear Model 
The function implemented by a neuron with no activation is the same as in Course 1, linear regression:
$$ f_{\mathbf{w},b}(x^{(i)}) = \mathbf{w}\cdot x^{(i)} + b \tag{1}$$

