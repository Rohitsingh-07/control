## Inference: making predictions (forward propagation)

- An algorithm which lets our neural network to make inferences or make predictions is called forward propagation. We are going to use the handwritten digit recognition example. For simplicity we are going to just distinguish between two digits 0 and 1 for now.
- It's just a binary classification problem where we're going to input an image and classify, is this the digit 0 or the digit 1?
- For this example, we are going to use an 8 x 8 image and this image of a 1 is this grid or matrix of 8 x 8 or 64 pixel intensity values where 255 denotes a bright white pixel and 0 would denote a black pixel.
- Given these 64 input features, we're going to use the neural network with two hidden layers. Where the first hidden layer has 25 neurons or 25 units. 2nd hidden layer has 15 neurons or 15 units. And then finally the output layer or the output unit, what's the chance of this being 1 versus 0?
- The first computation is to go from X to a1, and that's what the first layer of the first hidden layers. It carries out a computation of a[1]. The formula is shown in the image below. Notice that a1 has 25 numbers because the hidden layer has 25 units. Hence, the parameters go from w1 through w25 as well as b1 through b25. We have written x here but we could also write a0 because by convention the activation of layer 0, that is a0 is equal to the input feature value x.
- The next step would be to compute a2. Looking at the 2nd hidden layer, it then carries out the computation where a2 is the function of a1 and it's computed as the safe point activation function applied to w dot product a1 plus the corresponding value of b.
- Layer two has 15 neurons or 15 units which is why the parameter runs from w1 through w15 and b1 through b15.

<img width="1888" height="941" alt="image" src="https://github.com/user-attachments/assets/5e3cc7a9-38d8-4143-afb7-02d9fc68b6bb" />
---
<img width="1896" height="910" alt="image" src="https://github.com/user-attachments/assets/51b27fe5-1af5-43d3-bba1-b38ae9f514a9" />
---

- The final step is then to compute a3 and we do so using a very similar computation. The third layer has just one unit which is why there's just one output here. So, a3 is just a scalar. So finally we can take a[3] and threshold it at 0.5 to come up with a binary classification label.
- We also use f(x) to denote the function computed by the neural network as a function of x. Because this function goes from left to right, you start from x and compute a1, then a2, then a3. This album is also called forward propagation because we're propagating the activations of the neurons. So we're making these computations in the forward direction from left to right.
- This is in contract to a different algorithm called backward propagation or back propagation.

<img width="1919" height="951" alt="image" src="https://github.com/user-attachments/assets/f14959be-ea2a-4936-9f09-6ed113f5c122" />

---

## Lab: Neurons and Layers

#### Packages
**Tensorflow and Keras**  
Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by François Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface. 

```
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Sequential
from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy
from tensorflow.keras.activations import sigmoid
from lab_utils_common import dlc
from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic
plt.style.use('./deeplearning.mplstyle')
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)
```
- ```from tensorflow.keras.layers import Dense, Input``` https://chatgpt.com/s/t_689d2980f39081918626cd51beeb8aa5
- This line means that the input is the input layer, the dense is the hidden first layer where every neuron talks to every input layer neuron
- <img width="606" height="380" alt="image" src="https://github.com/user-attachments/assets/43f52f6b-cc9f-425b-b920-82d5447e945a" />
- What is Keras? https://chatgpt.com/s/t_689d2a19ffa08191a3f092c7eed5031f
- Sequential
  - Sequential is a model type in Keras which helps us in building a neural network layer by layer in order. Without sequential, we'd have to manually connect each layer 
- ```from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy``` Here we are importing losses package and in that we have the mean squared error is the function which we learned in regression. Binary cross entropy is in reference to logistic regression
- ```from tensorflow.keras.activations import sigmoid``` Here, the activations is again a package which has the different kind of activation functions inside of it. Sigmoid is a ready made function which is present in the activations package and is the logistic regression function and the same formula. https://chatgpt.com/s/t_689d36498ffc819186495be262205e48
- ```import logging ``` This imports python built in logging module. Logging is like a messenger which prints messages about what's happening in the program
- ```logging.getLogger("tensorflow").setLevel(logging.ERROR)``` This tells python that for all the messages coming from Tensorflow only show errors
- The last line is telling that Tensorflow has something called Autograph, which converts python code into Tensorflow Graphs BTS, This line says to stop doing that and keep the output clean.

We'll use an example from Course 1, linear regression on house prices.

```
X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet)
Y_train = np.array([[300.0], [500.0]], dtype=np.float32)       #(price in 1000s of dollars)

fig, ax = plt.subplots(1,1)
ax.scatter(X_train, Y_train, marker='x', c='r', label="Data Points")
ax.legend( fontsize='xx-large')
ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')
ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')
plt.show()
```
- This is a pretty self explanatory code

##### Regression/Linear Model 
The function implemented by a neuron with no activation is the same as in Course 1, linear regression:

`f_{w,b}(x^{(i)}) = w · x^{(i)} + b`

We can define a layer with one neuron or unit and compare it to the familiar linear regression function.

```linear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', )``` This code here is saying that for a dense layer, for 1 neuron that is Unit 1 the function is linear as in no activation
- https://chatgpt.com/s/t_689d394a12f88191a9209d241ee36eee

- <img width="801" height="551" alt="image" src="https://github.com/user-attachments/assets/a2c4f716-a33b-4988-89a1-d9adb4e0424e" />

- ```linear_layer.get_weights()``` This won't give any output because we have not added any input data to it
- ```a1 = linear_layer(X_train[0].reshape(1,1)) print(a1)``` When we run this, it does the linear regression computation and gives the output value as 0.56
  - The result is a Tensor (another name of an array) with a shape of (1,1) or one entry.
- ```w, b= linear_layer.get_weights() - print(f"w = {w}, b={b}")``` This line of code helps us get the weights that were used
- A linear regression model(1) with a single input feature will have a single weight and bias. This matches the dimensions of the linear layer above

```
set_w = np.array([[200]])
set_b = np.array([100])

# set_weights takes a list of numpy arrays
linear_layer.set_weights([set_w, set_b])
print(linear_layer.get_weights())
```
- Here we are setting the weights and then trying to get it. We are using the set weigts function

```
a1 = linear_layer(X_train[0].reshape(1,1))
print(a1)
alin = np.dot(set_w,X_train[0].reshape(1,1)) + set_b
print(alin)
```
- we get the same output for both which shows that the linear_layer neuron and the dot product are basically the same thing

```
prediction_tf = linear_layer(X_train)
prediction_np = np.dot( X_train, set_w) + set_b
```
- We make the predictions through both the functions

- We then plot the result and it is identical
- <img width="1209" height="357" alt="image" src="https://github.com/user-attachments/assets/03cbbaef-3d76-40c0-89ac-a91b8d7aea4a" />

<img width="1031" height="230" alt="image" src="https://github.com/user-attachments/assets/7aa5427f-46f7-498c-918c-388a1a9ede62" />

## Neuron with Sigmoid activation
The function implemented by a neuron/unit with a sigmoid activation is the same as in Course 1, logistic  regression:
$$ f_{\mathbf{w},b}(x^{(i)}) = g(\mathbf{w}x^{(i)} + b) \tag{2}$$
where $$g(x) = sigmoid(x)$$ 

Let's set $w$ and $b$ to some known values and check the model.

```
# Training Example

X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix
Y_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix
```

```
pos = Y_train == 1
neg = Y_train == 0

fig,ax = plt.subplots(1,1,figsize=(4,3))
ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label="y=1")
ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label="y=0", facecolors='none', 
              edgecolors=dlc["dlblue"],lw=3)

ax.set_ylim(-0.08,1.1)
ax.set_ylabel('y', fontsize=12)
ax.set_xlabel('x', fontsize=12)
ax.set_title('one variable plot')
ax.legend(fontsize=12)
plt.show()
```

<img width="368" height="254" alt="image" src="https://github.com/user-attachments/assets/528e61d1-8db9-48b2-b471-9f93d0ff211f" />

##### Logistic Neuron
We can implement a 'logistic neuron' by adding a sigmoid activation. The function of the neuron is then described by (2) above.   
This section will create a Tensorflow Model that contains our logistic layer to demonstrate an alternate method of creating models. Tensorflow is most often used to create multi-layer models. The [Sequential](https://keras.io/guides/sequential_model/) model is a convenient means of constructing these models.

```
model = Sequential(
    [
        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')
    ]
)
```
- https://chatgpt.com/s/t_689d3ec958208191aa06c22fceb2a4c6
- Here, we are mostly just using the dequential function for the sequence of the layer go through and using a single neuron and the sigmoid function
- `model.summary()` shows the layers and number of parameters in the model. There is only one layer in this model and that layer has only one unit. The unit has two parameters, $w$ and $b$.
- <img width="764" height="255" alt="image" src="https://github.com/user-attachments/assets/389418d4-6b6d-4cb5-95ce-2d723133fbf3" />

```
logistic_layer = model.get_layer('L1')
w,b = logistic_layer.get_weights()
print(w,b)
print(w.shape,b.shape)
```
- https://chatgpt.com/s/t_689d3f91988c8191a7988d6de7ffc75f
- The get layer function looks inside the sequential model and finds the layer with the name "L1"

```
set_w = np.array([[2]])
set_b = np.array([-4.5])
# set_weights takes a list of numpy arrays
logistic_layer.set_weights([set_w, set_b])
print(logistic_layer.get_weights())
```
- Here we are setting the weights

```
a1 = model.predict(X_train[0].reshape(1,1))
print(a1)
alog = sigmoidnp(np.dot(set_w,X_train[0].reshape(1,1)) + set_b)
print(alog)
```
- Here, we are doing the same process of checking the accuracy of the model with the manual calculation using the sigmoid function

<img width="1174" height="365" alt="image" src="https://github.com/user-attachments/assets/598a5c1a-d2bb-44c4-b73d-fde586b30294" />
- The shading above reflects the output of the sigmoid which varies from 0 to 1.
