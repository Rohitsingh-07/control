## How Neural Networks are implemented efficiently

- The reason why neurlal networks have been able to scale up is because neural networks can be vectorized. They can be implemented very efficiently using matrix multiplications.
- In the code that we saw in the last lecture, the one where we are taking the input, the weights of the 3 neurons and the parameters and implementing forward propagation on a single layer
- We can develop a vecorized implementation of this code. X (the input) becomes a 2D array. W stays the same. B also becomes a 1 x 3 2D array
- We are then defining a function and then using the Matmul function which carries out matrix multiplication.
- One thing to notice is that all the elements that is A_in, W, X, B are now 2D arrays. All of these are matrices.
- This turns out to be a very efficient implementation of one step of forward propagation through a dense layer in the neural network.

<img width="1868" height="899" alt="image" src="https://github.com/user-attachments/assets/72bb5907-1b68-4c54-b70c-25d3f95e4024" />

---

## Matrix Multiplication

- Dot Products

<img width="1868" height="899" alt="image" src="https://github.com/user-attachments/assets/01ed9354-8373-4fda-9adc-72852a227d1d" />

- Vector Matrix Multiplication

<img width="1824" height="859" alt="image" src="https://github.com/user-attachments/assets/96531160-f6f9-43ff-9ea5-5818b345477d" />

- Matrix Matrix Multiplication

<img width="1863" height="912" alt="image" src="https://github.com/user-attachments/assets/24374614-ed9b-4617-9728-b3041078cabf" />

---

## Matrix Multiplication Rules

<img width="1865" height="877" alt="image" src="https://github.com/user-attachments/assets/68e509f4-4578-45e8-9aa2-f5803bea5a3f" />

- One rule of Matrix multiplication is that we can take the dot product between a vector and a matrix only when they are of the same length.
- The numbers of columns of the first matrix should be equal to the number of rows of the second matrix

<img width="1870" height="897" alt="image" src="https://github.com/user-attachments/assets/bc669a2c-f523-4408-85f8-1c6eeb4d6d91" />

---

## Matrix Multiplication Code

<img width="1815" height="779" alt="image" src="https://github.com/user-attachments/assets/1d39c5bd-eaae-45c9-8486-371a4d303d07" />

- We discuss the explanation in terms of matrix in this slide

<img width="1686" height="899" alt="image" src="https://github.com/user-attachments/assets/b0143c19-f4e2-47df-9899-d1d7a83a15f5" />

- Same thing for this slide

---

## Tensorflow Implementation

- The initial step while training in Tensorflow is that we are asking Tensorflow to sequentially string together the three layers of a neural network. The first hidden layer with 25 units and sigmoid activation, the 2nd and 3rd layer as given
- The 2nd step is that we ask Tensorflow to compile the model. The key step in asking Tensorflow to compile the model is to specify what is the loss function we want to use. Here, we are using the Binary crossentropy loss function.
- The 3rd step is to call the fit function, which tells the Tensorflow to fit the model that we specified in step 1 using the loss of the cost function that we specified in the step 2 of the dataset X,Y.
- Epochs is the technical term for how many steps of a learning algorithm like gradient descent we may want to run.
- Step 1: Specify the model, which tells tensorflow how to compute for the inference.
- Step 2: Compiles the model using specific loss function
- Step 3: Train the model

<img width="1889" height="934" alt="image" src="https://github.com/user-attachments/assets/ebcf42a8-36d6-454c-a8f7-c1d4b9f8d23d" />

---

## Training Details

- Let's recall how we had trained a logistic regression model
  - Step 1: We would specify how to compute the output given the input feature x and the parameters w and b. The logistic regression function predicts f of x is equal to G. The sigmoid function applied to W x X + b
  - If z is the dot product of W and of X plus B, then F of X is 1 over 1 plus e to the negative z, so those first step were to specify what is the input to output function of logistic regression, and that depends on both the input x and the parameters of the model.
 
  - Step 2
    - The next step was to specify the loss function and also he cost function. It said that if logistic regression outputs f of x and the actual label y then the loss on that single training example was the formula that we see
    - This was a measure of how well is logistic regression doing on a single training example x, y. Given this definition of the loss function, we then define the cost function, and the cost function was a function of parameters W and B, and that was just the average that is taking an average overall M training examples of the loss function computed on the M training examples, X1, Y1 through the last values.
    - In this convention, we are using the loss function, it is a function of the output of the learning algorithm and the truth labels as computed over a single training example whereas the cost function J is an average of the loss function computed over the entire training set.
   
  - Step 3
    - The third step to train a logistic regression model was to use an algorithm specifically gradient descent to minimize the cost function J of WB to minimize it as a function of the parameters W and B. We minimize the cost J as a function of the parameters using gradient descent where W and B are updated

- Using these same three steps, we can train a neural network in Tensorflow. Let's see how these three steps map to training a neural network.
  - Step 1:
    - Specify how to compute the output given the input x and parameters W and B that's done with the code we saw earlier. This is enough to specify the computations needed in forward propagation or for the inference algorithm
  - Step 2:
    - Compile the model and tell it what loss we want to use, and here we used the binary cross entropy loss function, and once you specify this loss taking an average over the entire training set also gives the cost function for the neural network.
  - Step 3:
    - Call function to try to minimize the cost as a function of the parameters of the neural network.

<img width="1885" height="897" alt="image" src="https://github.com/user-attachments/assets/cdf2f91b-9ec0-492b-825f-eeb4435ad215" />

---
- The representation of the first step
<img width="1891" height="919" alt="image" src="https://github.com/user-attachments/assets/b3207ca4-5964-4c70-8d42-81eafee70d9b" />

- In 2nd step, we have to specify what the loss function is which will also define the cost function we use to train the neural network.
- In the handwritten digit classification problem, where images are either 0 or 1, the most common loss function to use is the same loss function which we had from the logistic regression. In Tensorflow, it is called the binary cross-entropy loss function.
- Having specified  the loss with respect to a single training example, Tensorflow knows that it costs we want to minimize is then the average, taking the average over all m training examples of the loss on all of the training examples.
- Optimizing this cost function will result in fitting the neural network to the binary classification data. In case, we want to solve a regression problem rather than a classification problem. We can ask for a different loss function.
- Just in case, if we have a regression problem and if we want to minimize the squared error loss. We would use the loss function as stated in the picture. The cost function is a function of all the parameters into the neural network.

<img width="1894" height="916" alt="image" src="https://github.com/user-attachments/assets/e61ea18b-e7b1-4cd5-bd47-446db46743b9" />

---
- The 3rd step is Gradient Descent. If we are using gradient descent to train the parameters of a neural network, then we are repeatedly, for every layer i and for every unit j, update w and b.
- After doing 100 iterations of gradient descent, hopefully, we get a good value of the parameters. In order to use gradient descent, the key thing we need to compute is the partial derivative terms.
- What tensorflow does or is standard in neural networks training is to use an algorithm called backpropagation in order to compute these partial derivative terms. It implements the backpropagartion all within this function called fit.
- Now, we know how to train a basic neural network, also called a mutilayer perceptron.

<img width="1810" height="882" alt="image" src="https://github.com/user-attachments/assets/7274947d-87e7-4363-a6af-98891195cf16" />
