## Advanced Optimization

- Gradient Descent is an optimization algorithm that is widely used in Machine learning, and was the foundation of many algorithms like linear regression and logistic regression and early implementations of neural networks. But, it turns out that there are now some other optimization algorithm for minimizing the cost function, that are even better than the gradient descent.
- Let's recall the expression for one step of gradient descent. The formula for updating parameter w. If we were to start gradient descent here, one step of gradient descent, if Alpha is small, may take us a little bit in that direction. Then it would take the multiple steps and we might think that why don't we make the alpha bigger, can we have an algorithm to automaticaly increase Alpha? That just make it take bigger steps and get to the minimum faster.
- There's an algorithm called the ADAM algorithm that can do that. If it sees that the learning rate is too small, and we are just taking tiny little steps in the same direction over and over, we should just make the learning rate Alpha bigger.
- If we start with the same cost function with a relatively big learning rate Alpha, then it would make bigger steps and bounce from one location to another and we ask ourselves that maybe we should make the learning rate smaller.
- The ADAM algorithm can also do that automatically, and with a smaller learning rate, we can then take a more smooth path towards the minimum of the cost function. Depending on how gradient descent is proceeding, sometimes we wish to have a bigger learning rate Alpha, and sometimes we wish we had a smaller learning rate Alpha.

<img width="1874" height="926" alt="image" src="https://github.com/user-attachments/assets/af891adc-0308-4269-8af1-97e1aa601e6c" />

- The Adam algorithm can adjust the learning rate automatically. Adam stands for Adaptive Moment Estimation. Interestingly, the Adam algorithm doesn't use a single global learning rate Alpha. It uses a different learning rates for every single parameter of the model. If we have parameters w_1 through w_10, as was b, then it actually has 11 rate parameters, Alpha 1, Alpha_2 all the way through Alpha_10 for w_1 to w_10, as well as Alpha_11 for the parameter b.

<img width="1837" height="867" alt="image" src="https://github.com/user-attachments/assets/d01b11e1-2a89-4eac-b0d2-ddc927576eca" />

- The intuition behind the Adam algorithm is, if a parameter w_j, or b seems to keep on moving in roughly the same direction, let's increase the learning rate for that parameter. Let's go faster in that direction.
- Conversely, If a parameter keeps oscillating back and forth, then let's not have it keep on oscillating or bouncing back and forth. Let's reduce Alpha_j for that parameter a little bit.
- In code to implement the adam, we just need to add on extra argument to the compile function, which is that we specify that the optimizer we want to use is tf.keras.optimizers.Adam optimizer. The Adam optimization algorithm does need some default initial learning rate Alpha, and here we've set that initial learning rate to be 10^-3.
- But when we are using the Adam algorithm in practice, it's worth trying a few values for this default global learning rate. We should try some large and some smaller values to see what gives us the fastest learning performance. Compared to the original gradient descent algorithm that we learned earlier, the ADAM algorithm, because it can adapt the learning rate a bit automatically, it is more robust to the exact choice of learning rate that we pick.

<img width="1909" height="898" alt="image" src="https://github.com/user-attachments/assets/a1ebf8c9-aa63-4f9f-99a1-f2703eecc2de" />

- The ADAM algorithm typically works much faster than gradient descent, and it's become a de facto standard in how practitioners train their neural networks. If we are trying to decide what learning algorithm to use, what optimization algorithm to use to train the neural network. A safe choice would be to just use the ADAM optimization algorithm, and most practitioners today will use ADAM rather than the optional gradient descent algorithm.

---

## Additional Layer Types

- All the neural network layers we have seen so far have been the dense layer type in which every neuron in the layer gets its inputs all the activations from the previous layer. And it turns out that just using the dense layer type, we can actually build some pretty powerful learning algorithms. It turns out that there's some other types of layers as well with other properties.
- To recap, in the dense layer that we've been using the activation of a neuron is say the 2nd hidden layer is a function of every single activation value from the previous layer of a_1.

<img width="1803" height="913" alt="image" src="https://github.com/user-attachments/assets/f56bcb48-b75f-416a-b456-3150d2994a20" />

- One other layer type that we may see in some work is called a convolutional layer. So let's say on the left is the input X which is a handwritten digit 9. What we are going to do is construct a hidden layer which will compute different activations as functions of this input image X. For the first unit, rather than saying that this neuron can look at all the pixels in this image. We might say that this neuron can only look at the pixels in this little rectangular region.
- 2nd Neuron, which I am going to illustrate in magenta is also not going to look at the entire input image X instead, it's only going to look at the pixels in a limited region of the image. And so on for the third neuron and the 4th neuron and so on and so forth. Down to the last neuron which may be looking only at that region of the image.
- The benefits of letting some neurons look at a given region and not letting every neuron to look at all the region is that it speeds up computation. And 2nd advantage is that a neural network that uses this type of layer called a convolutional layer can need less training data or alternatively, it can also be less prone to overfitting.
- This is the type of layer where each neuron only looks at a region of the input image and it is called a convolutional layer.

<img width="1820" height="909" alt="image" src="https://github.com/user-attachments/assets/96f6a7f9-8430-47b1-a90d-1a0106d056e5" />

- If we have multiple convolutional layers in a neural network, that's called a convolutional neural network. To illustrate the convolutional layer of the convolutional neural network, we are going to use a two D image input. We are going to use a 1-D input and the motivating example we are going to use is classification of EKG (ECG) signals or electrocrdiograms.
- If we put two electrodes on the chest, we will record the voltage that looks like something which corresponds to the heartbeat. In some places there's just a list of numbers corresponding to the height of the surface at different points in time.
- So we may have say 100 numbers corresponding to the height of the curve at 100 different points of time. And the learning tosses given this time series, given this EKG signal to classify say whether this patient has a heart disease or some diagnosable heart conditions.
- Here's what the convolutional neural network might do. We are going to take the EKG signal and rotate it 90 degrees to lay it on the side. So we have here 100 inputs X1, X2 all the way through X100. So when we construct the first hidden layer, Instead of having the first hidden unit take input of all 100 numbers. Let us have the first hidden unit look at only X 1 through X 20
- That corresponds to a small window of this EKG signal. The 2nd hidden unit shown in a different color here. We'll look at X11 to X30, so looks at a different window in this EKG signal. And the 3rd hidden layers looks at another window X21 through X40 and so on. The final hidden units in this example will look at X81 through X100. So, it looks like a small window towards the end of this EKG time series.
- So this is a convolutional layer because these units in this layer looks at only a limited window of the input. Now this layer of the neural network has 9 units. The next layer can also be a convolutional layer. So in the 2nd hidden layer, let me architect my first unit not to look at all nine activations from the previous layer, but to look at say just the first 5 activations from the previous layer.
- And then my 2nd unit they may look at just another five numbers, say A3-A7. And the 3rd and final hidden unit in this layer will only look at A5 through A9 and then maybe finally these activations.
- A2 gets inputs to a sigmoid unit that does look at all three of these values of A2 in order to make a binary classification regarding the presence or absence of heart disease. We see the example of the neural network with the first hidden layer being a convolutional layer. The 2nd hidden layer also being a convolutional layer and then the output layer being a sigmoid layer.
- It turns out that with convolutional layers we have many architecture choices such as how big is the window of inputs that a single neuron should look at and how many neurons should a layer have. By choosing those architecture parameters effectively, we can build new versions of neural networks that can be even more effective than the dense layer of some applications.

<img width="1917" height="952" alt="image" src="https://github.com/user-attachments/assets/b90c7c69-89b3-4b61-9af1-ba8aba306ed7" />

- The latest cutting edge architecture like a transformer model or an LSTM or an attention model. A lot of this research in neural networks even today pertains to researchers trying to invent new types of layers for neural networks. And plugging these different types of layers together as building blocks to form even more complex and hopefully more powerful neural networks.

---

## Complete Back Propagation

---

## Practice Lab: Neural Network for Handwritten Digit Recognition, Multiclass

```
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.activations import linear, relu, sigmoid
%matplotlib widget
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')

import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)

from public_tests import * 

from autils import *
from lab_utils_softmax import plt_softmax
np.set_printoptions(precision=2)
```

- Importing all the packages

<img width="1236" height="649" alt="image" src="https://github.com/user-attachments/assets/e5b848cc-78d7-4a2f-9e11-fc2013f5d289" />

<img width="1170" height="528" alt="image" src="https://github.com/user-attachments/assets/3153f314-b9bc-48ba-9889-76d20b52fd05" />

```
# UNQ_C1
# GRADED CELL: my_softmax

def my_softmax(z):  
    """ Softmax converts a vector of values to a probability distribution.
    Args:
      z (ndarray (N,))  : input data, N features
    Returns:
      a (ndarray (N,))  : softmax of z
    """    
    ### START CODE HERE ### 
    ez = np.exp(z)
    a = ez/np.sum(ez)
    ### END CODE HERE ### 
    return a
```

```
z = np.array([1., 2., 3., 4.])
a = my_softmax(z)
atf = tf.nn.softmax(z)
print(f"my_softmax(z):         {a}")
print(f"tensorflow softmax(z): {atf}")

# BEGIN UNIT TEST  
test_my_softmax(my_softmax)
# END UNIT TEST

Output:
my_softmax(z):         [0.03 0.09 0.24 0.64]
tensorflow softmax(z): [0.03 0.09 0.24 0.64]
 All tests passed.
```

Below, vary the values of the `z` inputs. Note in particular how the exponential in the numerator magnifies small differences in the values. Note as well that the output values sum to one.

```
plt.close("all")
plt_softmax(my_softmax)
```
<img width="1211" height="595" alt="image" src="https://github.com/user-attachments/assets/9becc817-f945-41b6-8fdd-e670be45f5bb" />

<img width="1180" height="716" alt="image" src="https://github.com/user-attachments/assets/18dc5256-879e-48d8-8c83-e21efcebdae7" />

<img width="1210" height="668" alt="image" src="https://github.com/user-attachments/assets/1e38372c-1874-4dd7-ae2e-0a4c5e9d47a7" />

<img width="1239" height="716" alt="image" src="https://github.com/user-attachments/assets/51ebbab3-ca10-462b-8b51-dbc5b2aff455" />

```
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
# You do not need to modify anything in this cell

m, n = X.shape

fig, axes = plt.subplots(8,8, figsize=(5,5))
fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]

#fig.tight_layout(pad=0.5)
widgvis(fig)
for i,ax in enumerate(axes.flat):
    # Select random indices
    random_index = np.random.randint(m)
    
    # Select rows corresponding to the random indices and
    # reshape the image
    X_random_reshaped = X[random_index].reshape((20,20)).T
    
    # Display the image
    ax.imshow(X_random_reshaped, cmap='gray')
    
    # Display the label above the image
    ax.set_title(y[random_index,0])
    ax.set_axis_off()
    fig.suptitle("Label, image", fontsize=14)
```

<img width="825" height="570" alt="image" src="https://github.com/user-attachments/assets/4de9d11e-765d-423a-8804-2e8e554ab25f" />

<img width="1155" height="715" alt="image" src="https://github.com/user-attachments/assets/ef3e2c2a-5bf3-4b65-b584-a0151ec909dd" />

<img width="1152" height="647" alt="image" src="https://github.com/user-attachments/assets/00ccd96b-e346-49ef-b89b-8c24bafacc93" />

```
# UNQ_C2
# GRADED CELL: Sequential model
tf.random.set_seed(1234) # for consistent results
model = Sequential(
    [               
        ### START CODE HERE ### 
        tf.keras.Input(shape=(400,)),
        Dense(25, activation='relu', name = "L1"),
        Dense(15, activation='relu', name = "L2"),
        Dense(10, activation='linear', name = "L3")
        
        
        
        ### END CODE HERE ### 
    ], name = "my_model" 
)
```
<img width="1182" height="339" alt="image" src="https://github.com/user-attachments/assets/aa0815f3-f3a3-43a4-a94d-e8a4d5fd27c9" />

```
# BEGIN UNIT TEST     
test_model(model, 10, 400)
# END UNIT TEST
```

```
[layer1, layer2, layer3] = model.layers
```

```
#### Examine Weights shapes
W1,b1 = layer1.get_weights()
W2,b2 = layer2.get_weights()
W3,b3 = layer3.get_weights()
print(f"W1 shape = {W1.shape}, b1 shape = {b1.shape}")
print(f"W2 shape = {W2.shape}, b2 shape = {b2.shape}")
print(f"W3 shape = {W3.shape}, b3 shape = {b3.shape}")

Output:
W1 shape = (400, 25), b1 shape = (25,)
W2 shape = (25, 15), b2 shape = (15,)
W3 shape = (15, 10), b3 shape = (10,)
```

The following code:
* defines a loss function, `SparseCategoricalCrossentropy` and indicates the softmax should be included with the  loss calculation by adding `from_logits=True`)
* defines an optimizer. A popular choice is Adaptive Moment (Adam) which was described in lecture.

```
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
)

history = model.fit(
    X,y,
    epochs=40
)
```

<img width="1252" height="443" alt="image" src="https://github.com/user-attachments/assets/422d5c73-550e-4e61-8f5a-bc35b10e6fce" />

<img width="1059" height="418" alt="image" src="https://github.com/user-attachments/assets/68ea3252-b238-4ed1-b111-c4f4ecb5081a" />

#### Prediction 
To make a prediction, use Keras `predict`. Below, X[1015] contains an image of a two.

```
image_of_two = X[1015]
display_digit(image_of_two)

prediction = model.predict(image_of_two.reshape(1,400))  # prediction

print(f" predicting a Two: \n{prediction}")
print(f" Largest Prediction index: {np.argmax(prediction)}")
```

<img width="1076" height="313" alt="image" src="https://github.com/user-attachments/assets/e2833971-534b-4236-8399-3fd5e4e43b21" />

The largest output is prediction[2], indicating the predicted digit is a '2'. If the problem only requires a selection, that is sufficient. Use NumPy [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) to select it. If the problem requires a probability, a softmax is required:

```
prediction_p = tf.nn.softmax(prediction)

print(f" predicting a Two. Probability vector: \n{prediction_p}")
print(f"Total of predictions: {np.sum(prediction_p):0.3f}")

Output:
 predicting a Two. Probability vector: 
[[1.42e-04 4.49e-02 8.98e-01 3.76e-02 3.61e-06 5.97e-06 3.03e-05 1.44e-02
  5.03e-03 3.22e-04]]
Total of predictions: 1.000
```

To return an integer representing the predicted target, you want the index of the largest probability. This is accomplished with the Numpy [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function.

```
yhat = np.argmax(prediction_p)

print(f"np.argmax(prediction_p): {yhat}")

Output:
np.argmax(prediction_p): 2
```

Let's compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run.

```
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
# You do not need to modify anything in this cell

m, n = X.shape

fig, axes = plt.subplots(8,8, figsize=(5,5))
fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]
widgvis(fig)
for i,ax in enumerate(axes.flat):
    # Select random indices
    random_index = np.random.randint(m)
    
    # Select rows corresponding to the random indices and
    # reshape the image
    X_random_reshaped = X[random_index].reshape((20,20)).T
    
    # Display the image
    ax.imshow(X_random_reshaped, cmap='gray')
    
    # Predict using the Neural Network
    prediction = model.predict(X[random_index].reshape(1,400))
    prediction_p = tf.nn.softmax(prediction)
    yhat = np.argmax(prediction_p)
    
    # Display the label above the image
    ax.set_title(f"{y[random_index,0]},{yhat}",fontsize=10)
    ax.set_axis_off()
fig.suptitle("Label, yhat", fontsize=14)
plt.show()
```

<img width="879" height="671" alt="image" src="https://github.com/user-attachments/assets/ab5ae022-d6db-4ba5-9cc8-60f56bff583a" />

```
print( f"{display_errors(model,X,y)} errors out of {len(X)} images")
```

<img width="934" height="217" alt="image" src="https://github.com/user-attachments/assets/46f7dd66-87d2-41fd-8cda-810662b9845b" />
