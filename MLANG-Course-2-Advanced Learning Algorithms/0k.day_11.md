# Week 3

## Advice for applying machine learning

- Let's assume, we implemented a regularized linear regression to predict housing prices, so we have the usual cost function for the learning algorithm, squared error plus the regularization term. But if we train the model, and find that it makes unacceptably large errors in it's predictions.
- When we are building a machine learning algorithm, there are usually a lot of different things we could try. For example, we could decide on
  - Getting more training examples
  - Try smaller set of features
  - Try getting additional features
  - Try adding polynomial features (x1^2, x2^2, x1x2, etc)
  - Try decreasing Lambda
  - Try increasing Lambda

- On any given machine learning application, it will often turn out that some of these things could be fruitful and some things not fruitful
- The key to being effective at how we build a machine learning algorithm will be if we can find a way to make good choices about where to invest the time
- For example: some times getting more training examples or getting more data helps and sometimes it doesn't.

<img width="1804" height="915" alt="image" src="https://github.com/user-attachments/assets/08de48ed-3481-43b2-9896-c89f124260dd" />

- By diagnostic, which is a test that we can run to gain insight into what is or isn't working with learning algorithm to gain guidance into improving its performance.
- Some of these diagnostics will tell us things like, is it worth weeks, or even months collecting more training data, which will hopefully lead to improved performance, or if it isn't then running that diagnostic could have saved our time
- Diagnostic can take time to implement, but running them can be a very good use of our time.

<img width="1842" height="837" alt="image" src="https://github.com/user-attachments/assets/a7af4b88-45b6-4dd4-9c81-200b407ed484" />

---

## Evaluating a model

- To evaluate the model's performance and having a systematic way to evaluate performance will hope to paint a clearer path for how to improve its performance. Let's take the example of learning to predict housing prices as a function of the size. Let's say we have trained the model to predict housing prices as a function of the size x. And for the model that is a fourth order polynomial.
- So features x, x^2, x^3 and x^4. Because we fit 1/4 order polynomial to a training set with 5 data points, this fits the training data very well. But it seems that even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set.
- When we are predicting prices, just a single feature at the size of the house, we could plot the model as given in the diagram and we could see that the curve is very wiggly so we know this is probably not a good model.
- If we were fitting this model with even more features, say we had x1 the size of the house, number of bedrooms, the number of floors of the house, also the age of the home in years. It becomes much harder to plot f because f is now a function of x1 through x4. So in order to tell if the model is doing well, especially for applications where we have more than one or two features, which makes it difficult to plot f of x. We need a more systematic way to evaluate how well the model is doing.

<img width="1913" height="947" alt="image" src="https://github.com/user-attachments/assets/9f04401a-519e-42dc-b26c-3b231d4a4390" />

- If we have a training set and this is a small training set that we see here, rather than taking all the data to train the parameters w and b of the model, we can instead split the training set into two subsets. We are going to select the 70% of data into the 1st part and it is going to be called as the training set. The 2nd part of data, let's say 30% of the data, we are going to put into it a test set.
- We are going to train the models, parameters on the training set on this first 70% of the data, and then we'll test its performance on the test set.
- The denotion is given in the slide below. This is a regular practice of spliting the data in 70-30 split or 80-20 split.

<img width="1914" height="941" alt="image" src="https://github.com/user-attachments/assets/d5c94192-3a9d-4095-87ff-450dc6eb4bf7" />

- In order to train a model and evaluate it, it is given below what it would look like if we are using linear regression with a squared error cost. Start off by fitting the parameters by minimizing the cost function j of w,b. So we see the usual cost function minimize over w,b of this square error cost, plus regularization term.
- And then to tell how well this model is doing, we would compute J test of w,b which is given below. So, it's a prediction on the ith test example input minus the actual price of the house on the test example squared
- The test error formula J test, does not include the regularization term
