# Week 3

## Advice for applying machine learning

- Let's assume, we implemented a regularized linear regression to predict housing prices, so we have the usual cost function for the learning algorithm, squared error plus the regularization term. But if we train the model, and find that it makes unacceptably large errors in it's predictions.
- When we are building a machine learning algorithm, there are usually a lot of different things we could try. For example, we could decide on
  - Getting more training examples
  - Try smaller set of features
  - Try getting additional features
  - Try adding polynomial features (x1^2, x2^2, x1x2, etc)
  - Try decreasing Lambda
  - Try increasing Lambda

- On any given machine learning application, it will often turn out that some of these things could be fruitful and some things not fruitful
- The key to being effective at how we build a machine learning algorithm will be if we can find a way to make good choices about where to invest the time
- For example: some times getting more training examples or getting more data helps and sometimes it doesn't.

<img width="1804" height="915" alt="image" src="https://github.com/user-attachments/assets/08de48ed-3481-43b2-9896-c89f124260dd" />

- By diagnostic, which is a test that we can run to gain insight into what is or isn't working with learning algorithm to gain guidance into improving its performance.
- Some of these diagnostics will tell us things like, is it worth weeks, or even months collecting more training data, which will hopefully lead to improved performance, or if it isn't then running that diagnostic could have saved our time
- Diagnostic can take time to implement, but running them can be a very good use of our time.

<img width="1842" height="837" alt="image" src="https://github.com/user-attachments/assets/a7af4b88-45b6-4dd4-9c81-200b407ed484" />

---

## Evaluating a model

- To evaluate the model's performance and having a systematic way to evaluate performance will hope to paint a clearer path for how to improve its performance. Let's take the example of learning to predict housing prices as a function of the size. Let's say we have trained the model to predict housing prices as a function of the size x. And for the model that is a fourth order polynomial.
- So features x, x^2, x^3 and x^4. Because we fit 1/4 order polynomial to a training set with 5 data points, this fits the training data very well. But it seems that even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set.
- When we are predicting prices, just a single feature at the size of the house, we could plot the model as given in the diagram and we could see that the curve is very wiggly so we know this is probably not a good model.
- If we were fitting this model with even more features, say we had x1 the size of the house, number of bedrooms, the number of floors of the house, also the age of the home in years. It becomes much harder to plot f because f is now a function of x1 through x4. So in order to tell if the model is doing well, especially for applications where we have more than one or two features, which makes it difficult to plot f of x. We need a more systematic way to evaluate how well the model is doing.

<img width="1913" height="947" alt="image" src="https://github.com/user-attachments/assets/9f04401a-519e-42dc-b26c-3b231d4a4390" />

- If we have a training set and this is a small training set that we see here, rather than taking all the data to train the parameters w and b of the model, we can instead split the training set into two subsets. We are going to select the 70% of data into the 1st part and it is going to be called as the training set. The 2nd part of data, let's say 30% of the data, we are going to put into it a test set.
- We are going to train the models, parameters on the training set on this first 70% of the data, and then we'll test its performance on the test set.
- The denotion is given in the slide below. This is a regular practice of spliting the data in 70-30 split or 80-20 split.

<img width="1914" height="941" alt="image" src="https://github.com/user-attachments/assets/d5c94192-3a9d-4095-87ff-450dc6eb4bf7" />

- In order to train a model and evaluate it, it is given below what it would look like if we are using linear regression with a squared error cost. Start off by fitting the parameters by minimizing the cost function j of w,b. So we see the usual cost function minimize over w,b of this square error cost, plus regularization term.
- And then to tell how well this model is doing, we would compute J test of w,b which is given below. So, it's a prediction on the ith test example input minus the actual price of the house on the test example squared
- The test error formula J test, does not include the regularization term. And this will give you a sense of how well the learning algorithm is doing. One of the quantity that's often useful to compute as well is the training error, which is a measure of how well the learning algorithm is doing on the training set. The compute error is mentioned in the diagram. And once again, this does not include the regularization term unlike the cost function that we are minimizing to fit the parameters.

<img width="1918" height="966" alt="image" src="https://github.com/user-attachments/assets/2efc7bc1-e5ab-483c-9b61-c33414005aa6" />

- In the model that we saw earlier, J train of w,b will be low because the average error on the training examples will be zero or very close to zero. So J train will be very close to zero.
- In case we have a few additional examples in the test set that the algorithm had not trained on, then those test examples might be away from the line of best fit which would increase the cost function so J test will be high
- So seeing that J test is high on this model, gives us a way to realize that even though it does great on the training set, it is not actually so good at generalizing to new examples to new data points that were not in the training set

<img width="1892" height="935" alt="image" src="https://github.com/user-attachments/assets/bbe9b8ed-8a62-4319-a58a-a7a65c682ab7" />

- Now, let's take a look at how to apply this procedure to a classification problem. The same example of classiying te hand written digit, so same as before, we fit the parameters by minimizing the cost function to find the parameters w,b. If we would be training logistic regression, then we see the cost function J, the usual logistic loss, and then plus also the regularization term. And to compute the test error, J test is then the average over the test examples, that's that 30% of the data that wasn't in the training set of the logistic loss on the test set.
- And the training error we can also compute using the formul given below, is the average logistic loss on the training data that the algorithm was using to minimize the cost function J. What we discussed here will work, okay, for figuring out if the learning algorithm is doing well, by seeing how it was doing in terms of test error.

<img width="1917" height="954" alt="image" src="https://github.com/user-attachments/assets/8a8ed5e4-84ff-416b-839d-bf95e3600aba" />

- The other definition of J test and J train is more commonly used while applying machine learning to classification problems. Which is instead of using the logistic loss to compute the test error and the training error, we measure what the fraction of the test set, and the fraction of the training set that the algorithm has misclassified. So specifically on the test set, we can have the algorithm make a prediction 1 or 0 on every test example.
- So we would predict y hat as 1, if it crosses the threshold of 0.5 and zero otherwise. And we can then count up in the test set the fraction of examples where y_hat is not equal to the actual ground truth label while in the test set. So concretely, if we are classifying handwritten digits 0 or 1 as binary classification loss, then J tests would be the fraction of that test set, where 0 was classified as 1 or 1 classified as 0.
- And similarly, J train is a fraction of the training set that has been misclassified. Taking a dataset and splitting it into a training set and a separate test set gives us a way to systematically evaluate how well the learning algorithm is doing.

<img width="1909" height="908" alt="image" src="https://github.com/user-attachments/assets/89508dff-04d8-4ad2-a045-4a706f318cac" />

- By computing both J tests and J train, we can now measure how well was the algorithm doing on the test set and on the training set. The procedure is one step to what we be able to automatically choose what model to use for a given machine learning application.

---

## Model Selection and Training/ Cross Validation/ Test Sets

- We saw how to use the test set to evaluate the performnce of a model. One thing we saw is that once the model's parameters w and b have been fit to the training set. The training error may not be a good indicator of how well the algorithm will do or how well will it generalize to new examples that were not in the training set and in particular, for this example, the training error will be pretty much 0. That's likely much lower than the actual generalization error on new examples that were not in the training set.
- We saw that J test will be a better indicator of the performance of the algorithm on examples, is not trained on and it also will tell us on how well the model will likely do on new data which was not included in the training set.

<img width="1895" height="928" alt="image" src="https://github.com/user-attachments/assets/b6ae92a6-8747-4c57-9aec-8bedbf009c56" />

- Let's take a look at how this affects, how we might use a test set to choose a model for a given ML application. If a fitting function to predict housing prices or some other regression problem, one model we might consider is to fit a linear model. This is a first order polynomial and we are going to use d = 1 to denote fitting a one or first order polynomial.
- If we were to fit a model like this one to our training set, we get some parameters, w and b, and you can then compute J tests to estimate how well does this generalize to new data?
- We might also consider fitting a 2nd order polynomial or quadratic model. We have the parameters written and how well will it work is given by J test of those parameters.
- We can go on to try d = 3, that's a 3rd order or a degree three polynomial that looks as given below, and fit parameters and similarly get J test. We might keep doing it until, say we try up to a 10th order polynomial and we end up with J test of w^10, b^10. That gives us a sense of how well the 10th order polynomial is doing.
- One procedure we could try which is not the best procedure but what we could try is, look at all of these J tests, and see whih one gives us the lowest value. Say, we find that, J test for the 5th order polynomial turns out to be the lowest, so we might decide that the 5th order polynomial d = 5 does best, and choose that model for the aplication. If we want to estimate how well this model performs, one thing we could do which is a bit flawed is to report the test set error, J test w^5, b^5.
- The reason this procedure is flawed is J test of w^5, b^5 is likely to be an optimistic estimate of the generalization error. In other words, it is likely to be lower than the actual generalization error, and the reason is, in the procedure we talked about with basic fits, one extra parameter, which is d, the degree of polynomial, and we choose this parameter using the test set.
- We saw earlier that we try to fit w, b to the training data, then the training data would be an overly optimistic estimate of the generalization error. It turns out too, that if we want to choose the parameter d using the test set, then the test set J test is now an overly optimistic, that is lower than actual estimate of the generalization error. The procedure is flawed and i is not recommended to use it.

<img width="1919" height="938" alt="image" src="https://github.com/user-attachments/assets/6f0ec6f6-5a02-46c9-b963-b012226e484b" />

- Instead, if we want to automatically choose a model, such as decide what degree polynomial to use. Here's how we modify the training and testing procedure in order to carry out model selection. Whereby model selection, we mean choosing amongst different models, such as these 10 different models that we might compensate using for our Machine Learning application.
- The way we'll modify the procedure is instead of splitting the data into just two subsets, the training set and the test set, we are going to split the data into 3 different subsets, which we are going to call the training set, the cross-validation set, and then also the test set. Using our example from before of these 10 training examples, we might split it into putting 60 percent of the data into the training set and so the notation we'll use for the training set portion will be same as before, except that now M train, the number of training examples will be six and we might put 20 percent of the data into the cross validation set and a notation is as in the picture for the first cross-validation example. So cv stands for cross validation.
- Then finally we have the test set same as before, so x1 through xm tests and y1 through ym, where m tests equal to 2. We are going to introduce a new subset of the data called the cross-validation set. The name cross-validation refers to that this is an extra dataset that we're going to use to check or cross check the validity or really the accuracy of different models. It is also referred to as validation set, development set, dev set.

<img width="1898" height="955" alt="image" src="https://github.com/user-attachments/assets/9163cb2c-591a-4bb6-b90b-19ccd4d5da12" />

- We see the formula for the subsets of the data training set, cross-validation set, and test set. We can then compute the training error, the cross-validation error, and the test error using these three formulas. The cross validation error, is also commonly called the validation error or development set error or the dev error.
- Armed with these three measures of learning algorithm performance, this is how we can then go about carrying out model selection.

<img width="1867" height="908" alt="image" src="https://github.com/user-attachments/assets/cdfde3e1-a283-4e6e-a1dd-c8da451bdc78" />

- We can then with the 10 models, with d equals 1, d equals 2, all the way upto the 10th degree polynomial, we can evaluate the parameters w_1, b_1. But instead of evaluating this on the test set, we will instead evaluate these parameters on the cross-validation sets and compute J_cv of w_1, b_1 and similarly, for the 2nd model, we get J_cv of w2, b2 and all the way down to J_cv of w_10, b_10.
- Then in order to choose a model, we will look at which model has the lowest cross-validation error, and concretely, let's say that J_cv of w4, b4 as low as, then that means is we pick the 4th order polynomial as the model we will use for the application.
- Finally, if we want to report out an estimate of the generalization error of how well this model will do on new data. We will do so using the 3rd subset of our data, the test set and we report out J test of w4, b4. We notice that throughout the entire procedure, we had fit these parameters using the training set.
- We then choose the parameter d or chose the degree of polynomial using the cross-validation set and so up until this point, we have not fit any parameters, either w or b or d to the test set and that's why Jtest in this example will be fair estimate of the generalization error of this model this parameter w4, b4.
- This gives a better procedure for model selection and it lets you automatically make a decision like what order polynomial to choose for the linear regression model. This model selection procedure also works for choosing among other types of models.

<img width="1914" height="951" alt="image" src="https://github.com/user-attachments/assets/61c0bf40-ac5a-48af-96ab-3dfafcdaacf1" />

- For example, choosing a neural network architecture. if we are fitting a model for handwritten digit recognition, we might consider three models as given, maybe even a larger set of models than just 3 but here are a few different neural networks of small, somewhat larger, and then even larger.
- To help us decide how many layers do the neural network have and how many hidden units per layer should we have, we can then train all three of these models an end up with parameters w1, b1 for the first model w2, b2 for the 2nd model and w3, b3 for the 3rd model.
- We can then evaluate the neural networks performance using Jcv, the most common choice would be to compute this as the fraction of cross-validation examples that the algorithm has misclassified. We would compute this using all three models and then pick the model with the lowest cross validation error.
- If in this example, the 2nd model has the lowest cross validation error, we will then pick the 2nd neural network and use parameters trained on this model and finally, If we want to report out an estimate of the generalization error, we then use the test set to estimate how well the neural network that we just chose will do.
- It's considered best practice in Machine learning that if we have to make decisions about the model, such as fitting parameters or choosing the model architecture, such as neural network architecture or degree of polynomial if we are fitting a linear regression, to make all those decisions only using our training set and the cross validation set, and not to look at the test set at all while we are still making decisions regarding the learning algorithm.
- It's only after we have come up with one model as our final model to only then evaluate it on the test set and because we haven't made any decisions using the test set, that ensures that our test set is a fair and not overly optimistic estimate of how well our model will generalize to new data.

<img width="1916" height="950" alt="image" src="https://github.com/user-attachments/assets/09857319-bac1-439d-97c3-41ba942d84fa" />

---
