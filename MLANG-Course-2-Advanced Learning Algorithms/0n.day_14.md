## Deciding what to try next revisited

- You've seen how by looking at J train and Jcv, that is the training error and cross-validation error, or maybe even plotting a learning curve. You can try to get a sense of whether your learning algorithm has high bias or high variance. This is the procedure I routinely do when I'm training a learning algorithm more often look at the training error and cross-validation error to try to decide if my algorithm has high bias or high variance.
- It turns out this will help you make better decisions about what to try next in order to improve the performance of your learning algorithm.
- These were the six ideas that we had when we had looked over this slide earlier. Getting more training examples, try small set of features, additional features, and so on. It turns out that each of these six items either helps fix a high variance or a high bias problem. In particular, if your learning algorithm has high bias, three of these techniques will be useful
- First one is get more training examples. We saw in the last video that if your algorithm has high bias, then if the only thing we do is get more training data, that by itself probably won't help that much. But in contrast, if your algorithm has high variance, say it was overfitting to a very small training set, then getting more training examples will help a lot.
- How about trying a smaller set of features? Sometimes if your learning algorithm has too many features, then it gives your algorithm too much flexibility to fit very complicated models. This is a little bit like if you had x, x squared, x cubed, x^4, x^5, and so on
- If only you were to eliminate a few of these, then your model won't be so complex and won't have such high variance. If you suspect that your algorithm has a lot of features that are not actually relevant or helpful to predicting housing price, or if you suspect that you had even somewhat redundant features, then eliminating or reducing the number of features will help reduce the flexibility of your algorithm to overfit the data. This is a tactic that will help you to fix high variance.
- Conversely, getting additional features, that's just adding additional features is the opposite of going to a smaller set of features. This will help you to fix a high bias problem. As a concrete example, if you're trying to predict the price of the house just based on the size, but it turns out that the price of house also really depends on the number of bedrooms and on the number of floors and on the age of the house, then the algorithm will never do that well unless you add in those additional features.
- That's a high bias problem because you just can't do that well on the training set when you only know the size, it is only when you tell the algorithm how many bedrooms are there, how many floors are there? What's the age of the house that it finally has enough information to even do better on the training set. Adding additional features is a way to fix a high bias problem.
- Adding polynomial features is a little bit like adding additional features. If you're linear functions, three-line can fit the training set that well, then adding additional polynomial features can help you do better on the training set, and helping you do better on the training set is a way to fix a high bias problem.
- Then decreasing Lambda means to use a lower value for the regularization parameter. That means we're going to pay less attention to the regularization term and pay more attention to this term to try to do better on the training set. Again, that helps you to fix a high bias problem.
- Increasing Lambda will make sense if is overfitting the training set, just putting too much attention to fit the training set, but at the expense of generalizing to new examples, and so increasing Lambda would force the algorithm to fit a smoother function, may be less wiggly function and use this to fix a high variance problem.
- The takeaways I hope you have are, if you find that your algorithm has high variance, then the two main ways to fix that are; neither get more training data or simplify your model. By simplifying model I mean, either get a smaller set of features or increase the regularization parameter Lambda. Your algorithm has less flexibility to fit very complex, very wiggly curves.
- Conversely, if your algorithm has high bias, then that means is not doing well even on the training set. If that's the case, the main fixes are to make your model more powerful or to give them more flexibility to fit more complex or more wiggly functions.
- Some ways to do that are to give it additional features or add these polynomial features, or to decrease the regularization parameter Lambda.
- Anyway, in case you're wondering if you should fix high bias by reducing the training set size, that doesn't actually help. If you reduce the training set size, you will fit the training set better, but that tends to worsen your cross-validation error and the performance of your learning algorithm, so don't randomly throw away training examples just to try to fix a high bias problem

<img width="1889" height="943" alt="image" src="https://github.com/user-attachments/assets/a9a6a3de-ec30-44fe-9f7c-f2e0dfb9f33b" />
