## Neural Network Layer

- In the previous example of Demand Prediction, we had four input features that were set to the layer of three neurons in the hidden layer that then sends its output to this output layer with just one neuron.
- The hidden layer inputs 4 numbers and these 4 numbers are inputs to each of these three neurons. Each of these three neurons is just implementing a little logistic regression unit or a logistic regression function. Take this 1st neuron, It has two parameters, w and b. To denote that, this is the first hidden unit, we write it as w_1, b_1.
- It outputs some activation value a, which is g of w_1 in a product with x plus b_1, where this is familiar z value that we learned in the logistic regression and g of z is the familiar logistic funstion.
- The second neuron has the parameters w_2 and b_2 and is similar as w_1, b_1 but it gives out the avtivation value of a_2. Similarly it is with the third neuron and the activation value
- These three neurons and these vector of 3 numbers becomes the vector of acctivation values a, that is then passed to the final output layer of the neural network. Now, when we build the neural networks with multiple layers, it'll be useful to give the layers different numbers.
- The input layer is called layer 0 and today, there are neural networks that can have dozens or even hundreds of layers. we need to also maintain on how to denote these neurons of a given layer and that is shown in the diagram below.
- That's the computation of layer 1 of this neural network. Its output is the activation vector, a^[1] and it becomes the input to layer 2. 

![image](https://github.com/user-attachments/assets/fa57ba74-e692-420a-a373-933f4294e002)

---

- Now, let's zoom into the computation of layer 2 of this neural network, which is also the output layer. The input to layer 2 is the output of layer 1. Because the output layer has just a single neuron, all it does is it computes a_1 that is the output of this first and only neuron, as g, the sigmoid function applied to w_1 in a product with a^[1], so this is the input into this layer, and then plus b_1.
- The output of this neuron becomes the output layer of the neural network. Here, since we have the output layer as just a single neuron, this output is a scalar, is a single number rather than a vector of numbers.

![image](https://github.com/user-attachments/assets/8a3343f6-7ca8-4c82-b99c-39d8ab78452f)
---

- Once a neural network has computed a_2, there's one final optional step that you can choose to implement or not, which is if you want a binary prediction, 1 or 0, is this a top seller? Yes or no?
- Let's say that we get an output of 0.84 and threshold this at 0.5, if it's greater than 0.5, we can predict y hat equals 1 and if it is less than 0.5, then predict your y hat equals 0.

![image](https://github.com/user-attachments/assets/cbddb976-cf2a-472e-ba28-2250c28899a9)

- So that's how a neural network works. Every layer inputs a vector of numbers and applies a bunch of logistic regression units to it, and then computes another vector of numbers that then gets passed from layer to layer until you get to the final output layers computation, which is the prediction of the neural network. Then you can either threshold at 0.5 or not to come up with the final prediction.

---

## More Complex Neural Networks

- Last video, we learned about the neural network layer and how that takes the inputs a vector of numbers and in turn, outputs another vector of numbers. Let's use that layer to build a more complex neural network
- Here, we can see a network with 4 layers, not counting the input layer which is called the layer 0, where layers 1,2,3 are the hidden layers and the layer 4 is the output layer. This is a neural network with 4 layers in the conventional way of counting layers in the network.

![image](https://github.com/user-attachments/assets/def67e5e-a123-4a87-b3ae-ea2e24f52bf0)

- This is the image where we discuss about how we denote the neural networks.

![image](https://github.com/user-attachments/assets/2d1e43ab-4d14-42ff-948d-af61552cdc96)
---
![image](https://github.com/user-attachments/assets/6597069b-4d00-47b8-a071-a5c1cfd4adc0)

- Here, we look at the formula on how can we write the activation output value. The activation function is just that function that outputs these activation values. 
