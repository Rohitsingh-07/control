## Classification with Multiple Outputs (Optional)

- We learned about multi-class classification, where the output label Y can be any one of two or potentially many more than two possible categories. There's a different type of classification problem called a multi-lable classification problem, which is where associate of each image, they could be multiple labels.
- This is an example of multi-label classification problems because associated with a single input, image X are three different labels corresponding to whether or not there are any cars, buses, or pedestrians in the image. In this case, the target of the Y is actually a vector of three numbers, and this is as distinct from multi-class classification, where for, say handwritten digit classification, Y was just a single number, even if that number could take on 10 different possible values.

<img width="1902" height="921" alt="image" src="https://github.com/user-attachments/assets/90c5b58d-683a-4a60-92c9-88e31ecccae4" />

- One way to go ahead and build this kind of neural network for multi-label classification is to just treat this as three completely separate machine learning problems. We could build one neural network to decide are there any cars? The second one to detect buses and the third one to detect pedestrians. That's actually not an unreasonable approach.
- But there's another way to do this, which is to train a single neural network to simultaneously detect all three of cars, buses, and pedestrians, which is, if your neural networ architecture, looks like this, there's input X. First hidden layer offers a_1, second hidden layer offers a_2, and then the final output layer, in this case, we'll have three output neurons and we'll output a_3, which is going to be a vector of three numbers.
- Because we are solving three binary classification problems, so is there a car? is there a bus? is there a pedestrian? We can use a sigmoid activation function for each of these three nodes in the output layer, and so a_3 in this case will be a_1^3, a_2^3 and a_3^3, corresponding to whether or not the learning algorithm thinks it is a car, and no bus, and no pedestrians in the image.

<img width="1906" height="888" alt="image" src="https://github.com/user-attachments/assets/99b6ac43-8b3b-4293-a17c-32fb428b3696" />

- Multi-class classification and multi-label classification are sometimes confused with each other. 

---

## Optional Lab - Softmax Function

<img width="1163" height="634" alt="image" src="https://github.com/user-attachments/assets/3fc2fc91-b64f-416a-aeb3-bcfd07f4a59c" />

> **Note**: Normally, in this course, the notebooks use the convention of starting counts with 0 and ending with N-1,  $\sum_{i=0}^{N-1}$, while lectures start with 1 and end with N,  $\sum_{i=1}^{N}$. This is because code will typically start iteration with 0 while in lecture, counting 1 to N leads to cleaner, more succinct equations. This notebook has more equations than is typical for a lab and thus  will break with the convention and will count 1 to N.

```
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from IPython.display import display, Markdown, Latex
from sklearn.datasets import make_blobs
%matplotlib widget
from matplotlib.widgets import Slider
from lab_utils_common import dlc
from lab_utils_softmax import plt_softmax
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)
```
<img width="1254" height="644" alt="image" src="https://github.com/user-attachments/assets/343aac3c-f43b-42f0-9a84-9c12220a34c5" />

<img width="1203" height="525" alt="image" src="https://github.com/user-attachments/assets/dc069927-0a9d-4261-b7ac-77fa78b6dd24" />

```
def my_softmax(z):
    ez = np.exp(z)              #element-wise exponenial
    sm = ez/np.sum(ez)
    return(sm)
```
- A function used to define the softmax regression

<img width="1218" height="630" alt="image" src="https://github.com/user-attachments/assets/5be1b9ae-84e3-424d-ae1a-6e9b9d2b46f0" />

<img width="1145" height="601" alt="image" src="https://github.com/user-attachments/assets/1ad175d2-9cd9-4218-b599-fbfb09e206d6" />

<img width="1149" height="619" alt="image" src="https://github.com/user-attachments/assets/fdaeaff6-abce-4f31-87af-1a4175361f6c" />

```
# make  dataset for example
centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]
X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)
```

- The Obvious organization
  - The model below is implemented with the softmax as an activation in the final Dense layer.
  - The loss function is separately specified in the `compile` directive.
  - The loss function is `SparseCategoricalCrossentropy`. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities.
 
```
model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'softmax')    # < softmax activation here
    ]
)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.001),
)

model.fit(
    X_train,y_train,
    epochs=10
)

```

```
p_nonpreferred = model.predict(X_train)
print(p_nonpreferred [:2])
print("largest value", np.max(p_nonpreferred), "smallest value", np.min(p_nonpreferred))

Output:

[[1.92e-03 7.49e-03 9.72e-01 1.83e-02]
 [9.96e-01 3.60e-03 2.30e-05 1.18e-06]]
largest value 0.99999857 smallest value 2.5366103e-11
```
- This is to get the predictions

<img width="1173" height="384" alt="image" src="https://github.com/user-attachments/assets/16d9c5cf-2658-49a2-b597-bb3b53916e5d" />

- The preferred method

```
preferred_model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'linear')   #<-- Note
    ]
)
preferred_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note
    optimizer=tf.keras.optimizers.Adam(0.001),
)

preferred_model.fit(
    X_train,y_train,
    epochs=10
)

```

- Output Handling
  - Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability.
 
```
p_preferred = preferred_model.predict(X_train)
print(f"two example output vectors:\n {p_preferred[:2]}")
print("largest value", np.max(p_preferred), "smallest value", np.min(p_preferred))

Output:

two example output vectors:
 [[-1.55  0.02  5.34  1.25]
 [ 8.56  3.54 -0.13 -4.22]]
largest value 15.003374 smallest value -5.912451
```

```
sm_preferred = tf.nn.softmax(p_preferred).numpy()
print(f"two example output vectors:\n {sm_preferred[:2]}")
print("largest value", np.max(sm_preferred), "smallest value", np.min(sm_preferred))

Output:

two example output vectors:
 [[9.94e-04 4.78e-03 9.78e-01 1.64e-02]
 [9.93e-01 6.55e-03 1.67e-04 2.79e-06]]
largest value 0.99999857 smallest value 1.133282e-09
```

To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html).

```
for i in range(5):
    print( f"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}")

Output:
[-1.55  0.02  5.34  1.25], category: 2
[ 8.56  3.54 -0.13 -4.22], category: 0
[ 6.44  3.12 -0.13 -3.47], category: 0
[-1.02  4.58 -0.31 -0.99], category: 1
[ 1.16 -0.06  6.9  -0.16], category: 2
```

<img width="1185" height="482" alt="image" src="https://github.com/user-attachments/assets/97410495-1396-4df3-a954-8fbfcb3e619e" />

---

## Optional Lab - Multi-class Classification

<img width="1148" height="483" alt="image" src="https://github.com/user-attachments/assets/7e2cbd22-cc3c-4f94-82b7-aa3024cbccca" />

```
import numpy as np
import matplotlib.pyplot as plt
%matplotlib widget
from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
np.set_printoptions(precision=2)
from lab_utils_multiclass_TF import *
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)
```

<img width="1212" height="556" alt="image" src="https://github.com/user-attachments/assets/26095dc4-8b4e-47cd-809f-2dfa33032b93" />

```
# make 4-class dataset for classification
classes = 4
m = 100
centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]
std = 1.0
X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)
```

```
plt_mc(X_train,y_train,classes, centers, std=std)
```

<img width="1216" height="535" alt="image" src="https://github.com/user-attachments/assets/fc866554-f774-4b17-8f30-f6ef58604e10" />


```
# show classes in data set
print(f"unique classes {np.unique(y_train)}")
# show how classes are represented
print(f"class representation {y_train[:10]}")
# show shapes of our dataset
print(f"shape of X_train: {X_train.shape}, shape of y_train: {y_train.shape}")

Output:
unique classes [0 1 2 3]
class representation [3 3 3 0 3 3 3 3 2 0]
shape of X_train: (100, 2), shape of y_train: (100,)
```

<img width="1210" height="403" alt="image" src="https://github.com/user-attachments/assets/e222c759-2c10-4ec5-93ab-d2a9173ca59d" />

```
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(0.01),
)

model.fit(
    X_train,y_train,
    epochs=200
)
```
```
plt_cat_mc(X_train, y_train, model, classes)
```

<img width="1026" height="446" alt="image" src="https://github.com/user-attachments/assets/e95832b9-e57a-4483-a2e9-ad4ccee56e12" />

<img width="1239" height="607" alt="image" src="https://github.com/user-attachments/assets/a4e2388f-7310-415c-869f-a6417dfe9cba" />

```
# gather the trained parameters from the output layer
l2 = model.get_layer("L2")
W2, b2 = l2.get_weights()
# create the 'new features', the training examples after L1 transformation
Xl2 = np.maximum(0, np.dot(X_train,W1) + b1)

plt_output_layer_linear(Xl2, y_train.reshape(-1,), W2, b2, classes,
                        x0_rng = (-0.25,np.amax(Xl2[:,0])), x1_rng = (-0.25,np.amax(Xl2[:,1])))
```

<img width="1236" height="661" alt="image" src="https://github.com/user-attachments/assets/cbb9583a-b1ed-4fc8-afec-2e249f110b0a" />

<img width="1197" height="624" alt="image" src="https://github.com/user-attachments/assets/922e41b2-e698-4677-93f7-ca1f0eb4f369" />

<img width="1155" height="627" alt="image" src="https://github.com/user-attachments/assets/48afe614-0bd0-4453-996e-bd7be3397030" />
