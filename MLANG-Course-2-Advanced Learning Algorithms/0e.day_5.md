## Inference in Code

- Tensorflow is one of the leading frameworks to implementing deep learning algorithms. The other popular tool is Pytorch. One of the remarkable things about neural networks is the same algorithm can be applied to so many different applications
- We are going to take the coffee beans roasting algorithm. Can the learning algorithm help optimize the quality of the beans we get from a roasting process like this.
- While Roasting coffee beans, we can control two parameters. Temperature at which we'are heating as well as the duration
- In the diagram below, we see that the coffee we roasted is good or not. Cross here, the positive cross y equals 1 corresponds to good-tasting coffee, and all the circles correspond to bad coffee
- This shows that if the coffee is undercooked, overcooked or all the possibilities where the coffee could be bad tasting.

<img width="1870" height="908" alt="image" src="https://github.com/user-attachments/assets/26546c34-6b0c-49fe-97b2-2f2830884eaa" />

- There have actually been serious projects using Machine Learning to optimize the coffee roasting. The task is given a feature vector x with both temperature and duration, say 200 degrees Celsius for 17 minutes, how can we do inference in a neural network to get it to tell us whether or not this temperature and duration will result in a good coffee

---
- We are going to set x to be an aeeay of two numbers. The input fetures 200 degrees celsius and 17 minutes.
- Then we create Layer 1 as this first hidden layer, the neural network, as dense with units 3, which means three units or three hidden units in this layer using as the activation function, the sigmoid function.
- Next, we compute a1 by taking layer 1, which is exactly a function, and applying this function Layer 1 to the values of x. That's how we get a1, which is a goint to be a list of three numbers because Layer 1 had three units. So a1 here may be, 0.2, 0.7, 0.3.
- Next, for the 2nd hidden layer, Layer 2, would be dense. Now this time, it has one unit and again to sigmoid activation function, and you can then compute a2 by applying this Layer 2 function to the activation values from Layer 1 to a1.
- This will give the value of a2. If we wish to threshold it at 0.5, then we can just test if a2 is greater and equal to 0.5 and y-hat equals to one or zero positive or negative cross accordingly.
- That is how Inference is done in the neural network using Tensorflow.
- These are the key steps for forward propagation in how we compute a1 and a2 and optionally threshold a2.

<img width="1721" height="890" alt="image" src="https://github.com/user-attachments/assets/18e24a09-9b06-4a8e-9a98-c820c8d1a8a9" />

---

- We look at the example where we go back to handwritten digit classification problem. In this example, x is a list of pixel intensity values. So x is equal to a numpy array of this list of pixel intensity values. Then to initialize and carry out one step of forward propagation, Layer 1 is a dense layer with 25 units and the sigmoid activation function. We then compute a1 equals the layer 1 function applied to x.
- To build and carry out inference through the second layer, similarly, we set up Layer 2 as follows, and compute a2 as layer 2 applied to a1. Then finally, Layer 3 is the third and final dense layer.
- Then finally, you can optionally threshold a3 to come up with a binary prediction for y-hat. That's the syntax for carrying out inference in Tensorflow.

<img width="1857" height="925" alt="image" src="https://github.com/user-attachments/assets/ecdc6bef-e4ab-4aff-8ef9-f509a773735a" />

---

## Data in Tensorflow

- There are some inconsistencies between how data is represented in NumPy and Tensorflow.

<img width="1713" height="862" alt="image" src="https://github.com/user-attachments/assets/8a618cdf-4ea4-4e8a-bb3d-e05b913be510" />

- Above are the representation of the data in the Numpy arrays
- A matrix is a 2D array of numbers. We saw the matrix of 2 X 3 and 4 X 2. A matrix can also be other dimensions like 1 x 2 or 2 x 1.
- In course 1, we used the 1 D vectors to represent the input features x. With Tensorflow, the convention is to use matrices to represent the data. It turns out that tensorflow was designed to handle very large datasets and by representing the data in matrices instead of 1D arrays, it lets Tensorflow be a bit more computationally efficient internally.

<img width="1765" height="895" alt="image" src="https://github.com/user-attachments/assets/0a5de917-f9e2-41e7-b69b-9c5facf5c3c5" />

- The first one is a 1 x 2 matrix whereas the 2nd one is a 2 x 1 matrix whereas the 3rd one is a 1 D vector

<img width="1848" height="755" alt="image" src="https://github.com/user-attachments/assets/cf005176-c620-4520-afa0-482275f6477a" />

- This is the example that we are working towards

---

- A Tensor here is a datatype that the Tensorflow team had created in order to store and carry out computations on matrices efficiently
- When we compute a1 equals layer 1 applied to x, a1 is actually going to be a 1 x 3 matrix. If we print a1, we will get something like this is tf.tensor with a shape and its datatype.
- Technically, a tensor is a little bit more general than the matrix but for the purposes of this course, think of tensor as just a way of representing matrices.

<img width="1861" height="925" alt="image" src="https://github.com/user-attachments/assets/f57c2a83-1d6a-4159-a42d-80822036979a" />

- We can also convert a1 which is a tensor back into a Numpy array with the function .numpy() and it will return it in the form of a Numpy array rather than in the form of a Tensorflow array or Tensorflow Matrix.

- If we look at the activation output of the second layer. Here, layer 2 is a dense layer with one unit and sigmoid activation and a2 is computed by taking layer 2 and applying it to a1.

<img width="1751" height="922" alt="image" src="https://github.com/user-attachments/assets/97b77862-bd49-4911-a217-e63fe45339d9" />

- We again see the Tensor, the shape of the data and the similar pattern that we saw in the previous layer

---

## Building a Neural Network

- Previously we saw, that if we want to do forward propagation, we initialize the data X create layer one then compute a1, then create layer two and compute a2.

<img width="1829" height="629" alt="image" src="https://github.com/user-attachments/assets/b0ea4723-69b5-465c-be23-c64b7ebe9927" />

- When we create in Tensorflow, we are going to create layer 1 and layer 2. But now, instead of us manually taking the data and passing it to layer 1 and then taking the activations from layer 1 and pass it to layer 2. We can instead tell Tensorflow that we would like to take layer 1 and layer 2 and string them together to form a neural network. That's what the sequential function in Tensorflow does.
- If we have the data as presented in the left side of the diagram and we want to train the neural network, all we need to do is call to functions. In this case, call model.compile with some parameters. We also have a function model.fit which tells the tensorflow to take the neural network that are created by sequentially string together layers one and two, and to train it on the data, X and Y.

<img width="1837" height="908" alt="image" src="https://github.com/user-attachments/assets/e845f6bb-7173-4d05-917e-1e1f2017bb26" />

--- 

- By convention we don't assign the layers to two variables but we write it in a single code

<img width="1851" height="906" alt="image" src="https://github.com/user-attachments/assets/5cf5fd04-ac60-40ee-962e-095a2c5bf267" />

---

<img width="1875" height="877" alt="image" src="https://github.com/user-attachments/assets/28472e9b-21bb-4d6f-bc6a-fbf6c7990f2e" />

- Here we can see the Digit Classification model where the execution of the model is shown above, we use sequential, compile and predict functions to do the forward propagation and do inference

---

## Coffee Roasting in Tensorflow

<img width="1029" height="345" alt="image" src="https://github.com/user-attachments/assets/b5cf3218-ac13-4873-9b29-cac548c73834" />

```
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from lab_utils_common import dlc
from lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit
import logging
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.autograph.set_verbosity(0)
```
- We read about the functions and the packages above in the video before

```
X,Y = load_coffee_data();
print(X.shape, Y.shape)
```
- Loading the data
- Let's plot the coffee roasting data below. The two features are Temperature in Celsius and Duration in minutes. [Coffee Roasting at Home](https://www.merchantsofgreencoffee.com/how-to-roast-green-coffee-in-your-oven/) suggests that the duration is best kept between 12 and 15 minutes while the temp should be between 175 and 260 degrees Celsius. Of course, as temperature rises, the duration should shrink.

```
plt_roast(X,Y)
```

<img width="501" height="322" alt="image" src="https://github.com/user-attachments/assets/f663ec4c-4420-4435-aaf1-f3a3d3139d9b" />

##### Normalize Data
Fitting the weights to the data (back-propagation, covered in next week's lectures) will proceed more quickly if the data is normalized. This is the same procedure you used in Course 1 where features in the data are each normalized to have a similar range. 
The procedure below uses a Keras [normalization layer](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/). It has the following steps:
- create a "Normalization Layer". Note, as applied here, this is not a layer in your model.
- 'adapt' the data. This learns the mean and variance of the data set and saves the values internally.
- normalize the data.  
It is important to apply normalization to any future data that utilizes the learned model.

```
print(f"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}")
print(f"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}")
norm_l = tf.keras.layers.Normalization(axis=-1)
norm_l.adapt(X)  # learns mean, variance
Xn = norm_l(X)
print(f"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}")
print(f"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}")
```

- Basically here, we are normalizing the data and tryin to reduce the rane of the data so that it is easy to compute. We are using the Keras Normalization Layer
- .adapt() function lets the layer look at the data and figure out the average and the variance of the data and later we apply the normalization.

```
Xt = np.tile(Xn,(1000,1))
Yt= np.tile(Y,(1000,1))   
print(Xt.shape, Yt.shape)

Output: (200000, 2) (200000, 1)
```

- np.tile is used to basically repeat the data a bunch of times. In our case 1000 times. We now have a much larger dataset where the same data is duplicated 1000 times. https://chatgpt.com/s/t_689e67dd0a4c8191b92b351cf24f2746

- Tile/copy our data to increase the training set size and reduce the number of training epochs. https://chatgpt.com/s/t_689e6824c9688191b3c9ceb7429c41ee
- Epoch means one complete pass through the entire training dataset.

<img width="1131" height="284" alt="image" src="https://github.com/user-attachments/assets/9be50132-82ba-41cd-b0bf-9c058060deda" />

```
tf.random.set_seed(1234)  # applied to achieve consistent results
model = Sequential(
    [
        tf.keras.Input(shape=(2,)),
        Dense(3, activation='sigmoid', name = 'layer1'),
        Dense(1, activation='sigmoid', name = 'layer2')
     ]
)
```
<img width="1184" height="205" alt="image" src="https://github.com/user-attachments/assets/838a8f26-3573-4a4e-9533-557e3040c661" />

- The input helps in saying that the expected shape of the input is as given.
- The set_seed is used to set the initial weight everytime as the same number. https://chatgpt.com/s/t_689e69a7da788191a9e6da3040a1539a
- model.summary() provides a description of the data
<img width="876" height="291" alt="image" src="https://github.com/user-attachments/assets/f81c4254-00f2-4515-8ffe-7c744cb249a6" />

```
L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters
L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters
print("L1 params = ", L1_num_params, ", L2 params = ", L2_num_params  )
```

- These explain the number of parameters formula that we see above. It is (input x neurons) + (biases = neurons) https://chatgpt.com/s/t_689e6a9b38948191a4c0fc6ec48185b5

Let's examine the weights and biases Tensorflow has instantiated.  The weights $W$ should be of size (number of features in input, number of units in the layer) while the bias $b$ size should match the number of units in the layer:
- In the first layer with 3 units, we expect W to have a size of (2,3) and $b$ should have 3 elements.
- In the second layer with 1 unit, we expect W to have a size of (3,1) and $b$ should have 1 element.

```
W1, b1 = model.get_layer("layer1").get_weights()
W2, b2 = model.get_layer("layer2").get_weights()
print(f"W1{W1.shape}:\n", W1, f"\nb1{b1.shape}:", b1)
print(f"W2{W2.shape}:\n", W2, f"\nb2{b2.shape}:", b2)

Output:

W1(2, 3):
 [[ 0.08 -0.3   0.18]
 [-0.56 -0.15  0.89]] 
b1(3,): [0. 0. 0.]
W2(3, 1):
 [[-0.43]
 [-0.88]
 [ 0.36]] 
b2(1,): [0.]
```

- The `model.compile` statement defines a loss function and specifies a compile optimization.
- The `model.fit` statement runs gradient descent and fits the weights to the data.

```
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)

model.fit(
    Xt,Yt,            
    epochs=10,
)
```

- The model.compile tells the model how to learn. The loss function and the optimizer are given in the function. Adam is Adaptive Moment Estimation which is a popular optimization used in training neural networks, It's the lagorithm which decides on how to update the model's weights after each mistake
- Adam updates the weights, but it does that by also adjusting the effective learning rate for each weight individually
- model.fit starts the training and goes through the entire data the no of times the epoch is specified

<img width="1152" height="275" alt="image" src="https://github.com/user-attachments/assets/f685ce0d-a84f-475a-ad48-5a5774732d29" />

```
W1, b1 = model.get_layer("layer1").get_weights()
W2, b2 = model.get_layer("layer2").get_weights()
print("W1:\n", W1, "\nb1:", b1)
print("W2:\n", W2, "\nb2:", b2)

Output:

W1:
 [[ -0.13  14.3  -11.1 ]
 [ -8.92  11.85  -0.25]] 
b1: [-11.16   1.76 -12.1 ]
W2:
 [[-45.71]
 [-42.95]
 [-50.19]] 
b2: [26.14]
```

You can see that the values are different from what you printed before calling `model.fit()`. With these, the model should be able to discern what is a good or bad coffee roast.

For the purpose of the next discussion, instead of using the weights you got right away, you will first set some weights we saved from a previous training run. This is so that this notebook remains robust to changes in Tensorflow over time. Different training runs can produce somewhat different results and the following discussion applies when the model has the weights you will load below. 

Feel free to re-run the notebook later with the cell below commented out to see if there is any difference. If you got a low loss after the training above (e.g. 0.002), then you will most likely get the same results.

<img width="1157" height="340" alt="image" src="https://github.com/user-attachments/assets/f910a372-06af-4fb7-aa50-9706b92ab174" />

```
X_test = np.array([
    [200,13.9],  # positive example
    [200,17]])   # negative example
X_testn = norm_l(X_test)
predictions = model.predict(X_testn)
print("predictions = \n", predictions)

Outut:
predictions = 
 [[9.63e-01]
 [3.03e-08]]
```

```
yhat = np.zeros_like(predictions)
for i in range(len(predictions)):
    if predictions[i] >= 0.5:
        yhat[i] = 1
    else:
        yhat[i] = 0
print(f"decisions = \n{yhat}")

Output: decisions = 
[[1.]
 [0.]]
```

```
yhat = (predictions >= 0.5).astype(int)
print(f"decisions = \n{yhat}")

Output: decisions = 
[[1]
 [0]]
```

---

## Forward prop in a single layer

<img width="1852" height="912" alt="image" src="https://github.com/user-attachments/assets/dce07aec-a145-4ca3-ba5d-74d422d54b57" />

- This code was explained in the video which is pretty straightforward

---
## General Implementation of Forward Propagation

<img width="1855" height="924" alt="image" src="https://github.com/user-attachments/assets/3f199164-78d4-445a-9f23-e50b5c9a4a69" />

- This code was explained in the video which is pretty straight forward

---
## CoffeeRoastingNumpy

- The optional lab states what was in the vides before it and we saw it. Need to document it more
