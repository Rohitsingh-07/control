## Inference in Code

- Tensorflow is one of the leading frameworks to implementing deep learning algorithms. The other popular tool is Pytorch. One of the remarkable things about neural networks is the same algorithm can be applied to so many different applications
- We are going to take the coffee beans roasting algorithm. Can the learning algorithm help optimize the quality of the beans we get from a roasting process like this.
- While Roasting coffee beans, we can control two parameters. Temperature at which we'are heating as well as the duration
- In the diagram below, we see that the coffee we roasted is good or not. Cross here, the positive cross y equals 1 corresponds to good-tasting coffee, and all the circles correspond to bad coffee
- This shows that if the coffee is undercooked, overcooked or all the possibilities where the coffee could be bad tasting.

<img width="1870" height="908" alt="image" src="https://github.com/user-attachments/assets/26546c34-6b0c-49fe-97b2-2f2830884eaa" />

- There have actually been serious projects using Machine Learning to optimize the coffee roasting. The task is given a feature vector x with both temperature and duration, say 200 degrees Celsius for 17 minutes, how can we do inference in a neural network to get it to tell us whether or not this temperature and duration will result in a good coffee

---
- We are going to set x to be an aeeay of two numbers. The input fetures 200 degrees celsius and 17 minutes.
- Then we create Layer 1 as this first hidden layer, the neural network, as dense with units 3, which means three units or three hidden units in this layer using as the activation function, the sigmoid function.
- Next, we compute a1 by taking layer 1, which is exactly a function, and applying this function Layer 1 to the values of x. That's how we get a1, which is a goint to be a list of three numbers because Layer 1 had three units. So a1 here may be, 0.2, 0.7, 0.3.
- Next, for the 2nd hidden layer, Layer 2, would be dense. Now this time, it has one unit and again to sigmoid activation function, and you can then compute a2 by applying this Layer 2 function to the activation values from Layer 1 to a1.
- This will give the value of a2. If we wish to threshold it at 0.5, then we can just test if a2 is greater and equal to 0.5 and y-hat equals to one or zero positive or negative cross accordingly.
- That is how Inference is done in the neural network using Tensorflow.
- These are the key steps for forward propagation in how we compute a1 and a2 and optionally threshold a2.

<img width="1721" height="890" alt="image" src="https://github.com/user-attachments/assets/18e24a09-9b06-4a8e-9a98-c820c8d1a8a9" />

---

- We look at the example where we go back to handwritten digit classification problem. In this example, x is a list of pixel intensity values. So x is equal to a numpy array of this list of pixel intensity values. Then to initialize and carry out one step of forward propagation, Layer 1 is a dense layer with 25 units and the sigmoid activation function. We then compute a1 equals the layer 1 function applied to x.
- To build and carry out inference through the second layer, similarly, we set up Layer 2 as follows, and compute a2 as layer 2 applied to a1. Then finally, Layer 3 is the third and final dense layer.
- Then finally, you can optionally threshold a3 to come up with a binary prediction for y-hat. That's the syntax for carrying out inference in Tensorflow.

<img width="1857" height="925" alt="image" src="https://github.com/user-attachments/assets/ecdc6bef-e4ab-4aff-8ef9-f509a773735a" />

---

## Data in Tensorflow

- There are some inconsistencies between how data is represented in NumPy and Tensorflow.

<img width="1713" height="862" alt="image" src="https://github.com/user-attachments/assets/8a618cdf-4ea4-4e8a-bb3d-e05b913be510" />

- Above are the representation of the data in the Numpy arrays
- A matrix is a 2D array of numbers. We saw the matrix of 2 X 3 and 4 X 2. A matrix can also be other dimensions like 1 x 2 or 2 x 1.
- In course 1, we used the 1 D vectors to represent the input features x. With Tensorflow, the convention is to use matrices to represent the data. It turns out that tensorflow was designed to handle very large datasets and by representing the data in matrices instead of 1D arrays, it lets Tensorflow be a bit more computationally efficient internally.

<img width="1765" height="895" alt="image" src="https://github.com/user-attachments/assets/0a5de917-f9e2-41e7-b69b-9c5facf5c3c5" />

- The first one is a 1 x 2 matrix whereas the 2nd one is a 2 x 1 matrix whereas the 3rd one is a 1 D vector

<img width="1848" height="755" alt="image" src="https://github.com/user-attachments/assets/cf005176-c620-4520-afa0-482275f6477a" />

- This is the example that we are working towards

---

- A Tensor here is a datatype that the Tensorflow team had created in order to store and carry out computations on matrices efficiently
- When we compute a1 equals layer 1 applied to x, a1 is actually going to be a 1 x 3 matrix. If we print a1, we will get something like this is tf.tensor with a shape and its datatype.
- Technically, a tensor is a little bit more general than the matrix but for the purposes of this course, think of tensor as just a way of representing matrices.

<img width="1861" height="925" alt="image" src="https://github.com/user-attachments/assets/f57c2a83-1d6a-4159-a42d-80822036979a" />

- We can also convert a1 which is a tensor back into a Numpy array with the function .numpy() and it will return it in the form of a Numpy array rather than in the form of a Tensorflow array or Tensorflow Matrix.

- If we look at the activation output of the second layer. Here, layer 2 is a dense layer with one unit and sigmoid activation and a2 is computed by taking layer 2 and applying it to a1.

<img width="1751" height="922" alt="image" src="https://github.com/user-attachments/assets/97b77862-bd49-4911-a217-e63fe45339d9" />

- We again see the Tensor, the shape of the data and the similar pattern that we saw in the previous layer
