## Alternatives to the sigmoid activation

- So far, we have been using the sigmoid activation function in all the nodes in the hidden layers and in the output layer. We did that because we were building up neural networks by taking logistic regression and creating a lot of logistic regression units and string them together.
- But there are other neural networks which can be used to make our neural network much more powerful
- Let's go back to the demand prediction example where price, shipping cost, marketing, material were given and we would try to predict if something is highly affordable. If there's good awareness and high perceived quaity and based on that we try to predict if it was a top seller.
- But this assumes that awareness is maybe binary and people either are aware or they are not. However, it seems that the degree to which possible buyers are aware of the t shirt we are selling may not be binary, they can be a little bit aware, somewhat aware, extremely aware or it could have gone completely viral.
- So rather than modeling awareness as a binary number 0, 1 that we try to estimate the probability of awareness or rather than modeling awareness is just a number between 0 and 1. Maybe awareness should be any non negative number because there can be any non negative value of awareness going from 0 up to really large numbers.
- So whereas previously we had used this equation to calculate the activation of that 2nd hidden unit estimating awareness where g was the sigmoid function and just goes between 0 and 1. If we want the activation function to potentially take on much larger positive values, we can instead swap in a different activation function. It turns out to be a really common choice of activation function in neural networks.
- It goes if z < 0, then g(z) is to the left and then there's this straight line 45 degree to the right of 0. And so when z >= 0, g(z) is just equal to z. The mathematical function for this g(z) = max(0,z)
- This reactivation function goes by the name ReLU and it stands for Rectified Linear Unit.

<img width="1913" height="923" alt="image" src="https://github.com/user-attachments/assets/b2cdf17c-299e-4d29-b7f9-c227980a67a2" />

- We are seeing three differenct activation functions, the sigmoid, ReLU and the linear activation function. Linear activation function is also said to be no activation function

<img width="1893" height="911" alt="image" src="https://github.com/user-attachments/assets/5931bbb7-8bab-4bd4-86a1-ea19bfeee989" />

---

## Choosing activation functions

- We are going to look at how we can choose the activation function for different neurons in the neural network. We will start with the output layer.
- It depends on what the target label or the ground truth label y is. We can choose different activation functions for differenet neurons in the neural network, and when considering the activation function for the output layer, it turns out that there'll  often be one fairly natural choice, depending on what is the target or the gorund truth label y.
- Specifically, if we are working on a classification problem where y is either 0 or 1, so a binary classification problem, then the sigmoid activation function will almost always be the most natural choice, because then the neural network learns to predict the probability that y = 1, just like we had for logistic regression. It is recommended to use a sigmoid function while working on a binary classification problem.
- Alternatively, if we are solving a regression problem, then we might choose a different activation function. In case when the output label y is probable to be a number and can be between a negative and a positive number, in that case, it's best to use the linear activation function.
- Finally, if y can only take on non-negative values, such as if we are predicting the price of the house, that can never be negative, then the most natural choice will be the ReLU activation function because this activation function only takes on non-negative values, either 0 or positive values.

<img width="1913" height="930" alt="image" src="https://github.com/user-attachments/assets/33f3fb0a-9694-43be-aced-6306b18113d5" />

- In case of the hidden layers of a neural network, it turns out that the ReLU activation function is by far the most common choice in modern neural network training.
- Initially, sigmoid function was used quite often but now, ReLU function is the one which used more
- The one exception that we do use a sigmoid activation function in the output layer if we have a binary classification problem.
- If we compare the ReLU and the sigmoid activaion functions, the ReLU is a bit faster to compute because it just requires computing max of 0,z, whereas the sigmoid requires taking an exponentiation and then an inverse and so on, and hence it is a little bit less efficient.
- The 2nd reason which turns out to be even more important is that the ReLU function goes flat only in one part of the graph; here on the left is completely flat, whereas the sigmoid activation function, it goes flat in two places. It goes flat to the left of the graph and it goes flat to the right of the graph
- If we are using a gradient descent to train a neural network, then when we have a function that is flat in a lot of places, gradient descent would be really slow.
- The gradient descent optimizes the cost function J of W,B rather than optimizes the activation function, but the activation function is a piece of what goes into computing, and that results in more places in the cost function J of W,B that are flats as well and with a small gradient and it slows down learning.
- Researchers have found that using the ReLU activation function can cause the neural network to learn a bit faster as well, which is why ReLU activation function is the most common choice

<img width="1913" height="918" alt="image" src="https://github.com/user-attachments/assets/f48312a5-0cbd-496e-87c1-b19e6715e365" />

- To summarize, for the output layer, use a sigmoid function, if we have a binary classification problem. Linear if y is a number that can take on positive and negative values or use ReLU if y can take on only positive values or zero positive values or non-negative values.
- Then for the hidden layers, I would recommend just using ReLU as a default activation function.

<img width="1901" height="930" alt="image" src="https://github.com/user-attachments/assets/f3cca131-5ce5-4707-8746-aa9188e20338" />

- There are also some other activation functions such as the tan h activation function or the LeakyReLU activation function or the swish activation function. 
