## Alternatives to the sigmoid activation

- So far, we have been using the sigmoid activation function in all the nodes in the hidden layers and in the output layer. We did that because we were building up neural networks by taking logistic regression and creating a lot of logistic regression units and string them together.
- But there are other neural networks which can be used to make our neural network much more powerful
- Let's go back to the demand prediction example where price, shipping cost, marketing, material were given and we would try to predict if something is highly affordable. If there's good awareness and high perceived quaity and based on that we try to predict if it was a top seller.
- But this assumes that awareness is maybe binary and people either are aware or they are not. However, it seems that the degree to which possible buyers are aware of the t shirt we are selling may not be binary, they can be a little bit aware, somewhat aware, extremely aware or it could have gone completely viral.
- So rather than modeling awareness as a binary number 0, 1 that we try to estimate the probability of awareness or rather than modeling awareness is just a number between 0 and 1. Maybe awareness should be any non negative number because there can be any non negative value of awareness going from 0 up to really large numbers.
- So whereas previously we had used this equation to calculate the activation of that 2nd hidden unit estimating awareness where g was the sigmoid function and just goes between 0 and 1. If we want the activation function to potentially take on much larger positive values, we can instead swap in a different activation function. It turns out to be a really common choice of activation function in neural networks.
- It goes if z < 0, then g(z) is to the left and then there's this straight line 45 degree to the right of 0. And so when z >= 0, g(z) is just equal to z. The mathematical function for this g(z) = max(0,z)
- This reactivation function goes by the name ReLU and it stands for Rectified Linear Unit.

<img width="1913" height="923" alt="image" src="https://github.com/user-attachments/assets/b2cdf17c-299e-4d29-b7f9-c227980a67a2" />

- We are seeing three differenct activation functions, the sigmoid, ReLU and the linear activation function. Linear activation function is also said to be no activation function

<img width="1893" height="911" alt="image" src="https://github.com/user-attachments/assets/5931bbb7-8bab-4bd4-86a1-ea19bfeee989" />
