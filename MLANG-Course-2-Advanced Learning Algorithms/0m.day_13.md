# Bias and Variance

## Diagnosing bias and variance

- The typical workflow of developing a machine learning system is that we have an idea and we train the model. Looking at the bias and variance of a learning algorithm gives us very good guidance on what to try next.
- We can think of the example we used to explain underfitting and overfitting of an algorithm on the dataset. Here, we can see that trying to fit a straight line doesn't work on the data. If we were to fit a 4th order polynomial, then it has high variance or it overfits.
- In the middle, if we fit a quadratic polynomial, then it looks pretty good. Since this is a problem with just a single feature x, we could plot the function f and look at it in the graph. But if we had more features, we can't plot f and visualize whether it's doing well as easily. Instead of trying to look at plots like this, a more systematic way to diagnose or to find out if our algorithm has high bias or high variance will be to look at the performance of our algorithm on the training set and on the cross validation set.
- If we were to compute J_train for the example on the left, the algorithm would not do good on the training set. J_train herewould be high because there are actually pretty large errors between the examples and the actual predictions of the model. J_cv would be high if we had a few new examples, the examples which the algorithm had not previously seen.
- One characteristic of an algorithm with high bias, something that is under fitting, is that it's not even doing that well on the training set. When J_train is high, it is a strong indicator that the algorithm has high bias.

<img width="632" height="903" alt="image" src="https://github.com/user-attachments/assets/8da4dfce-7988-4889-9629-0f9c54cccbeb" />

---

<img width="683" height="845" alt="image" src="https://github.com/user-attachments/assets/f2f0e079-5dff-4336-9377-cd0108909da4" />

- Let's now look at the example on the right. If we were to compute J_train, the algorithm is actually doing great on the training set. Fits the training data really well. J_train here will be low. But, if we were to evaluate this model on other houses not in the training set, then we find that J_cv, the cross validation error will be quite high.
- A characteristic signature that our algorithm has high variance will be that J_cv is much higher than J_train. In other words, it does much better on data it has seen than on data it has not seen. This turns out to be a strong indicator that the algorithm has high variance.
- Again, the point of what we are doing is that I'm computing J_train and J_cv and seeing that if J_train is high or if J_cv is much higher than J_train. This gives us a sense, even if we can't plot the function f, of whether the algorithm has high bias or high variance.

---

<img width="574" height="832" alt="image" src="https://github.com/user-attachments/assets/bb1a4076-98b5-402c-a0f9-42aefb43f01b" />

- Finally, the case in the middle. If we look at J_train, it's pretty low, so this is doing quite well on the training set. If we were to look at a few new examples, like those from, say, cross validation set, we find that J_cv is also pretty low. J_train not being high indicates that this doesn't have a high bias problem and J_cv not being much worse than J_train indicates that this doesn't have a high bias problem either.
- This is why the quadractic model seems to be pretty good one for this appliation. To summarize, when d equals 1 for a linear polynomial, J_train was high and J_cv was high. When d equals 4, J_train was low, but J_cv is high. When d equals 2, both were pretty low

<img width="1898" height="944" alt="image" src="https://github.com/user-attachments/assets/2b4cad61-95cb-455d-80b7-e95d281d8427" />

---

<img width="1832" height="916" alt="image" src="https://github.com/user-attachments/assets/fe06f9d3-129b-485d-964e-420db834909e" />


- Let's look at how J_train and J_cv vary as a function of the degree of the polynomial we are fitting. Let the horizontal axis be the degree of polynomial that we're fitting the data. Over on the left we'll correspond to a small value of d like d = 1, which corresponds to fitting straight line. Over to the right, we'll correspond to, say, d = 4 or even higher values of d.
- We are fitting this high order polynomial. so, if we were to plot J train or W, B as a function of the degree of polynomial, what we find is that as we fit a higher and higher degree polynomial. Here, it's assumed that we're not using regularization, but as we fit a higher and higher order polynomial, the training error will tend to go down because when we have a very simple linear function, it doesn't fit the training data that well.
- When we fit a quadratic function or third order polynomial or 4th order polynomial, it fits the training data better and better. As the degree of polynomial increases, J train will typically go down.
- Next, let's look at J_cv which is how well does it do on data that it did not get to fit to? What we saw was when d = 1, when the degree of polynomial was very low, J_cv was pretty high because it underfits, so it didn't do well on the cross validation set. Here, on the right as well, when the degree of polynomial is very large, say 4, it doesn't do well on the cross-validation set either, and so it's also high.
- But if d was in between say, a 2nd order polynomial then it actually did much better. If we were to vary the degree of polynomial, we would actually get a curve that looks like this, which comes down and then goes back up. Where if the degree of polynomial is too low, it underfits and so doesn't do well on the cross validation set, if it's too high, it overfits and also doesn't do well on the cross validation set.
- It is only somewhere in the middle, that is just right, which is why the 2nd order polynomial in our example ends up with a lower cross-validation error and neither high bias nor high variance.

---

<img width="1907" height="944" alt="image" src="https://github.com/user-attachments/assets/ece872e6-d14a-4f86-a6cd-b1b88dfb1da4" />

- In order to diagnose bias and variance in our learning algorithm, if our learning algorithm has high bias or it has undefeated data, the key indicator will be if J_train is high. That corresponds to the leftmost position of the curve, which is where J_train is high. Usually we will have J_train and J_cv close to each other. To diagnose high variance, the key indicator will be if J_cv is much greater than J_train
- The rightmost portion of the plot is where J_cv is much greater than J_train. This is what happens when we had fit a very high order polynomial to this small dataset. Eve though we've seen bias in the areas, it turns out, in some cases, is possible to simultaneously have high bias and have high-variance.
- We won't see this happen that much for linear regression, but it turns out that if we are training a neural network, there are some applications where unfortunately we have high bias and high variance. One way to recognize that situation will be if J_train is high, so we are not doing that well on the training set, but even worse, the cross-validation error is again, even much larger than the training set. The notion of high bias and high variance doesn't really happen for linear models applied to 1D.
- But to give intuition about what it looks like, it would be as if for part of the input, we had a very complicated model that overfit, so it overfits to part of the inputs. But for some reason, for other parts of the input, it doesn't even fit the training data well and so it underfits for part of the input.
- In this example, which looks artificial because it's a single feature input, we fit the training set really well and we overfit in part of the input, and we don't even fir the training data well, and we underfit the part of the input.
- That's how in some applications we can unfortunately end up with both high bias and high variance. The indicator for that will be if the algorithm does poorly on the training set, and it even does much worse than on the training set.
- For most learning applications, we probably have primarily a high bias or high variance problem rather than both at the same time. But it is possible sometimes they're both at the same time.

---

- The key takeaways are high bias means the algorithm is not even doing well on the training set, and high variance means, it does much worse on the cross validation set than the training set. Try to figure out to what extent the algorithm has a high bias or underfitting versus a high-variance when overfitting problem.

---

## Regularization and bias/variance

- We saw in the last video how different choices of the degree on polynomial D affects the bias in variance of our learning algorithm and therefore its overall performance. Let's take a look at how regularization, specifically the choice of the regularization parameter Lambda affects the bias and variance and therefore the overall performance of the algorithm.
- This will be helpful fo when we want to choose a good value of Lambda of the regularization parameter for the algorithm. In this example, we are going to use a 4th order polynomial, but we're going to fit this model using regularization. Where the value of Lambda is the regularization parameter that controls how much we trade-off keeping the parameters w small versus fitting the training data well
- If we kept lambda a very large value, say Lambda is equal to 10,000. If we were to do so, then the learning algorithm is highly motivated to keep these parameters w very small and we end up with w_1, w_2, really all of these parameters to be very close to 0.
- The model ends up being f of x is just approximately b a constant value, which is why we end up with a model as shown here. This model clearly has high bias and it underfits the data because it doesn't do well on the training set and J_train is large. Let's say we set lambda to be a very small value.
- With a small value of lambda, in fact, let's go to extreme of setting Lambda equals 0. With that choice of Lambda, there is no regularization, so we're just fitting a 4th order polynomial with no regularization and we end up with a curve that overfits the data.
- What we saw previously was when we have a model like this, J_train is small, but J_cv is much larger than J_train or J_cv is large. This indicates we have high variance and it overfits this data. It would be if we have some intermediate value of Lambda, not really largely 10,000 but not so small as zero that hopefully we get a model that is just right and fits the data well with small J_train and small J_cv.
- If we are trying to decide what is a good value of Lambda to use for the regularization parameter, cross-validation gives us a way to do so well.

<img width="1916" height="945" alt="image" src="https://github.com/user-attachments/assets/e6a4f418-a14c-47ac-8cff-f9e98208b993" />

---

- We are trying to figure out that if we are fitting a 4th order polynomial, how can we choose a good value of Lambda?
- This would be procedures similar to what we had seen for choosing the degree of polynomial D using cross-validation. Specifically, let's say we try to fit a model using Lambda = 0. We would minimize the cost function using lambda = 0 and end up with some parameter w1, b1 and we can then compute the cross-validation error, J_cv of w1, b1. Now, let's try a different value of lambda.
- Let's say we try Lambda = 0.01. Then again, minimizing the cost function gives us a 2nd set of parameters, w2, b2 and we can also see how well that does on the cross-validation set and so on.
- When we keep doubling it down, we end up with Lambda approximately = 10, and that will give parameters w12, b12 and J_cv.
- By trying out a large range of possible values for lambda, fitting parameters using those different regularization parameters, and then evaluating the performance on the cross validation set, we can then try to pick what is the best value for the regularization parameter.
- In this example, if we find that J_cv of W5, B5 has the lowest value of all of these different cross-validation errors, we might decide to pick that value of lambda and it's corresponding parameters w,b as the chosen parameters. 
- Finally, if we want to report out an estimate of the generalization error, we would then report out the test set error, J tests of W5, B5.

<img width="1899" height="945" alt="image" src="https://github.com/user-attachments/assets/5b691c26-8e92-4f16-9e1c-24aa17f93ab1" />

---

- To further hone intuition about what this algorithm is doing, let's take a look at how training error and cross validation error vary as a function of the parameter Lambda. In this figure, the extreme of Lambda equals 0 corresponds to not using any regularization, and so that's where we wound up with this very wiggly curve.
- If Lambda was small or it was even zero, and in that case, we have a high variance model, and so J train is going to be small and J_cv is going to be large because it does great on the training data but does much worse on the cross validation data. This extreme on the right were very large values of Lambda. Say Lambda equals 10,000 ends up with fitting a model that looks like that. This has high bias, it underfits the data, and it turns out J train will be high and J_cv will be high as well.
- In fact, if you were to look at how J train varies as a function of Lambda, you find that J train will go up like this because in the optimization cost function, the larger Lambda is, the more the algorithm is trying to keep W squared small. That is, the more weight is given to this regularization term, and thus the less attention is paid to actually do well on the training set.
- This term on the left is J train, so the most trying to keep the parameters small, the less good a job it does on minimizing the training error. That's why as Lambda increases, the training error J train will tend to increase like so.
- Turns out the cross-validation error will look like this. Because we've seen that if Lambda is too small or too large, then it doesn't do well on the cross-validation set.
- It either overfits here on the left or underfits here on the right. There'll be some intermediate value of Lambda that causes the algorithm to perform best. What cross-validation is doing is, it's trying out a lot of different values of Lambda.
- This is what we saw on the last slide; trial Lambda equals zero, Lambda equals 0.01, logic is 0,02. Try a lot of different values of Lambda and evaluate the cross-validation error in a lot of these different points, and then hopefully pick a value that has low cross validation error, and this will hopefully correspond to a good model for your application
- If you compare this diagram to the one that we had in the previous video, where the horizontal axis was the degree of polynomial, these two diagrams look a little bit not mathematically and not in any formal way, but they look a little bit like mirror images of each other, and that's because when you're fitting a degree of polynomial, the left part of this curve corresponded to underfitting and high bias, the right part corresponded to overfitting and high variance. Whereas in this one, high-variance was on the left and high bias was on the right.
- But that's why these two images are a little bit like mirror images of each other. But in both cases, cross-validation, evaluating different values can help you choose a good value of t or a good value of Lambda.

<img width="1918" height="934" alt="image" src="https://github.com/user-attachments/assets/cf0873c7-e71e-41a1-9750-441076e249b4" />

---

## Establishing a baseline level of performance

- Let's look at some concrete numbers for what J-train and JCV might be, and see how you can judge if a learning algorithm has high bias or high variance. Here, we are going to use as a running example the application of speech recognition.
- A lot of users doing web search on a mobile phone will use speech recognition rather than type on the tiny keyboards on our phones because speaking to a phone is often faster than typing. Typical audio that's a web search engine we get would be like this, "What is today's weather?" Or like this, "Coffee shops near me." It's the job of the speech recognition algorithms to output the transcripts whether it's today's weather or coffee shops near me.
- Now, if you were to train a speech recognition system and measure the training error, and the training error means what's the percentage of audio clips in your training set that the algorithm does not transcribe correctly in its entirety.
- Let's say the training error for this data-set is 10.8 percent meaning that it transcribes it perfectly for 89.2 percent of your training set, but makes some mistake in 10.8 percent of your training set. If you were to also measure your speech recognition algorithm's performance on a separate cross-validation set, let's say it gets 14.8 percent error
- If you were to look at these numbers it looks like the training error is really high, it got 10 percent wrong, and then the cross-validation error is higher but getting 10 percent of even your training set wrong that seems pretty high. It seems like that 10 percent error would lead you to conclude it has high bias because it's not doing well on your training set, but it turns out that when analyzing speech recognition it's useful to also measure one other thing which is what is the human level of performance?
- In other words, how well can even humans transcribe speech accurately from these audio clips? Concretely, let's say that you measure how well fluent speakers can transcribe audio clips and you find that human level performance achieves 10.6 percent error.
- Why is human level error so high? It turns out that for web search, there are a lot of audio clips that sound like this, "I'm going to navigate to [inaudible]." There's a lot of noisy audio where really no one can accurately transcribe what was said because of the noise in the audio. If even a human makes 10.6 percent error, then it seems difficult to expect a learning algorithm to do much better
- In order to judge if the training error is high, it turns out to be more useful to see if the training error is much higher than a human level of performance, and in this example it does just 0.2 percent worse than humans. Given that humans are actually really good at recognizing speech I think if I can build a speech recognition system that achieves 10.6 percent error matching human performance I'd be pretty happy, so it's just doing a little bit worse than humans. But in contrast, the gap or the difference between JCV and J-train is much larger.
- There's actually a four percent gap there, whereas previously we had said maybe 10.8 percent error means this is high bias. When we benchmark it to human level performance, we see that the algorithm is actually doing quite well on the training set, but the bigger problem is the cross-validation error is much higher than the training error which is why I would conclude that this algorithm actually has more of a variance problem than a bias problem

<img width="1815" height="936" alt="image" src="https://github.com/user-attachments/assets/4b6e4bca-9f94-4f3f-aea6-27deca853efd" />

---

- It turns out when judging if the training error is high is often useful to establish a baseline level of performance, and by baseline level of performance I mean what is the level of error you can reasonably hope your learning algorithm to eventually get to. One common way to establish a baseline level of performance is to measure how well humans can do on this task because humans are really good at understanding speech data, or processing images or understanding texts
- Human level performance is often a good benchmark when you are using unstructured data, such as: audio, images, or texts. Another way to estimate a baseline level of performance is if there's some competing algorithm, maybe a previous implementation that someone else has implemented or even a competitor's algorithm to establish a baseline level of performance if you can measure that, or sometimes you might guess based on prior experience.
- If you have access to this baseline level of performance that is, what is the level of error you can reasonably hope to get to or what is the desired level of performance that you want your algorithm to get to?

<img width="1865" height="852" alt="image" src="https://github.com/user-attachments/assets/9065ac53-2274-4e70-b3f7-1f606cae90ae" />

---

- Then when judging if an algorithm has high bias or variance, you would look at the baseline level of performance, and the training error, and the cross-validation error. The two key quantities to measure are then: what is the difference between training error and the baseline level that you hope to get to. This is 0.2, and if this is large then you would say you have a high bias problem.
- You will then also look at this gap between your training error and your cross-validation error, and if this is high then you will conclude you have a high variance problem. That's why in this example we concluded we have a high variance problem, whereas let's look at the second example. If the baseline level of performance; that is human level performance, and training error, and cross validation error look like this, then this first gap is 4.4 percent and so there's actually a big gap. The training error is much higher than what humans can do and what we hope to get to whereas the cross-validation error is just a little bit bigger than the training error.
- If your training error and cross validation error look like this, I will say this algorithm has high bias. By looking at these numbers, training error and cross validation error, you can get a sense intuitively or informally of the degree to which your algorithm has a high bias or high variance problem.
- Just to summarize, this gap between these first two numbers gives you a sense of whether you have a high bias problem, and the gap between these two numbers gives you a sense of whether you have a high variance problem. Sometimes the baseline level of performance could be zero percent. If your goal is to achieve perfect performance than the baseline level of performance it could be zero percent, but for some applications like the speech recognition application where some audio is just noisy then the baseline level of a performance could be much higher than zero.
- The method described on this slide will give you a better read in terms of whether your algorithm suffers from bias or variance. By the way, it is possible for your algorithms to have high bias and high variance.
- Concretely, if you get numbers like these, then the gap between the baseline and the training error is large. That would be a 4.4 percent, and the gap between training error and cross validation error is also large. This is 4.7 percent. If it looks like this you will conclude that your algorithm has high bias and high variance, although hopefully this won't happen that often for your learning applications.

<img width="1919" height="897" alt="image" src="https://github.com/user-attachments/assets/ca8485e6-e058-4aa0-9935-ea1cf0760b85" />

- To summarize, we've seen that looking at whether your training error is large is a way to tell if your algorithm has high bias, but on applications where the data is sometimes just noisy and is infeasible or unrealistic to ever expect to get a zero error then it's useful to establish this baseline level of performance.
- Rather than just asking is my training error a lot, you can ask is my training error large relative to what I hope I can get to eventually, such as, is my training large relative to what humans can do on the task? That gives you a more accurate read on how far away you are in terms of your training error from where you hope to get to.
- Then similarly, looking at whether your cross-validation error is much larger than your training error, gives you a sense of whether or not your algorithm may have a high variance problem as well.

---

## Learning Curves

- Learning curves are a way to help understand how your learning algorithm is doing as a function of the amount of experience it has, whereby experience, I mean, for example, the number of training examples it has.
- Let's take a look. Let me plot the learning curves for a model that fits a second-order polynomial quadratic function like so. I'm going to plot both J_cv, the cross-validation error, as well as J_train the training error. On this figure, the horizontal axis is going to be m_train. That is the training set size or the number of examples so the algorithm can learn from. On the vertical axis, I'm going to plot the error.
- By error, I mean either J_cv or J_train. Let's start by plotting the cross-validation error. It will look something like this. That's what J_cv of (w, b) will look like. Is maybe no surprise that as m_train, the training set size gets bigger, then you learn a better model and so the cross-validation error goes down. Now, let's plot J_train of (w, b) of what the training error looks like as the training set size gets bigger. It turns out that the training error will actually look like this
- That as the training set size gets bigger, the training set error actually increases. Let's take a look at why this is the case. We'll start with an example of when you have just a single training example. Well, if you were to fit a quadratic model to this, you can fit easiest straight line or a curve and your training error will be zero. How about if you have two training examples like this? Well, you can again fit a straight line and achieve zero training error. In fact, if you have three training examples, the quadratic function can still fit this very well and get pretty much zero training error, but now, if your training set gets a little bit bigger, say you have four training examples, then it gets a little bit harder to fit all four examples perfectly.
- You may get a curve that looks like this, is a pretty well, but you're a little bit off in a few places here and there. When you have increased, the training set size to four the training error has actually gone up a little bit. How about we have five training examples. Well again, you can fit it pretty well, but it gets even a little bit harder to fit all of them perfectly.
- We haven't even larger training sets it just gets harder and harder to fit every single one of your training examples perfectly. To recap, when you have a very small number of training examples like one or two or even three, is relatively easy to get zero or very small training error, but when you have a larger training set is harder for quadratic function to fit all the training examples perfectly. Which is why as the training set gets bigger, the training error increases because it's harder to fit all of the training examples perfectly.
- Notice one other thing about these curves, which is the cross-validation error, will be typically higher than the training error because you fit the parameters to the training set. You expect to do at least a little bit better or when m is small, maybe even a lot better on the training set than on the cross validation set.

<img width="1919" height="958" alt="image" src="https://github.com/user-attachments/assets/725babe5-4b86-48d6-9dd3-c7c56e8bf020" />

---

- Let's now take a look at what the learning curves will look like for an algorithm with high bias versus one with high variance. Let's start at the high bias or the underfitting case. Recall that an example of high bias would be if you're fitting a linear function, so curve that looks like this. If you were to plot the training error, then the training error will go up like so as you'd expect. In fact, this curve of training error may start to flatten out.
- We call it plateau, meaning flatten out after a while. That's because as you get more and more training examples when you're fitting the simple linear function, your model doesn't actually change that much more. It's fitting a straight line and even as you get more and more and more examples, there's just not that much more to change, which is why the average training error flattens out after a while
- Similarly, your cross-validation error will come down and also fattened out after a while, which is why J_cv again is higher than J_train, but J_cv will tend to look like that. It's because beyond a certain point, even as you get more and more and more examples, not much is going to change about the straight line you're fitting.
- It's just too simple a model to be fitting into this much data. Which is why both of these curves, J_cv, and J_train tend to flatten after a while.
- If you had a measure of that baseline level of performance, such as human-level performance, then they'll tend to be a value that is lower than your J_train and your J_cv. Human-level performance may look like this. There's a big gap between the baseline level of performance and J_train, which was our indicator for this algorithm having high bias
- That is, one could hope to be doing much better if only we could fit a more complex function than just a straight line. Now, one interesting thing about this plot is you can ask, what do you think will happen if you could have a much bigger training set?
- What would it look like if we could increase even further than the right of this plot, you can go further to the right as follows? Well, you can imagine if you were to extend both of these curves to the right, they'll both flatten out and both of them will probably just continue to be flat like that.
- No matter how far you extend to the right of this plot, these two curves, they will never somehow find a way to dip down to this human level performance or just keep on being flat like this, pretty much forever no matter how large the training set gets.
- That gives this conclusion, maybe a little bit surprising, that if a learning algorithm has high bias, getting more training data will not by itself hope that much. I know that we're used to thinking that having more data is good, but if your algorithm has high bias, then if the only thing you do is throw more training data at it, that by itself will not ever let you bring down the error rate that much.
- It's because of this really, no matter how many more examples you add to this figure, the straight linear fitting just isn't going to get that much better. That's why before investing a lot of effort into collecting more training data, it's worth checking if your learning algorithm has high bias, because if it does, then you probably need to do some other things other than just throw more training data at it.

<img width="1919" height="946" alt="image" src="https://github.com/user-attachments/assets/b7afd87e-8d35-41af-a9f4-4f72b3fc5ba4" />

---

- Let's now take a look at what the learning curve looks like for learning algorithm with high variance. You might remember that if you were to fit the fourth-order polynomial with small lambda, say, or even lambda equals zero, then you get a curve that looks like this, and even though it fits the training data very well, it doesn't generalize.
- Let's now look at what a learning curve might look like in this high variance scenario. J train will be going up as the training set size increases, so you get a curve that looks like this, and J cv will be much higher, so your cross-validation error is much higher than your training error. The fact there's a huge gap here is what I can tell you that this high-variance is doing much better on the training set than it's doing on your cross-validation set
- If you were to plot a baseline level of performance, such as human level performance, you may find that it turns out to be here, that J train can sometimes be even lower than the human level performance or maybe human level performance is a little bit lower than this. But when you're over fitting the training set, you may be able to fit the training set so well to have an unrealistically low error, such as zero error in this example over here, which is actually better than how well humans will actually be able to predict housing prices or whatever the application you're working on
- But again, to signal for high variance is whether J cv is much higher than J train. When you have high variance, then increasing the training set size could help a lot, and in particular, if we could extrapolate these curves to the right, increase M train, then the training error will continue to go up, but then the cross-validation error hopefully will come down and approach J train.
- So in this scenario, it might be possible just by increasing the training set size to lower the cross-validation error and to get your algorithm to perform better and better, and this is unlike the high bias case, where if the only thing you do is get more training data, that won't actually help you learn your algorithm performance much
- To summarize, if a learning algorithm suffers from high variance, then getting more training data is indeed likely to help. Because extrapolating to the right of this curve, you see that you can expect J cv to keep on coming down. In this example, just by getting more training data, allows the algorithm to go from relatively high cross-validation error to get much closer to human level performance.
- You can see that if you were to add a lot more training examples and continue to fill the fourth-order polynomial, then you can just get a better fourth order polynomial fit to this data than this very wiggly curve up on top.

<img width="1902" height="926" alt="image" src="https://github.com/user-attachments/assets/4ddd1047-9e42-48fe-8f9f-57eb3b846c0f" />


- If you're building a machine learning application, you could plot the learning curves if you want, that is, you can take different subsets of your training sets, and even if you have, say, 1,000 training examples, you could train a model on just 100 training examples and look at the training error and cross-validation error, then train a model on 200 examples, holding out 800 examples and just not using them for now, and plot J train and J cv and so on the repeats and plot out what the learning curve looks like. If we were to visualize it that way, then that could be another way for you to see if your learning curve looks more like a high bias or high variance one.
- One downside of the plotting learning curves like this is something I've done, but one downside is, it is computationally quite expensive to train so many different models using different size subsets of your training set, so in practice, it isn't done that often, but nonetheless, I find that having this mental visual picture in my head of what the training set looks like, sometimes that helps me to think through what I think my learning algorithm is doing and whether it has high bias or high variance.

