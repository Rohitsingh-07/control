# Bias and Variance

## Diagnosing bias and variance

- The typical workflow of developing a machine learning system is that we have an idea and we train the model. Looking at the bias and variance of a learning algorithm gives us very good guidance on what to try next.
- We can think of the example we used to explain underfitting and overfitting of an algorithm on the dataset. Here, we can see that trying to fit a straight line doesn't work on the data. If we were to fit a 4th order polynomial, then it has high variance or it overfits.
- In the middle, if we fit a quadratic polynomial, then it looks pretty good. Since this is a problem with just a single feature x, we could plot the function f and look at it in the graph. But if we had more features, we can't plot f and visualize whether it's doing well as easily. Instead of trying to look at plots like this, a more systematic way to diagnose or to find out if our algorithm has high bias or high variance will be to look at the performance of our algorithm on the training set and on the cross validation set.
- If we were to compute J_train for the example on the left, the algorithm would not do good on the training set. J_train herewould be high because there are actually pretty large errors between the examples and the actual predictions of the model. J_cv would be high if we had a few new examples, the examples which the algorithm had not previously seen.
- One characteristic of an algorithm with high bias, something that is under fitting, is that it's not even doing that well on the training set. When J_train is high, it is a strong indicator that the algorithm has high bias.

<img width="608" height="734" alt="image" src="https://github.com/user-attachments/assets/c8a03eb1-9831-4073-bf4b-f98a083f088f" />

---
