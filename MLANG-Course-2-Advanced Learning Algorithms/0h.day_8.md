## Multiclass

- Multiclass classification refers to classification problems where we can have more than just two possible output labels so not just 0 or 1. In case of the hand written digit classification problem, we were trying to determine whether the digit is 0 or 1, but when we would try to identify a zip code or read protocols, there could be 10 possible diigts that we might want to recon=gnize.
- So a multiclass classification problem is still a classification problem in that y we can take on only a small number of discrete categories is not any number, but now y can take on more than just two possible values.

<img width="1873" height="888" alt="image" src="https://github.com/user-attachments/assets/2884fc56-6ddf-4b25-bcbc-645ed23eb95c" />

- So, previously while building the classification, we may have had a data set with features x1 and x2. In this case, logistic regression would fit the model to estimate what the probability of y being 1, given the features x.
- With multiclass classification problems, we would instead have a dataset that maybe looks like below. Here, we have 4 classes where O, X, Square and Triangle all represent different classes. And instead of just estimating the chance of y being equal to 1, well now we want to estimate what's the chance that y is equal to 1, or what's the chance that y is equal to 2? Or what's the chance that y is equal to 3, ir the chance that y being equal to 4?
- It turns out that the algorithm we learn about in the next video can learn a decision boundary that mayb looks like this that divides the space exploded next to into fout categories rather than just two categories.
- Later, we are going to look at the softmax refression algorithm which is a generalization of the logistic regression algorithm and using that we will be able to carry out multiclass classification problems. And after that we'll take softmax regression and fit it into a new neural network so that we will also be able to train a neural network to carry out multiclass classification problems.

<img width="1837" height="930" alt="image" src="https://github.com/user-attachments/assets/7e0a5019-ad3f-4f99-ace5-1ff157d414f6" />

---

## Softmax

- The softmax regression alogrithm is a generalization of logistic regression, which is a binary classification algorithm to the multiclass classification contexts.
- Logistic regression works when y can take on two possible output values, either 0 or 1, and the way it computes this output is, you would first calculate z = w.x + b. Then we would compute a = g(z) which is a sigmoid function applied to z.
- We interpreted this as logistic regressions estimates of the probability of y being equal to 1 given those input features x.
- To embellish logistic regression a little bit in order to set us up for the generalization of softmax regression, we are going to think of logistic regression as computing two numbers: First a_1 which is the quatity that we had previously of the chance of y being equal to 1 given x, and second we are going to think of logistic regression as also computing a_2, which is 1 - a_1, which is just the chance of y being equal to zero given the input features x.
- Let's now generalize this to softmax regression, and we are going to do this with the example of when y can take on four possible outputs, so y can take on the values 1,2,3 or 4. The softmax regression will compute z_1 = w1.x + b_1 and then z_2 = w@.x + b_2 and so on for z3 and z4.
- Here w_1, w_2, w_3, w_4 as well as b_1, b_2, b_3, b_4, these are the parameters of the softmax regression. Next we'll compute the a_1 by the formula given below and it will be interepreted as the algorithms estimate of the chance of y being equal to 1 given the input features x. Then on similar terms, we will compute a_2 as the algorithms estimate of the chance that y is equal to 2 given the input features x. Similar for a_3 and a_4.

<img width="1883" height="926" alt="image" src="https://github.com/user-attachments/assets/a37c7fde-8636-4bb9-989a-456a3a89030d" />

- These equations are the specification for the softmax regression model. It has parameters w_1 through w_4, and b_1 through b_4, and if we can learn appropriate choices all these parameters, then this gives us a way of predicting what's the chance of y being 1,2,3 or 4, given a set of input features x.
- In case we take on n possible values, so y can be 1,2,3 and so on up to n. In that case, softmax regression will compute to z_j = w_j.x + b_j, where now the parameters of softmax regression are w_1, w_2 through w_n, as well as b_1, b_2 through b_n. Then we compute a_j

<img width="748" height="506" alt="image" src="https://github.com/user-attachments/assets/b30ab361-671e-4625-9714-3a4c16187da9" />

- If we apply softmax regression with n equals 2, so there are only two possible output classses then softmax regression ends up computing basically the same thing as logistic regression. The parameters end up being a little bit different, but it ends up reducing to logistic regression model. That's why the softmax regression model is the generalization of logistic regression.
- Having defined how softmax regression computes it's outputs, let's now take a look at how to specify the cost function for softmax regression. 
