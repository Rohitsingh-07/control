## Multiclass

- Multiclass classification refers to classification problems where we can have more than just two possible output labels so not just 0 or 1. In case of the hand written digit classification problem, we were trying to determine whether the digit is 0 or 1, but when we would try to identify a zip code or read protocols, there could be 10 possible diigts that we might want to recon=gnize.
- So a multiclass classification problem is still a classification problem in that y we can take on only a small number of discrete categories is not any number, but now y can take on more than just two possible values.

<img width="1873" height="888" alt="image" src="https://github.com/user-attachments/assets/2884fc56-6ddf-4b25-bcbc-645ed23eb95c" />

- So, previously while building the classification, we may have had a data set with features x1 and x2. In this case, logistic regression would fit the model to estimate what the probability of y being 1, given the features x.
- With multiclass classification problems, we would instead have a dataset that maybe looks like below. Here, we have 4 classes where O, X, Square and Triangle all represent different classes. And instead of just estimating the chance of y being equal to 1, well now we want to estimate what's the chance that y is equal to 1, or what's the chance that y is equal to 2? Or what's the chance that y is equal to 3, ir the chance that y being equal to 4?
- It turns out that the algorithm we learn about in the next video can learn a decision boundary that mayb looks like this that divides the space exploded next to into fout categories rather than just two categories.
- Later, we are going to look at the softmax refression algorithm which is a generalization of the logistic regression algorithm and using that we will be able to carry out multiclass classification problems. And after that we'll take softmax regression and fit it into a new neural network so that we will also be able to train a neural network to carry out multiclass classification problems.

<img width="1837" height="930" alt="image" src="https://github.com/user-attachments/assets/7e0a5019-ad3f-4f99-ace5-1ff157d414f6" />

---

## Softmax

- The softmax regression alogrithm is a generalization of logistic regression, which is a binary classification algorithm to the multiclass classification contexts.
- Logistic regression works when y can take on two possible output values, either 0 or 1, and the way it computes this output is, you would first calculate z = w.x + b. Then we would compute a = g(z) which is a sigmoid function applied to z.
- We interpreted this as logistic regressions estimates of the probability of y being equal to 1 given those input features x.
- To embellish logistic regression a little bit in order to set us up for the generalization of softmax regression, we are going to think of logistic regression as computing two numbers: First a_1 which is the quatity that we had previously of the chance of y being equal to 1 given x, and second we are going to think of logistic regression as also computing a_2, which is 1 - a_1, which is just the chance of y being equal to zero given the input features x.
- Let's now generalize this to softmax regression, and we are going to do this with the example of when y can take on four possible outputs, so y can take on the values 1,2,3 or 4. The softmax regression will compute z_1 = w1.x + b_1 and then z_2 = w@.x + b_2 and so on for z3 and z4.
- Here w_1, w_2, w_3, w_4 as well as b_1, b_2, b_3, b_4, these are the parameters of the softmax regression. Next we'll compute the a_1 by the formula given below and it will be interepreted as the algorithms estimate of the chance of y being equal to 1 given the input features x. Then on similar terms, we will compute a_2 as the algorithms estimate of the chance that y is equal to 2 given the input features x. Similar for a_3 and a_4.

<img width="1883" height="926" alt="image" src="https://github.com/user-attachments/assets/a37c7fde-8636-4bb9-989a-456a3a89030d" />

- These equations are the specification for the softmax regression model. It has parameters w_1 through w_4, and b_1 through b_4, and if we can learn appropriate choices all these parameters, then this gives us a way of predicting what's the chance of y being 1,2,3 or 4, given a set of input features x.
- In case we take on n possible values, so y can be 1,2,3 and so on up to n. In that case, softmax regression will compute to z_j = w_j.x + b_j, where now the parameters of softmax regression are w_1, w_2 through w_n, as well as b_1, b_2 through b_n. Then we compute a_j

<img width="748" height="506" alt="image" src="https://github.com/user-attachments/assets/b30ab361-671e-4625-9714-3a4c16187da9" />

- If we apply softmax regression with n equals 2, so there are only two possible output classses then softmax regression ends up computing basically the same thing as logistic regression. The parameters end up being a little bit different, but it ends up reducing to logistic regression model. That's why the softmax regression model is the generalization of logistic regression.
- Having defined how softmax regression computes it's outputs, let's now take a look at how to specify the cost function for softmax regression.
---

- In terms of the logistic loss, the loss if y = 1 is -log a1. If y = 0, then the loss is -log a2, and then same as before the cost function for all the parameters in the model is the average loss, average over the entire set which was a cost function for this regression.
- We first write the equations that we use for the softmax regression. The loss is also mentioned in the diagram below which is if the ground truth label is if y = 1, the loss = -log a1. Which means the negative log of the probability that it thought y was equal to 1, or if y is equal to 2, then I'm going to define as -log a2. Y is equal to 2.
- The loss of the algorithm on this example is negative log of the probability it's thought y was equal to 2. On all the way down to if y = n, then the loss is - log a_n.
- To illustrate, if y = j, then the loss is - log a_j. That's what this function looks like. Negative log of a_j is a curve that looks like this. If a_j was very close to 1, then the loss would be very small. The smaller the a_j is, the bigger the loss. This incentivizes the algorithm to make a_j as large as possible, as close to 1 as possible.
- Because whatever, the actual value y was, we want the algorithm to say hopefully that the chance of y being that value was pretty large. Notice that in this loss function, y in each training example can take on only one value.
- We end up computing this -log a_j only for one value of a_j, which is whatever was the actual value of y = j in that particular training example.

<img width="1912" height="939" alt="image" src="https://github.com/user-attachments/assets/b86ac861-82f7-41d9-8dc7-d9f0f25c437a" />

---

## Neural Network with Softmax Output

- In order to build a Neural that can carry out multi class classification, we are going to take the softmax regression model and put it into essentially the Output of the Neural Network.
- If we want to work on a handwritten digit recognition with 10 classes, then we're going to change this Neural Network to have 10 output units. And this new output layer will be a softmax output layer. So sometimes we'll say this Neural Network has a Softmax output or that this upper layer is a softmax layer
- And the way forward propogation (inference) works in this Neural Network is given an input X, A1(the activation or the output of a given layer) gets computed exactly the same as before. And then A2, the activation for 2nd hidden layer also gets computed the same as before. We used the ReLU function for the hidden layers.
- And now we have to compute the activations for the output layer. In case of Softmax regression, we will compute Z1, Z2 through Z10 using these expressions. Zn = Wn.X + b ( here x would be replaced with the activation of the 2nd hidden layer). Then we apply the formula for the softmax regression and that's out estimate of the chance that y = 1. Similarly from a2 to all the way upto a10.
- And this gives us the estimates of the chance of y being good to 1, 2 and so on up to through the 10th possible label for y.
- With this the softmax open layer now gives us estimates of the chance of y being any of these 10 possible output labels. The softmax layer will sometimes also called the Softmax activation function. We have seen before that in sigmoid, ReLU or linear activation functions, a1 was a function of Z1 and a2 was a function of Z2.
- In other words, to obtain the activation values, we could apply the activation function g be it sigmoid or ReLU or something else element wise to Z1 and Z2 and so on to get a1 and a2 and a3 and a4. With the softmax activation function, notice that a1 is a function of Z1 and Z2 and Z3 all the way up to Z10. So each of the activation values, depends on all the values of Z.
- And this is a property that's a bit unique to the Softmax output or the Softmax activation funct tion or stated differently if you want to compute a1 through a10,hat is a function of Z1 all the way up to Z 10 simultaneously. And this is unlike the other activation functions we've seen so far.

<img width="1893" height="939" alt="image" src="https://github.com/user-attachments/assets/8d76940d-95e2-46c4-829e-879635970000" />

- Similar to before, we are going to follow the three steps to specifying and training the model. The first step is to tell Tensorflow to string together three layers. Then we will tell Tensorflow to use the softmax activation function. The cost function that we saw in the previous video is called SparseCategoricalCrossentropy function. Here categorical is for the categories of the ouput and Sparse refers to that y can only take on one of these 10 values.
- So each image is either 0 or 1 or so on up to 9. We are not going to see a picture that is simultaneously the number 2 and the number 7 so sparse refers to that each digit is only one of these categories.

<img width="1848" height="937" alt="image" src="https://github.com/user-attachments/assets/dd07f0d8-d912-4a99-b574-7e0e33119097" />

---

## Improved Implementation of Softmax

- The implementation that we saw in the last video of a neural network with a softmax layer will work okay. But we are going to see an even better way to implement it.

<img width="1566" height="862" alt="image" src="https://github.com/user-attachments/assets/967058b1-fef4-496d-9b18-f9a35549991c" />

<img width="1709" height="901" alt="image" src="https://github.com/user-attachments/assets/797881b8-c6ef-43eb-85a3-1df11c9c34ff" />

- The 2nd implementation looks a little bit off as if there's some round-off error. Since the computer has only a finite amount of memory to store each number, called a floating-point number in this case, depending on how we decide to compute the value 2/10,000, the result can have more or less numerical round-off error.
- It turns out that while the way we have been computing the cost function for softmax is correct, there's a different way of formulating it that reduces these numerical round-off errors, leading to more accurate computations within Tensorflow.
- First, let's illustrate these ideas using logistic regression. Recall that for Logistic regression, if we want to compute the loss function for a given example, we would first compute this output activation a, the formula for the sigmoid function. Then we will compute the loss using the original loss function formula.

<img width="1914" height="848" alt="image" src="https://github.com/user-attachments/assets/3764708a-6223-4f9b-9a23-d7368ae05658" />

- For Logidtic Regression, this works good and usually the numerical round-off errors aren't that bad. But it turns out that if we allow TensorFlow to not have to compute a as an intermediate value. But instead, we expand the term a and write it in the given manner. Then Tendoflow can rearrange terms in this expression and come up with a more numerically accurate way to compute this loss function.
- Whereas, the original procedure was like insisting on computing a as an intermediate value
- This partial implementation was insisting on explicitly computing a as an intermediate quantity. But instead, by specifying this expression at the bottom directly as the loss function, it gives Tensorflow more flexibility in terms of how to compute this and whether or not it wants to compute a explicitly.
- The code is shown below and what this does is it sets the output layer to just use a linear activation function and it puts both the activation function, as well as the cross entropy loss into the specification of the loss function over here.

<img width="1879" height="931" alt="image" src="https://github.com/user-attachments/assets/4c9e15fe-2d23-49f7-bb0d-da9f1ee07494" />

- That's what the from_logits = true argument causes Tensorflow to do. logits is basically the number z. Tensorflow will compute z as an intermediate value, but it can rearrange terms to make this become computed more accurately. One downside of this code is it becomes a little bit less legible. But it causes Tensorflow to have a little bit less numerical roundoff error.
- In the case of logistic regression, either of these implementation actually works okay, but the numerical roundoff errors can get worse when it comes to softmax.
- As we saw in the last lecture, the softmax regression loss and the code is shown on the slide. Even in this case, if we expand the value of a in the loss function, then this gies Tensorflow the ability to rearrange terms and compute this integral numerically accurate.

<img width="1912" height="918" alt="image" src="https://github.com/user-attachments/assets/c54cfd44-9524-4d76-9b82-ddb42530f084" />

- It turns out if one of the z's really small, then e to negative small number becomes really small ot if one of the z's is a very large number, then e to the z can become a very large number and by rearranging terms, TensorFlow can avoid some of these very small or very large numbers and therefore come up with more accurate computation for the loss function.
- The code for doing this is shown here in the output layer, we're now using just a linear activation function so the output layer just computes z_1 through z_10 and this whole computation of the loss is then captured in the loss function. where again we have the from_logits equals true parameter.
- The numerical roundoff errors for logistic regression aren't that bad, but it is recommended that we use this implementation on the bottom instead.

<img width="1919" height="927" alt="image" src="https://github.com/user-attachments/assets/3835f36f-7ebe-494d-a6b7-65066f57570c" />

- There's just one more detail that we've now changed the neural network to use a linear activation function rather than a softmax activation function. The neural network's final layer no longer outputs these probabilities A_1 through A_10.

<img width="1891" height="917" alt="image" src="https://github.com/user-attachments/assets/62383bf7-a430-4c36-9621-aa3961cbfdbf" />

- We didn't talk about it in case of logistic regression, but if we were combining the output's logistic function with the loss function, then for the logistic regressions, we also have to change the code this way to take the output value and map it through the logistic function in order to actually get the probability.

<img width="1876" height="938" alt="image" src="https://github.com/user-attachments/assets/acd3535c-41ee-4b29-b5f2-2b2be7288e1b" />
