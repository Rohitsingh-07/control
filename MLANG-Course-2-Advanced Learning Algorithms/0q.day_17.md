# Decision Trees

## Decision Tree Model

- One of the learning algorithms that is very powerful, widely used in many applications is decision trees and tree ensembles. To explain how decision trees work, I'm going to use as a running example this week a cat classification example. You are running a cat adoption center and given a few features, you want to train a classifier to quickly tell you if an animal is a cat or not.
- I have here 10 training examples. Associated with each of these 10 examples, we're going to have features regarding the animal's ear shape, face shape, whether it has whiskers, and then the ground truth label that you want to predict this animal cat.
- The first example has pointy ears, round face, whiskers are present, and it is a cat. The second example has floppy ears, the face shape is not round, whiskers are present, and yes, that is a cat, and so on for the rest of the examples. This dataset has five cats and five dogs in it.
- The input features X are these three columns, and the target output that you want to predict, Y, is this final column of, is this a cat or not? In this example, the features X take on categorical values.
- In other words, the features take on just a few discrete values. Your shapes are either pointy or floppy. The face shape is either round or not round and whiskers are either present or absent. This is a binary classification task because the labels are also one or zero.
- For now, each of the features X_1, X_2, and X_3 take on only two possible values. We'll talk about features that can take on more than two possible values, as well as continuous-valued features later in this week.

<img width="1906" height="949" alt="image" src="https://github.com/user-attachments/assets/92ed8b47-b547-4a29-bbc3-d3749a6cc24b" />

- Here's an example of a model that you might get after training a decision tree learning algorithm on the data set that you just saw. The model that is output by the learning algorithm looks like a tree.
- Every one of these ovals or rectangles is called a node in the tree. The way this model works is if you have a new test example, she has a cat where the ear-shaped has pointy, face shape is round, and whiskers are present
- The way this model will look at this example and make a classification decision is will start with this example at this topmost node of the tree, this is called the root node of the tree, and we will look at the feature written inside, which is ear shape.
- Based on the value of the ear shape of this example we'll either go left or go right. The value of the ear-shape with this example is pointy, and so we'll go down the left branch of the tree, like so, and end up at this oval node over here.
- We then look at the face shape of this example, which turns out to be round, and so we will follow this arrow down over here. The algorithm will make a inference that it thinks this is a cat.
- You get to this node and the algorithm will make a prediction that this is a cat. What I've shown on this slide is one specific decision tree model. To introduce a bit more terminology, this top-most node in the tree is called the root node.
- All of these nodes, that is, all of these oval shapes, but excluding the boxes at the bottom, all of these are called decision nodes. They're decision nodes because they look at a particular feature and then based on the value of the feature, cause you to decide whether to go left or right down the tree.

<img width="1919" height="938" alt="image" src="https://github.com/user-attachments/assets/0401266d-2750-4b9c-a37d-14bb0993d328" />

- This is a different decision tree for trying to classify cat versus not cat. In this tree, to make a classification decision, you would again start at this topmost root node. Depending on their ear shape of an example, you'd go either left or right. If the ear shape is pointy, then you look at the whiskers feature, and depending on whether whiskers are present or absent, you go left or right to gain and classify cat versus not cat.
- Just for fun, here's a second example of a decision tree, here's a third one, and here's a fourth one. Among these different decision trees, some will do better and some will do worse on the training sets or on the cross-validation and test sets.
- The job of the decision tree learning algorithm is, out of all possible decision trees, to try to pick one that hopefully does well on the training set, and then also ideally generalizes well to new data such as your cross-validation and test sets as well

<img width="1898" height="946" alt="image" src="https://github.com/user-attachments/assets/8da2cdb6-39fd-4d87-9795-6ea316bd4e68" />

---

## Learning Process

- The process of building a decision tree given a training set has a few steps. In this video, let's take a look at the overall process of what you need to do to build a decision tree. Given a training set of 10 examples of cats and dogs like you saw in the last video. The first step of decision tree learning is, we have to decide what feature to use at the root node
- That is the first node at the very top of the decision tree. Via an algorithm that we'll talk about in the next few videos. Let's say that we decided to pick as the feature and the root node, the ear shape feature.
- What that means is we will decide to look at all of our training examples, all ten examples shown here, I split them according to the value of the ear shape feature. In particular, let's pick out the five examples with pointy ears and move them over down to the left
- Let's pick the five examples with floppy ears and move them down to the right. The second step is focusing just on the left part or sometimes called the left branch of the decision tree to decide what nodes to put over there.
- In particular, what feature that we want to split on or what feature do we want to use next. Via an algorithm that again, we'll talk about later this week. Let's say you decide to use the face shape feature there.
- What we'll do now is take these five examples and split these five examples into two subsets based on their value of the face shape. We'll take the four examples out of these five with a round face shape and move them down to the left.
- The one example with a not round face shape and move it down to the right. Finally, we notice that these four examples are all cats four of them are cats. Rather than splitting further, we've created a leaf node that makes a prediction that things that get down to that node of cats.
- Over here we notice that none of the examples zero of the one examples are cats or alternative 100 percent of the examples here are dogs. We can create a leaf node here that makes a prediction of not cat.
- Having done this on the left part to the left branch of this decision tree, we now repeat a similar process on the right part or the right branch of this decision tree. Focus attention on just these five examples, which contains one cat and four dogs
- We would have to pick some feature over here to use the split these five examples further, if we end up choosing the whiskers feature, we would then split these five examples based on where the whiskers are present or absent, like so.
- You notice that one out of one examples on the left for cats and zeros out of four are cats. Each of these nodes is completely pure, meaning that is, all cats or not cats and there's no longer a mix of cats and dogs. We can create these leaf nodes, making a cat prediction on the left and a notcat prediction here on the right. This is a process of building a decision tree.
- Through this process, there were a couple of key decisions that we had to make at various steps during the algorithm. Let's talk through what those key decisions were and we'll keep on session of the details of how to make these decisions in the next few videos

<img width="1650" height="924" alt="image" src="https://github.com/user-attachments/assets/096a363e-0acd-4ba6-a0a6-1d206e76ad27" />

- The first key decision was, how do you choose what features to use to split on at each node? At the root node, as well as on the left branch and the right branch of the decision tree, we had to decide if there were a few examples at that node comprising a mix of cats and dogs
- Do you want to split on the ear-shaped feature or the facial feature or the whiskers feature? We'll see in the next video, that decision trees will choose what feature to split on in order to try to maximize purity
- By purity, I mean, you want to get to what subsets, which are as close as possible to all cats or all dogs. For example, if we had a feature that said, does this animal have cat DNA, we don't actually have this feature. But if we did, we could have split on this feature at the root node, which would have resulted in five out of five cats in the left branch and zero of the five cats in the right branch.
- Both these left and right subsets of the data are completely pure, meaning that there's only one class, either cats only or not cats only in both of these left and right sub-branches, which is why the cat DNA feature if we had this feature, would have been a great feature to use.
- But with the features that we actually have, we had to decide, what is the split on ear shape, which result in four out of five examples on the left being cats, and one of the five examples on the right being cats or face shape where it resulted in the four of the seven on the left and one of the three on the right, or whiskers, which resulted in three out four examples being cast on the left and two out of six being not cats on the right
- The decision tree learning algorithm has to choose between ear-shaped, face shape, and whiskers. Which of these features results in the greatest purity of the labels on the left and right sub branches? Because it is if you can get to a highly pure subsets of examples, then you can either predict cat or predict not cat and get it mostly right.
- The next video on entropy, we'll talk about how to estimate impurity and how to minimize impurity. The first decision we have to make when learning a decision tree is how to choose which feature to split on on each node

<img width="1883" height="911" alt="image" src="https://github.com/user-attachments/assets/50bb56be-31a3-4e04-b3aa-df01197cfbf5" />

- The second key decision you need to make when building a decision tree is to decide when do you stop splitting. The criteria that we use just now was until I know there's either 100 percent, all cats or a 100 percent of dogs and not cats. Because at that point is seems natural to build a leaf node that just makes a classification prediction.
- Alternatively, you might also decide to stop splitting when splitting and no further results in the tree exceeding the maximum depth. Where the maximum depth that you allow the tree to go to, is a parameter that you could just say.
- In decision tree, the depth of a node is defined as the number of hops that it takes to get from the root node that is denoted the very top to that particular node. So the root node takes zero hops, to get to itself and is at Depth 0. The notes below it are at depth one and in those below it would be at Depth 2
- If you had decided that the maximum depth of the decision tree is say two, then you would decide not to split any nodes below this level so that the tree never gets to Depth 3. One reason you might want to limit the depth of the decision tree is to make sure for us to tree doesn't get too big and unwieldy and second, by keeping the tree small, it makes it less prone to overfitting

<img width="1881" height="864" alt="image" src="https://github.com/user-attachments/assets/005a47ae-f5c4-42b8-938e-189eee39bd5a" />

- Another criteria you might use to decide to stop splitting might be if the improvements in the purity score, which you see in a later video of below a certain threshold. If splitting a node results in minimum improvements to purity or you see later is actually decreases in impurity.
- But if the gains are too small, they might not bother. Again, both to keep the trees smaller and to reduce the risk of overfitting. Finally, if the number of examples that a node is below a certain threshold, then you might also decide to stop splitting.
- For example, if at the root node we have split on the face shape feature, then the right branch will have had just three training examples with one cat and two dogs and rather than splitting this into even smaller subsets, if you decided not to split further set of examples with just three of your examples, then you will just create a decision node and because there are mainly dogs, 2 out three are dogs here, this would be a node and this makes a prediction of not cat.
- Again, one reason you might decide this is not worth splitting on is to keep the tree smaller and to avoid overfitting. When I look at decision tree learning algorithms myself, sometimes I feel like, boy, there's a lot of different pieces and lots of different things going on in this algorithm. Part of the reason it might feel is in the evolution of decision trees.

<img width="1872" height="910" alt="image" src="https://github.com/user-attachments/assets/76e21a66-0ec2-499d-994c-1218b3dbe4f2" />

- There was one researcher that proposed a basic version of decision trees and then a different researcher said, oh, we can modify this thing this way, such as his new criteria for splitting. Then a different researcher comes up with a different thing like, oh, maybe we should stop splitting when it reaches a certain maximum depth. Over the years, different researchers came up with different refinements to the algorithm. As a result of that, it does work really well but we look at all the details of how to implement a decision tree. It feels a lot of different pieces such as why there's so many different ways to decide when to stop splitting.

---

# Decision Tree Learning

## Measuring Purity

- We will look at the way of measuring the purity of a set of examples. If the examples are all cats of a single class then that's very pure, if it's all not cats that's also very pure, but if it's somewhere in between how do you quantify how pure is the set of examples? Let's take a look at the definition of entropy, which is a measure of the impurity of a set of data.
- Given a set of six examples like this, we have three cats and three dogs, let's define p_1 to be the fraction of examples that are cats, that is, the fraction of examples with label one, that's what the subscript one indicates. p_1 in this example is equal to 3/6. We're going to measure the impurity of a set of examples using a function called the entropy which looks like this
- The entropy function is conventionally denoted as capital H of this number p_1 and the function looks like this curve over here where the horizontal axis is p_1, the fraction of cats in the sample, and the vertical axis is the value of the entropy. In this example where p_1 is 3/6 or 0.5, the value of the entropy of p_1 would be equal to one.
- You notice that this curve is highest when your set of examples is 50-50, so it's most impure as an impurity of one or with an entropy of one when your set of examples is 50-50, whereas in contrast if your set of examples was either all cats or not cats then the entropy is zero. Let's just go through a few more examples to gain further intuition about entropy and how it works
- Here's a different set of examples with five cats and one dog, so p_1 the fraction of positive examples, a fraction of examples labeled one is 5/6 and so p_1 is about 0.83. If you read off that value at about 0.83 we find that the entropy of p_1 is about 0.65. And here I'm writing it only to two significant digits.
- Here's one more example. This sample of six images has all cats so p_1 is six out of six because all six are cats and the entropy of p_1 is this point over here which is zero. We see that as you go from 3/6 to six out of six cats, the impurity decreases from one to zero or in other words, the purity increases as you go from a 50-50 mix of cats and dogs to all cats. Let's look at a few more examples.
- Here's another sample with two cats and four dogs, so p_1 here is 2/6 which is 1/3, and if you read off the entropy at 0.33 it turns out to be about 0.92. This is actually quite impure and in particular this set is more impure than this set because it's closer to a 50-50 mix, which is why the impurity here is 0.92 as opposed to 0.65
- Finally, one last example, if we have a set of all six dogs then p_1 is equal to 0 and the entropy of p_1 is just this number down here which is equal to 0 so there's zero impurity or this would be a completely pure set of all not cats or all dogs

<img width="1886" height="901" alt="image" src="https://github.com/user-attachments/assets/5f1d0ffc-75d9-4b5c-8589-fad5d00d244c" />

- Now, let's look at the actual equation for the entropy function H(p_1). Recall that p_1 is the fraction of examples that are equal to cats so if you have a sample that is 2/3 cats then that sample must have 1/3 not cats.
- Let me define p_0 to be equal to the fraction of examples that are not cats to be just equal to 1 minus p_1. The entropy function is then defined as negative p_1log_2 (p_1), and by convention when computing entropy we take logs to base two rather than to base e, and then minus p_0log_2(p_0)
- If you were to plot this function in a computer you will find that it will be exactly this function on the left.
- We take log_2 just to make the peak of this curve equal to one, if we were to take log_e or the base of natural logarithms, then that just vertically scales this function, and it will still work but the numbers become a bit hard to interpret because the peak of the function isn't a nice round number like one anymore.
- One note on computing this function, if p_1 or p_0 is equal to 0 then an expression like this will look like 0log(0), and log(0) is technically undefined, it's actually negative infinity. But by convention for the purposes of computing entropy, we'll take 0log(0) to be equal to 0 and that will correctly compute the entropy as zero or as one to be equal to zero.
- If you're thinking that this definition of entropy looks a little bit like the definition of the logistic loss that we learned about in the last course, there is actually a mathematical rationale for why these two formulas look so similar.
- But applying this formula for entropy should work just fine when you're building a decision tree. To summarize, the entropy function is a measure of the impurity of a set of data. It starts from zero, goes up to one, and then comes back down to zero as a function of the fraction of positive examples in your sample
- There are other functions that look like this, they go from zero up to one and then back down. For example, if you look in open source packages you may also hear about something called the Gini criteria, which is another function that looks a lot like the entropy function, and that will work well as well for building decision trees.

<img width="1911" height="878" alt="image" src="https://github.com/user-attachments/assets/ecbcbe03-bd8b-4f51-8118-08ad6a81cb99" />

---

## Choosing a split: Information Gain

- When building a decision tree, the way we'll decide what feature to split on at a node will be based on what choice of feature reduces entropy the most. Reduces entropy or reduces impurity, or maximizes purity. In decision tree learning, the reduction of entropy is called information gain.
- Let's take a look at how to compute information gain and therefore choose what features to use to split on at each node in a decision tree. Let's use the example of deciding what feature to use at the root node of the decision tree we were building just now for recognizing cats versus not cats
- If we had split using their ear shape feature at the root node, this is what we would have gotten, five examples on the left and five on the right. On the left, we would have four out of five cats, so P1 would be equal to 4/5 or 0.8.
- On the right, one out of five are cats, so P1 is equal to 1/5 or 0.2. If you apply the entropy formula from the last video to this left subset of data and this right subset of data, we find that the degree of impurity on the left is entropy of 0.8, which is about 0.72, and on the right, the entropy of 0.2 turns out also to be 0.72.
- This would be the entropy at the left and right subbranches if we were to split on the ear shape feature. One other option would be to split on the face shape feature. If we'd done so then on the left, four of the seven examples would be cats, so P1 is 4/7 and on the right, 1/3 are cats, so P1 on the right is 1/3.
- The entropy of 4/7 and the entropy of 1/3 are 0.99 and 0.92. So the degree of impurity in the left and right nodes seems much higher, 0.99 and 0.92 compared to 0.72 and 0.72.
- Finally, the third possible choice of feature to use at the root node would be the whiskers feature in which case you split based on whether whiskers are present or absent. In this case, P1 on the left is 3/4, P1 on the right is 2/6, and the entropy values are as follows.
- The key question we need to answer is, given these three options of a feature to use at the root node, which one do we think works best? It turns out that rather than looking at these entropy numbers and comparing them, it would be useful to take a weighted average of them, and here's what I mean
- If there's a node with a lot of examples in it with high entropy that seems worse than if there was a node with just a few examples in it with high entropy. Because entropy, as a measure of impurity, is worse if you have a very large and impure dataset compared to just a few examples and a branch of the tree that is very impure.
- The key decision is, of these three possible choices of features to use at the root node, which one do we want to use?
- Associated with each of these splits is two numbers, the entropy on the left sub-branch and the entropy on the right sub-branch. In order to pick from these, we like to actually combine these two numbers into a single number.
- So you can just pick of these three choices, which one does best? The way we're going to combine these two numbers is by taking a weighted average. Because how important it is to have low entropy in, say, the left or right sub-branch also depends on how many examples went into the left or right sub-branch.
- Because if there are lots of examples in, say, the left sub-branch then it seems more important to make sure that that left sub-branch's entropy value is low. In this example we have, five of the 10 examples went to the left sub-branch, so we can compute the weighted average as 5/10 times the entropy of 0.8, and then add to that 5/10 examples also went to the right sub-branch, plus 5/10 times the entropy of 0.2.
- Now, for this example in the middle, the left sub-branch had received seven out of 10 examples. and so we're going to compute 7/10 times the entropy of 0.57 plus, the right sub-branch had three out of 10 examples, so plus 3/10 times entropy of 0.3 of 1/3. Finally, on the right, we'll compute 4/10 times entropy of 0.75 plus 6/10 times entropy of 0.33. The way we will choose a split is by computing these three numbers and picking whichever one is lowest because that gives us the left and right sub-branches with the lowest average weighted entropy
- In the way that decision trees are built, we're actually going to make one more change to these formulas to stick to the convention in decision tree building, but it won't actually change the outcome.
- Which is rather than computing this weighted average entropy, we're going to compute the reduction in entropy compared to if we hadn't split at all. If we go to the root node, remember that the root node we have started off with all 10 examples in the root node with five cats and dogs, and so at the root node, we had p_1 equals 5/10 or 0.5. The entropy of the root nodes, entropy of 0.5 was actually equal to 1
- This was maximum impurity because it was five cats and five dogs. The formula that we're actually going to use for choosing a split is not this weighted entropy at the left and right sub-branches, instead is going to be the entropy at the root node, which is entropy of 0.5, then minus this formula.
- In this example, if you work out the math, it turns out to be 0.28. For the face shape example, we can compute entropy of the root node, entropy of 0.5 minus this, which turns out to be 0.03, and for whiskers, compute that, which turns out to be 0.12
- These numbers that we just calculated, 0.28, 0.03, and 0.12, these are called the information gain, and what it measures is the reduction in entropy that you get in your tree resulting from making a split. Because the entropy was originally one at the root node and by making the split, you end up with a lower value of entropy and the difference between those two values is a reduction in entropy, and that's 0.28 in the case of splitting on the ear shape

<img width="1900" height="952" alt="image" src="https://github.com/user-attachments/assets/bbf19f6b-8d16-4bd7-aaaa-26eac1660c2f" />

- Why do we bother to compute reduction in entropy rather than just entropy at the left and right sub-branches?
- It turns out that one of the stopping criteria for deciding when to not bother to split any further is if the reduction in entropy is too small. In which case you could decide, you're just increasing the size of the tree unnecessarily and risking overfitting by splitting and just decide to not bother if the reduction in entropy is too small or below a threshold
- In this other example, spitting on ear shape results in the biggest reduction in entropy, 0.28 is bigger than 0.03 or 0.12 and so we would choose to split onto ear shape feature at the root node.
- On the next slide, let's give a more formal definition of information gain. By the way, one additional piece of notation that we'll introduce also in the next slide is these numbers, 5/10 and 5/10. I'm going to call this w^left because that's the fraction of examples that went to the left branch, and I'm going to call this w^right because that's the fraction of examples that went to the right branch. Whereas for this another example, w^left would be 7/10, and w^right will be 3/10.
- Let's now write down the general formula for how to compute information gain. Using the example of splitting on the ear shape feature, let me define p_1^left to be equal to the fraction of examples in the left subtree that have a positive label, that are cats. In this example, p_1^left will be equal to 4/5.
- Also, let me define w^left to be the fraction of examples of all of the examples of the root node that went to the left sub-branch, and so in this example, w^left would be 5/10. Similarly, let's define p_1^right to be of all the examples in the right branch. The fraction that are positive examples and so one of the five of these examples being cats, there'll be 1/5, and similarly, w^right is 5/10 the fraction of examples that went to the right sub-branch. Let's also define p_1^root to be the fraction of examples that are positive in the root node.
- In this case, this would be 5/10 or 0.5. Information gain is then defined as the entropy of p_1^root, so what's the entropy at the root node, minus that weighted entropy calculation that we had on the previous slide, minus w^left those were 5/10 in the example, times the entropy applied to p_1^left, that's entropy on the left sub-branch, plus w^right the fraction of examples that went to the right branch, times entropy of p_1^right
- With this definition of entropy, and you can calculate the information gain associated with choosing any particular feature to split on in the node. Then out of all the possible futures, you could choose to split on, you can then pick the one that gives you the highest information gain.
- That will result in, hopefully, increasing the purity of your subsets of data that you get on the left and right sub-branches of your decision tree and that will result in choosing a feature to split on that increases the purity of your subsets of data in both the left and right sub-branches of your decision tree.
- Now that you know how to calculate information gain or reduction in entropy, you know how to pick a feature to split on another node. Let's put all the things we've talked about together into the overall algorithm for building a decision tree given a training set.

<img width="1840" height="906" alt="image" src="https://github.com/user-attachments/assets/dafc352c-426d-40bf-ae9e-7d8ca099a83d" />
