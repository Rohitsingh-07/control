## Demand Prediction

- We'll use an example from demand prediction in which we look at the product and try to predict whether the product will be a top seller or not?
- In this example, we are selling T shirts and we have collected data of different t-shirts that were sold at different prices, as well as which ones became a top seller. This type of applications are used by retailers today in order to plan better inventory levels as well as marketing campaigns.
- In this example, the input feature x is the price of the T shirt. If we apply logistic regression to fit a sigmoid function to the data that might look like the diagram below and predictions as per the same diagram.
- Previously, we use to write this as f(x) and now we are going to use the alphabet a to denote the output of the logistic regression algorithm. The term a stands for activation, and it refers to how much a neuron is sending a high output to other neurons downstream from it. It turns out that this logistic regression algorithm, can be thought of a very simplified model of asingle neuron in the brain.
- Here the neuron takes input the price x, and then it computes this formula on top, and it outputs the number a, which is computed by this formula, and it outputs the probability of this T shirt being a top seller.
- Another way to think of a neuron is as a tiny little computer whose only job is to input one number or a few numbers, such as a price, and then output one or maybe few other numbers which in this case is the probability of T-shirt being a top seller.
- A logistic regression algorithm is much simpler than what any biological neuron. Even though, in practice, we know deep learning algorithms do work very well. Given this description of a single neuron, building a neural network now it just requires taking a bunch of these neurons and wiring them together or putting them together

![image](https://github.com/user-attachments/assets/655dbee8-e1c2-4a45-9725-1d6bbae3b292)

- If we look at another example, here we are going to have 4 features to predict whether the t-shirt is a top seller or not. The features consists of the price of the T shirt, it's shipping costs, the amounts of marketing of that particular tshirt, as well as the material quality.
- We may think that whether or not a t shirt becomes a top seller depends on few factors
  - First one is the affordability of this T-shirt.
  - 2nd is the degree of awareness that the potential buyers have.
  - 3rd is the perceived quality to bias or potential bias saying that this is a high-quality T-shirt.
- What we are going to do is create an artificial neuron to try to estimate the probability that this T-shirt is perceived as highly affordable. Affordability is mainly a function of price and the shipping costs because the total amount of the pay is some of the price plus the shipping costs. We are going to use a little neuron here, a logistic regression unit to input price and shipping costs and predict do people think this is affordable?
- 2nd, we are going to create another artificial neuron to estimate, is there high awareness of this? Awareness, in this case is mainly a function of marketing.
- Finally, going to create another neuron to estimate do people perceive this to be high quality and that would mainly be the function of the price of the T-shirt and of the material quality. Price is a factor here because people think if it's high price, it would be good quality.
- Given these estimates of affordability, awareness and perceived quality we then wire the outputs of these three neurons to another neuron on the right, that then there's another logistic regression unit. That finally inputs these three numbers and outputs the probability of this t-shirt being the top seller.

![image](https://github.com/user-attachments/assets/5777a772-fc3f-44f4-b6c1-5c2977f21e37)

- In the neural network terminology, we're going to group these three neurons together into what's called a layer. A layer is a grouping of neurons which takes as input the same or similar features, and then in turn ouputs a few numbers together. These three neurons on the left form one layer which is why we draw them on top of each other, and this single neuron is another layer.
- The layer on the left has 3 neurons, so a layer can have multiple neurons or it can also have a single neuron as in the case of this layer on the right.
- This layer on the right is also called the output layer because the outputs of this final neuron is the output probability predicted by the neural network. In this terminology of neural networks we're also going to call affordability, awareness and perceived quality to be activations.
- The term activations comes from biological neurons, and it refers to the degree that the biological neuron is sending a high output value or sending many electrical impulses to other neurons to the downstream from it
- These numbers on affordability, awarenes, and perceived quality are the activations of these three neurons in this layer and also the output probability is the activation of the neuron on the right.
- Hence, the computation of the neural network is as follows.
  - It inputs 4 numbers, then this layer of the neural network uses those four numbers to compute the new numbers also called as activation values. Then the final layer, the output layer of the neural network used those three numbers to compute one number.
- In a neural network this list of four numbers is also called the input layer, and that's just a list of 4 numbers.
- One simplification that we can look here is that we had to go through the neurons one at a time and decide what inputs it would take from he previous layer. If we are building a large neural network it'd be a lot of work to go through and manually decide which neurons should take which features as inputs.
- The way a neural network is implemented is practice, each neuron in a certain layer; say this layer in the middle, will have access to every feature, to every value from the previous layer, from the input layer which is why we draw arrows from every input feature to every one of those neurons shown in the middle.
- We can think of it like this that if we are trying to predict the affordability and it knows the price, shipping cost, marketing and material, maybe we'll learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of the features that are most relevant to affordability.
- To simplify the notation, and the description of this neural network, we are going to view the neural network as having four features that will comprise the feature vector x, and we're going to view the neural network as having four features that comprise this feature vector x. This feature vector is fed to this layer in the middle which then computes three activation values.
- Then these numbers and these activation values in turn becomes another vector which is fed to this final output layer that finally outputs the probability of this t-shirt to being a top seller.

![image](https://github.com/user-attachments/assets/ee2545e2-1ab7-41fd-98c0-74b4f215a6ab)
---
![image](https://github.com/user-attachments/assets/f9e90056-c945-406b-9d4a-a288e0447b0f)

- That's all a neural network is, It has a few layers, where each layer inputs a vector and outputs another vector of numbers. This layer in the middle inputs 4 numbers x and outputs 3 numbers corresponding to the affordability, awareness and perceived quality. The first layer that contains inputs is called the input layer, the last layer is called the output layer. The layer in the middle is called a hidden layer.
- In a training set, we get to observe both x and y, the dataset tells what is x and what is y and we get the data that tells about the correct inputs and correct outputs. But it doesn't tell us what are the values for affordability, awareness and perceived quality. The correct values for those are hidden.

![image](https://github.com/user-attachments/assets/f31dca12-195c-43a8-b76e-8cdc8a7b36e8)

---

- In the below diagram, we can see that there is a logistic regression algorithm or logistic regression unit, that is taking as input, affordability, awareness and perceived quality of the t-shirt, and using these three features to estimate the probability of t-shirt being the top seller. This is just logistic regression. But rather then using the original features, price, shipping costs, marketing and so on, it is using a better set of features, affordability, awareness and perceived quality, that are hopefully more predictive of whether or not this t-shirt will be a top seller.
- So one way to think of this neural network, is just logistic regression. But, as a version of logistic regression, they can learn its own features that makes it easier to make accurate predictions.
- In one of the previous example, we said that if we want to predict the price of the house, we might take the frontage or the width of lots and multiply that by the depth of a lot to construct a more complex feature, x_1 times x_2 which was the size of the lawn. There we were doing manual feature engineering where we had to look at the features x_1 and x_2 and decide by hand how to combine the, together to come up with better features.
- What the neural network does is instead of we needing to manually engineer the features, it can learn, it's own features to make the learning problem easier for itself.

![image](https://github.com/user-attachments/assets/1979b60e-519b-4335-b752-f1c3a3447ba1)

- To summarize, a neural network, does this, the input layer has a vector of features, four numbers in this example, it is input to the hidden layer, which outputs the three numbers.
- We use a vector to denote the vecotr of activations that this hidden layer outputs. Then the output layer takes its input to three numbers and outputs one number, which would be the final activation, or the final prediction of the neural network.
- One of the really nice properties of a neural network is when we train it from data, we don't need to go in to explicitly decide what other features, such as affordability and so on, the neural network should compute instead or figure out all by itself what are the features it wants to use in this hidden layer.
---

- We can have more than one hidden layer is a neural network. When we are building a neural network, one of the decisions we need to make is how many hidden layers do we want and how many neurons do we want each hidden layer to have. This question of how many hidden layers and how many neurons is a question of the architecture of the neural network.

![image](https://github.com/user-attachments/assets/40768689-7c13-450a-9278-78f43959d470)

- The architecture that we are seeing on the left is called a multilayer perceptron.

---

## Example: Recognizing Images

- The last example of the neural network was the demand predictin example. Here, we are going to see how wecan apply a similar type of idea to computer vision application. If we are building a face recognition application, we might want to train a neural network that takes as input a picture and outputs the identity of the person in the picture. The image here is 1000 X 1000 pixels
- Its representation in the computer is actually as 1,000 by 1,000 grid, or also called 1,000 by 1,000 matrix of pixel intensity values
- In this example, my pixel intensity values or pixel brightness values, goes from 0-255 and so 197 here would be the brightness of the pixel in the very upper left of the image, 185 is brightness of the pixel, one pixel over, and so on down to 214 would be the lower right corner of this image
- If we were to take these pixel intensity values and unroll them into a vector, we end up with a list or a vector of a million pixel intensity values.
- The face recognition problem is, can you train a neural network that takes as input a feature vector with a million pixel brightness values and outputs the identity of the person in the picture.

![image](https://github.com/user-attachments/assets/03f06bc2-cbc2-4ea5-b31b-ca1c861d25b8)

- The input image X is fed to this layer of neurons. The output of this first hidden layer is fed to a second hidden layer and that output is fed to a third layer and then finally to the output layer, which then estimates, say the probability of this being a particular person.
- One interesting thing would be if we look at a neural network that's been trained on a lot of images of faces and to try to visualize what are these hidden layers, trying to compute. It turns out that when you train a system like this on a lot of pictures of faces and you peer at the different neurons in the hidden layers to figure out what they may be computing this is what you might find.
- In the first hidden layer, we might find one neuron that is looking for the low vertical line or a vertical edge like that. A second neuron looking for an oriented line or oriented edge. The third neuron looking for a line at that orientation, and so on.
- In the earliest layers of a neural network, we might find that the neurons are looking for very short lines or very short edges in the image.
- If you look at the next hidden layer, you find that these neurons might learn to group together lots of little short lines and little short edge segments in order to look for parts of faces.
- The next hidden layer, the neural network is aggregating different parts of faces to then try to detect presence or absence of larger, coarser face shapes.
- Then finally, detecting how much the face corresponds to different face shapes creates a rich set of features that then helps the output layer try to determine the identity of the person picture. A remarkable thing about the neural network is you can learn these feature detectors at the different hidden layers all by itself.
- In this example, no one ever told it to look for short little edges in the first layer, and eyes and noses and face parts in the second layer and then more complete face shapes at the third layer
- The neural network is able to figure out these things all by itself from data
- Just one note, in this visualization, the neurons in the first hidden layer are shown looking at relatively small windows to look for these edges. In the second hidden layer is looking at bigger window, and the third hidden layer is looking at even bigger window.
- These little neurons visualizations actually correspond to differently sized regions in the image.

![image](https://github.com/user-attachments/assets/f963ca91-f829-4007-b5d9-8706f76ae1e9)

- Just for fun, let's see what happens if you were to train this neural network on a different dataset, say on lots of pictures of cars, picture on the side. The same learning algorithm is asked to detect cars, will then learn edges in the first layer.
- Pretty similar but then they'll learn to detect parts of cars in the second hidden layer and then more complete car shapes in the third hidden layer. Just by feeding it different data, the neural network automatically learns to detect very different features so as to try to make the predictions of car detection or person recognition or whether there's a particular given task that is trained on.

![image](https://github.com/user-attachments/assets/a6da7cf1-b794-45aa-9471-c179fe54c153)

---
