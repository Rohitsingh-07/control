# Practice Lab: Decision Trees

In this exercise, you will implement a decision tree from scratch and apply it to the task of classifying whether a mushroom is edible or poisonous.

## Outline
- [ 1 - Packages ](#1)
- [ 2 -  Problem Statement](#2)
- [ 3 - Dataset](#3)
  - [ 3.1 One hot encoded dataset](#3.1)
- [ 4 - Decision Tree Refresher](#4)
  - [ 4.1  Calculate entropy](#4.1)
    - [ Exercise 1](#ex01)
  - [ 4.2  Split dataset](#4.2)
    - [ Exercise 2](#ex02)
  - [ 4.3  Calculate information gain](#4.3)
    - [ Exercise 3](#ex03)
  - [ 4.4  Get best split](#4.4)
    - [ Exercise 4](#ex04)
- [ 5 - Building the tree](#5)

<a name="1"></a>
## 1 - Packages 

First, let's run the cell below to import all the packages that you will need during this assignment.
- [numpy](https://www.numpy.org) is the fundamental package for working with matrices in Python.
- [matplotlib](https://matplotlib.org) is a famous library to plot graphs in Python.
- ``utils.py`` contains helper functions for this assignment. You do not need to modify code in this file.

```
import numpy as np
import matplotlib.pyplot as plt
from public_tests import *
from utils import *

%matplotlib inline
```

<img width="1227" height="246" alt="image" src="https://github.com/user-attachments/assets/7286e048-1886-4fe5-aa58-4cfd82aed665" />

<img width="1133" height="704" alt="image" src="https://github.com/user-attachments/assets/fac4610f-2535-401d-86fb-4996f9b10935" />

<img width="1258" height="204" alt="image" src="https://github.com/user-attachments/assets/fb1c25c0-fe4a-49f6-96d7-0e885937f52e" />

<img width="1158" height="704" alt="image" src="https://github.com/user-attachments/assets/5895fd86-d1b2-4eca-94f4-a8e4bca483b7" />

<img width="1109" height="233" alt="image" src="https://github.com/user-attachments/assets/eceba017-5671-4a29-bc9d-ecf039345521" />

```
X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])
y_train = np.array([1,1,0,0,1,0,0,1,1,0])
```

#### View the variables
Let's get more familiar with your dataset.  
- A good place to start is to just print out each variable and see what it contains.

The code below prints the first few elements of `X_train` and the type of the variable.

```
print("First few elements of X_train:\n", X_train[:5])
print("Type of X_train:",type(X_train))

Output:
First few elements of X_train:
 [[1 1 1]
 [1 0 1]
 [1 0 0]
 [1 0 0]
 [1 1 1]]
Type of X_train: <class 'numpy.ndarray'>
```

Now, let's do the same for y_train

```
print("First few elements of y_train:", y_train[:5])
print("Type of y_train:",type(y_train))

Output:
print("First few elements of y_train:", y_train[:5])
print("Type of y_train:",type(y_train))
print("First few elements of y_train:", y_train[:5])
print("Type of y_train:",type(y_train))
First few elements of y_train: [1 1 0 0 1]
Type of y_train: <class 'numpy.ndarray'>

```

#### Check the dimensions of your variables

Another useful way to get familiar with your data is to view its dimensions.

Please print the shape of `X_train` and `y_train` and see how many training examples you have in your dataset.

```
print ('The shape of X_train is:', X_train.shape)
print ('The shape of y_train is: ', y_train.shape)
print ('Number of training examples (m):', len(X_train))

Output:
The shape of X_train is: (10, 3)
The shape of y_train is:  (10,)
Number of training examples (m): 10
```

<img width="1187" height="451" alt="image" src="https://github.com/user-attachments/assets/599366b7-3a5a-443b-a525-f5e635b10722" />

<img width="1091" height="504" alt="image" src="https://github.com/user-attachments/assets/865b1004-25a7-4b23-94da-f6a66203c454" />

```
# UNQ_C1
# GRADED FUNCTION: compute_entropy

def compute_entropy(y):
    """
    Computes the entropy for 
    
    Args:
       y (ndarray): Numpy array indicating whether each example at a node is
           edible (`1`) or poisonous (`0`)
       
    Returns:
        entropy (float): Entropy at that node
        
    """
    # You need to return the following variables correctly
    entropy = 0.
    
    ### START CODE HERE ###
    if len(y) != 0:
        
     # Your code here to calculate the fraction of edible examples (i.e with value = 1 in y)
         p1 = len(y[y == 1]) / len(y) 

     # For p1 = 0 and 1, set the entropy to 0 (to handle 0log0)
         if p1 != 0 and p1 != 1:
         # Your code here to calculate the entropy using the formula provided above
             entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)
         else:
             entropy = 0. 
        
            
    ### END CODE HERE ###        
    
    return entropy
```

```
# Compute entropy at the root node (i.e. with all examples)
# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1"

print("Entropy at root node: ", compute_entropy(y_train)) 

# UNIT TESTS
compute_entropy_test(compute_entropy)

Output:
Entropy at root node:  1.0
 All tests passed.
```

<img width="1179" height="407" alt="image" src="https://github.com/user-attachments/assets/3401e24d-599e-43b0-a823-0432944a4107" />

<img width="766" height="639" alt="image" src="https://github.com/user-attachments/assets/3418834c-677d-4a5f-b44f-d604d0104723" />

<img width="1261" height="233" alt="image" src="https://github.com/user-attachments/assets/03bb969c-7bd9-400d-90a5-4e6342bc113b" />

```
# UNQ_C2
# GRADED FUNCTION: split_dataset

def split_dataset(X, node_indices, feature):
    """
    Splits the data at the given node into
    left and right branches
    
    Args:
        X (ndarray):             Data matrix of shape(n_samples, n_features)
        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.
        feature (int):           Index of feature to split on
    
    Returns:
        left_indices (list):     Indices with feature value == 1
        right_indices (list):    Indices with feature value == 0
    """
    
    # You need to return the following variables correctly
    left_indices = []
    right_indices = []
    
    ### START CODE HERE ###
    for i in node_indices:
        if X[i][feature] == 1: # Your code here to check if the value of X at that index for the feature is 1
            left_indices.append(i)
        else:
            right_indices.append(i)
        
            
        
            
    ### END CODE HERE ###
        
    return left_indices, right_indices
```

Now, let's check your implementation using the code blocks below. Let's try splitting the dataset at the root node, which contains all examples at feature 0 (Brown Cap) as we'd discussed above. We've also provided a helper function to visualize the output of the split.


```
# Case 1

root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# Feel free to play around with these variables
# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)
feature = 0

left_indices, right_indices = split_dataset(X_train, root_indices, feature)

print("CASE 1:")
print("Left indices: ", left_indices)
print("Right indices: ", right_indices)

# Visualize the split 
generate_split_viz(root_indices, left_indices, right_indices, feature)

print()

# Case 2

root_indices_subset = [0, 2, 4, 6, 8]
left_indices, right_indices = split_dataset(X_train, root_indices_subset, feature)

print("CASE 2:")
print("Left indices: ", left_indices)
print("Right indices: ", right_indices)

# Visualize the split 
generate_split_viz(root_indices_subset, left_indices, right_indices, feature)

# UNIT TESTS    
split_dataset_test(split_dataset)

Output:
CASE 1:
Left indices:  [0, 1, 2, 3, 4, 7, 9]
Right indices:  [5, 6, 8]
```

<img width="741" height="684" alt="image" src="https://github.com/user-attachments/assets/7cf7b3c9-00b0-4e88-98cf-343c58da0ffe" />

-  Calculate information gain
  - Next, you'll write a function called information_gain that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split.

<img width="1158" height="419" alt="image" src="https://github.com/user-attachments/assets/2f6c2ca0-73ad-429b-9f64-cb6e2dc5c16e" />

```
# UNQ_C3
# GRADED FUNCTION: compute_information_gain

def compute_information_gain(X, y, node_indices, feature):
    
    """
    Compute the information of splitting the node on a given feature
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.
        feature (int):           Index of feature to split on
   
    Returns:
        cost (float):        Cost computed
    
    """    
    # Split dataset
    left_indices, right_indices = split_dataset(X, node_indices, feature)
    
    # Some useful variables
    X_node, y_node = X[node_indices], y[node_indices]
    X_left, y_left = X[left_indices], y[left_indices]
    X_right, y_right = X[right_indices], y[right_indices]
    
    # You need to return the following variables correctly
    information_gain = 0
    
    ### START CODE HERE ###
    
    node_entropy = compute_entropy(y_node)
   # Your code here to compute the entropy at the left branch
    left_entropy = compute_entropy(y_left)
   # Your code here to compute the entropy at the right branch
    right_entropy = compute_entropy(y_right)

   # Your code here to compute the proportion of examples at the left branch
    w_left = len(X_left) / len(X_node)

   # Your code here to compute the proportion of examples at the right branch
    w_right = len(X_right) / len(X_node)

   # Your code here to compute weighted entropy from the split using 
   # w_left, w_right, left_entropy and right_entropy
    weighted_entropy = w_left * left_entropy + w_right * right_entropy

   # Your code here to compute the information gain as the entropy at the node
   # minus the weighted entropy
    information_gain = node_entropy - weighted_entropy
    

    ### END CODE HERE ###  
    
    return information_gain
```

```
info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)
print("Information Gain from splitting the root on brown cap: ", info_gain0)

info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)
print("Information Gain from splitting the root on tapering stalk shape: ", info_gain1)

info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)
print("Information Gain from splitting the root on solitary: ", info_gain2)

# UNIT TESTS
compute_information_gain_test(compute_information_gain)

Output:

Information Gain from splitting the root on brown cap:  0.034851554559677034
Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313
Information Gain from splitting the root on solitary:  0.2780719051126377
 All tests passed.
```

Splitting on "Solitary" (feature = 2) at the root node gives the maximum information gain. Therefore, it's the best feature to split on at the root node.

<img width="1142" height="311" alt="image" src="https://github.com/user-attachments/assets/ad199058-7918-4c00-ad73-43672d3ea962" />

```
# UNQ_C4
# GRADED FUNCTION: get_best_split

def get_best_split(X, y, node_indices):   
    """
    Returns the optimal feature and threshold value
    to split the node data 
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.

    Returns:
        best_feature (int):     The index of the best feature to split
    """    
    
    # Some useful variables
    num_features = X.shape[1]
    
    # You need to return the following variables correctly
    best_feature = -1
    
    ### START CODE HERE ###
    max_info_gain = 0

   # Iterate through all features
    for feature in range(num_features): 
        

       # Your code here to compute the information gain from splitting on this feature
        info_gain = compute_information_gain(X, y, node_indices, feature)

       # If the information gain is larger than the max seen so far
        if info_gain > max_info_gain:  
            
           # Your code here to set the max_info_gain and best_feature
            max_info_gain = info_gain
            best_feature = feature

            
    ### END CODE HERE ##    
   
    return best_feature
```

```
best_feature = get_best_split(X_train, y_train, root_indices)
print("Best feature to split on: %d" % best_feature)

# UNIT TESTS
get_best_split_test(get_best_split)

Output:
Best feature to split on: 2
```

As we saw above, the function returns that the best feature to split on at the root node is feature 2 ("Solitary")

- Building the tree
  - In this section, we use the functions you implemented above to generate a decision tree by successively picking the best feature to split on until we reach the stopping criteria (maximum depth is 2).

- You do not need to implement anything for this part.

```
# Not graded
tree = []

def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):
    """
    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.
    This function just prints the tree.
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.
        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']
        max_depth (int):        Max depth of the resulting tree. 
        current_depth (int):    Current depth. Parameter used during recursive call.
   
    """ 

    # Maximum depth reached - stop splitting
    if current_depth == max_depth:
        formatting = " "*current_depth + "-"*current_depth
        print(formatting, "%s leaf node with indices" % branch_name, node_indices)
        return
   
    # Otherwise, get best split and split the data
    # Get the best feature and threshold at this node
    best_feature = get_best_split(X, y, node_indices) 
    
    formatting = "-"*current_depth
    print("%s Depth %d, %s: Split on feature: %d" % (formatting, current_depth, branch_name, best_feature))
    
    # Split the dataset at the best feature
    left_indices, right_indices = split_dataset(X, node_indices, best_feature)
    tree.append((left_indices, right_indices, best_feature))
    
    # continue splitting the left and the right child. Increment current depth
    build_tree_recursive(X, y, left_indices, "Left", max_depth, current_depth+1)
    build_tree_recursive(X, y, right_indices, "Right", max_depth, current_depth+1)
```

```
build_tree_recursive(X_train, y_train, root_indices, "Root", max_depth=2, current_depth=0)
generate_tree_viz(root_indices, y_train, tree)

Output:
 Depth 0, Root: Split on feature: 2
- Depth 1, Left: Split on feature: 0
  -- Left leaf node with indices [0, 1, 4, 7]
  -- Right leaf node with indices [5]
- Depth 1, Right: Split on feature: 1
  -- Left leaf node with indices [8]
  -- Right leaf node with indices [2, 3, 6, 9]
```

<img width="1194" height="617" alt="image" src="https://github.com/user-attachments/assets/e59fca83-1161-4fa4-8160-6f5cd739843e" />
