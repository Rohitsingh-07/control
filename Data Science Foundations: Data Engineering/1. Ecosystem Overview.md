# Data Science System Overview

![image](https://github.com/user-attachments/assets/2353a378-c408-4fbb-a2fd-0cf85f0f9557)

- Data Sources and Data Hub: Data comes from real-time events and data at rest, which are processed and stored in a central Data Hub.
- Back Office and Front Office: The Data Hub is divided into a Back Office (where data is staged, cleansed, and conformed) and a Front Office (where analysts and data scientists access clean, structured data).
- Data Processing Phases: The Back Office involves three phases: staging (landing raw data), cleansing (cleaning and transforming data), and conforming (standardizing data for analysis).

This structure helps ensure that data is well-organized and ready for analysis by different teams within an organization.

# STAR Schema Design Overview

![image](https://github.com/user-attachments/assets/b8983862-046d-499a-a8b6-6f394da172da)

- The Idea of STAR schema is that the Fact table is the centre point of the ecosystem and the dimension tables are in 5 different points which make a star like structure
- It resembles that the fact table can be used with respect to any dimension table using a simple join

- Star Schema Structure: A star schema consists of fact tables (numerical data) and dimension tables (contextual data) arranged in a star-like shape.
- Facts and Dimensions: Facts are numerical and stored at the atomic grain (lowest level of detail), while dimensions are textual and provide context for the facts.
- Usage in Analysis: Dimensions are used to "slice and dice" data in BI tools, allowing for detailed analysis based on various attributes like time, location, and product.

# Where does Data Engineering Fit?

![image](https://github.com/user-attachments/assets/9beee989-5f9d-4392-85c1-9f28c0baf8ed)
![image](https://github.com/user-attachments/assets/02811a11-9327-4c5c-9f51-f5d4e3785217)

- Role of Data Engineering: Data engineering is responsible for managing everything within the Data Hub, including the inputs and outputs.
- Tasks Involved: Key tasks include data operations (infrastructure and system performance), data preparation (staging, cleansing, conforming, and delivering analytical data sets), and managing data interfaces (APIs and query tools).
- Importance: Data engineers ensure the system is up and running, and they prepare data for use by applications, data scientists, and business teams.

# Components of a goof data pipeline

![image](https://github.com/user-attachments/assets/165b3d60-f5ec-4b03-b783-1adfa9e1f0d5)

- Data Profiling and Quality Checks: Initially, create data profiles and log row counts. Run quality checks to test for null values in important columns.
- Cleansing and Logging: Manipulate the data, log missing values, and ensure the ability to roll back operations if needed.
- Conforming and Monitoring: Perform go/no-go checks based on logged errors and red flags. Send notifications to the data ops team.
Post-Delivery Quality Checks: Run additional quality checks after data delivery and perform automated rollbacks if necessary.

# Environment Setup

- Download and Install VirtualBox: Set up VirtualBox and import the Cloudera sandbox image.
- Configure Shared Folders: Ensure exercise files are accessible within the virtual machine by configuring shared folders.
- Set Up Hadoop Environment: Launch the Hue UI (Hadoop user interface) and configure it for data processing tasks.
