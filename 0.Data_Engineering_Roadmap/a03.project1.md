Great â€” letâ€™s begin **Step 1: Create the Monorepo**, and Iâ€™ll explain **exactly why this step matters**, **what it teaches you**, and **how to set it up properly** so your project mirrors a real startup data engineering environment.

---

# âœ… **STEP 1 â€” Create the Monorepo**

## ğŸ¯ **Goal of This Step**

To create a **single, unified repository** that contains all the components of your data engineering pipeline:

* Ingestion
* Transformation
* Data Quality
* Orchestration
* Data Warehouse models
* Infrastructure config
* Documentation
* CI/CD

This mirrors how modern Data Engineering teams structure production-grade systems.

---

# ğŸ”¥ **WHY THIS STEP IS NECESSARY (Important Explanation)**

### **1. Teaches You How Real DE Teams Organize Production Systems**

Companies rarely have small, isolated scripts.
They have a **system** with many moving parts:

* Airflow DAGs
* dbt models
* Great Expectations suites
* Warehouse schemas
* Docs
* Infra configs
* CI/CD pipelines

A monorepo reflects that reality.

---

### **2. Forces You to Think in Modular Components**

Each folder is its **own subsystem**, just like in real pipelines:

| Component        | What it Teaches               |
| ---------------- | ----------------------------- |
| ingestion/       | Data extraction architecture  |
| transformations/ | SQL modeling, dbt, ELT design |
| airflow/         | Orchestration and scheduling  |
| warehouse/       | Schema design, data modeling  |
| tests/           | Quality, validations          |
| infra/           | Docker, pipelines, configs    |

This modular structure will help you understand how to build scalable systems.

---

### **3. Makes CI/CD and DevOps Behaviors Natural**

With a monorepo, you can add:

* GitHub Actions
* Pre-commit hooks
* Linting
* Automatic dbt testing
* DAG validation

Just like real-world DE teams.

---

### **4. Clean Separation of â€œLayersâ€ â†’ Teaches Data Engineering Architecture**

A DE system has layers:

* **Bronze** â†’ raw
* **Silver** â†’ cleaned
* **Gold** â†’ business KPIs

A monorepo makes you keep those boundaries clean â€” a core DE skill.

---

### **5. Makes Your Project Portfolio-Ready (Very Important)**

Reviewers want to see:

* Structure
* Organization
* Multi-component systems

This repo structure will automatically look *senior-level*, not beginner.

---

### **6. Enables Multi-environment Setup (Local, Dev, Prod)**

You will later add:

```
configs/
  â”œâ”€â”€ local.yaml
  â”œâ”€â”€ dev.yaml
  â”œâ”€â”€ prod.yaml
```

This teaches you *environment management* â€” crucial for real pipelines.

---

# ğŸ“ **FULL MONOREPO STRUCTURE (Use this)**

Here is the exact structure you should create:

```
de-ecommerce-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ extract/
â”‚   â”œâ”€â”€ load/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ docker-compose.yaml
â”‚
â”œâ”€â”€ transformations/
â”‚   â””â”€â”€ dbt_project/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/
â”‚       â”‚   â”œâ”€â”€ marts/
â”‚       â”‚   â”œâ”€â”€ dim/
â”‚       â”‚   â””â”€â”€ fact/
â”‚       â”œâ”€â”€ tests/
â”‚       â””â”€â”€ profiles/
â”‚
â”œâ”€â”€ quality/
â”‚   â”œâ”€â”€ great_expectations/
â”‚   â””â”€â”€ expectations/
â”‚
â”œâ”€â”€ warehouse/
â”‚   â”œâ”€â”€ schema/
â”‚   â””â”€â”€ create_tables.sql
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ configs/
â”‚   â””â”€â”€ terraform/        (optional)
â”‚
â”œâ”€â”€ bi/
â”‚   â””â”€â”€ superset_dashboards/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ utilities/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data_flow.md
â”‚   â”œâ”€â”€ erd_diagram.md
â”‚   â””â”€â”€ setup_guide.md
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# ğŸ§  **WHAT THIS STEP TEACHES YOU (Crucial Skills You Gain)**

### **Skill 1 â€” Designing Data Platform Architecture**

You understand how real teams design:

* ingestion layer
* orchestration layer
* warehouse layer
* modeling layer
* BI layer

This is exactly what senior data engineers do.

---

### **Skill 2 â€” Thinking in Pipelines, Not Scripts**

Youâ€™ll understand why a random Python script is **not** a data pipeline.

This structure forces you to think in:

* dependencies
* layers
* environments
* modularity

---

### **Skill 3 â€” Documentation & Maintainability**

Every good DE team requires:

* README
* architecture docs
* lineage docs
* ERD diagrams

You are learning professional practice.

---

### **Skill 4 â€” Extensibilityâ€”The Pipeline Can Grow**

After this project, you can easily add:

* Streaming pipeline with Kafka
* Reverse ETL
* Feature store
* ML workflows

Your structure will support it.

---

# ğŸ“Œ **WHAT YOU NEED TO DO NOW**

Create the repo locally:

### **1. Create the folder**

```bash
mkdir de-ecommerce-pipeline
cd de-ecommerce-pipeline
```

### **2. Create subfolders**

```bash
mkdir -p data/raw data/landing data/bronze data/silver data/gold
mkdir -p ingestion/extract ingestion/load ingestion/utils
mkdir -p airflow/dags airflow/plugins
mkdir -p transformations/dbt_project/models/{staging,marts,dim,fact}
mkdir -p quality/great_expectations quality/expectations
mkdir -p warehouse/schema
mkdir -p infra/docker infra/configs
mkdir -p bi/superset_dashboards
mkdir -p scripts/utilities
mkdir -p docs
mkdir -p tests/unit tests/integration
mkdir -p .github/workflows
touch README.md requirements.txt .env.example
```

---

# ğŸ‰ **DONE WITH STEP 1**

If you want, I can now help with:

### ğŸ‘‰ Step 2 â€” Download the Brazilian E-Commerce Dataset

(in the most efficient folder structure for your pipeline)

or

### ğŸ‘‰ Generate `README.md` with a professional structure

or

### ğŸ‘‰ Add `requirements.txt` + environment setup

Just tell me:

**"Let's start Step 2"**

============================================================================================================================================================

Hereâ€™s exactly **where and how** to create the monorepo on your system using Command Prompt (Windows).
I'll give you two optionsâ€”choose the one you prefer.

---

# âœ… **Where to Create This Repository**

You can create your project **anywhere on your system**, but the **three most common and best locations** are:

---

## **Option A (recommended): Inside your `Documents` folder**

This keeps your projects organized and easy to find.

Path example:

```
C:\Users\<YourName>\Documents\
```

---

## **Option B: Inside a dedicated `Dev` or `Projects` folder**

If you want to be organized like real developers:

```
C:\Users\<YourName>\Dev\
```

or

```
C:\Users\<YourName>\Projects\
```

---

## **Option C: Inside your Desktop**

If you just want quick access:

```
C:\Users\<YourName>\Desktop\
```

---

# ğŸ”§ **How to Create the Monorepo (Step-by-Step in Command Prompt)**

### **1. Open Command Prompt**

Press:

```
Win + R â†’ type â€œcmdâ€ â†’ Enter
```

---

### **2. Navigate to the folder where you want your project**

Example (Documents folder):

```cmd
cd C:\Users\<YourName>\Documents
```

Example (Desktop):

```cmd
cd C:\Users\<YourName>\Desktop
```

---

### **3. Create the repository folder**

```cmd
mkdir de-ecommerce-pipeline
cd de-ecommerce-pipeline
```

âœ”ï¸ Inside this folder is where your entire data engineering project will live.

---

### **4. Create the subfolders**

Copy/paste this block into Command Prompt:

```cmd
mkdir data
mkdir data\raw data\landing data\bronze data\silver data\gold

mkdir ingestion
mkdir ingestion\extract ingestion\load ingestion\utils

mkdir airflow
mkdir airflow\dags airflow\plugins

mkdir transformations
mkdir transformations\dbt_project
mkdir transformations\dbt_project\models
mkdir transformations\dbt_project\models\staging transformations\dbt_project\models\marts transformations\dbt_project\models\dim transformations\dbt_project\models\fact
mkdir transformations\dbt_project\tests
mkdir transformations\dbt_project\profiles

mkdir quality
mkdir quality\great_expectations quality\expectations

mkdir warehouse
mkdir warehouse\schema

mkdir infra
mkdir infra\docker infra\configs infra\terraform

mkdir bi
mkdir bi\superset_dashboards

mkdir scripts
mkdir scripts\utilities

mkdir docs

mkdir tests
mkdir tests\unit tests\integration

mkdir .github
mkdir .github\workflows

type nul > README.md
type nul > requirements.txt
type nul > .env.example
```

---

### âœ”ï¸ When youâ€™re done, your folder structure is ready.

You can confirm with:

```cmd
tree /F
```

This will print the entire directory tree.

---

# ğŸ§  **Summary**

### **You should create the monorepo in a place where you can easily find and manage it, such as:**

* `Documents`
* `Dev`
* `Projects`
* `Desktop`

### **Command Prompt Steps:**

1. `cd` into your chosen directory
2. `mkdir de-ecommerce-pipeline`
3. Run the folder-creation commands

---

# âœ… Ready for Step 2?

If youâ€™re ready, say:

**â€œLetâ€™s start Step 2 â€” Download the dataset.â€**

And Iâ€™ll guide you through where to put the Brazilian E-Commerce dataset and how to organize it efficiently.

=======================================================================================================================================================================

Perfect â€” letâ€™s begin **STEP 2: Download + Organize the Brazilian E-Commerce Public Dataset**
This step is extremely important because it sets the foundation for the pipelineâ€™s **scalability, reproducibility, and data governance**.

---

# âœ… **STEP 2 â€” Download & Structure the Dataset (Production-Style)**

We are using the **Brazilian E-Commerce Dataset by Olist** â€” one of the best open-source datasets for real data engineering pipelines because:

* Itâ€™s multi-table (orders, customers, sellers, payments, reviews, geolocation)
* It mimics real business complexity
* It has relational integrity
* Itâ€™s large enough to simulate production pipelines
* Itâ€™s entirely open and free

---

# ğŸ¯ **Goal of This Step**

1. Download the dataset
2. Store it in the correct layers of your mono-repo
3. Set up **raw â†’ landing â†’ bronze â†’ silver â†’ gold** structure
4. Understand WHY each layer exists (important for DE interviews)

---

# ğŸ”¥ **Step 2A â€” Where to Download**

You will download the dataset manually from:

**Kaggle: "Brazilian E-Commerce Public Dataset by Olist"**
(search this on Kaggle)

After downloading the ZIP file, youâ€™ll see files like:

```
olist_customers_dataset.csv
olist_orders_dataset.csv
olist_order_items_dataset.csv
olist_order_reviews_dataset.csv
olist_order_payments_dataset.csv
olist_products_dataset.csv
olist_sellers_dataset.csv
olist_geolocation_dataset.csv
```

---

# ğŸ”¥ **Step 2B â€” Where to Place the Files (BEST PRACTICE)**

Inside your monorepo, you already have:

```
data/
  raw/
  landing/
  bronze/
  silver/
  gold/
```

You will place the downloaded CSVs inside:

```
data/raw/olist/
```

The command:

```cmd
mkdir data\raw\olist
```

Then manually move the CSVs into that folder.

---

# ğŸ§  **Why do we store data inside /data/raw?**

Because this follows real-world **medallion architecture** â€” every modern DE team structures data like this.

### **Layer Meaning**

| Layer       | Purpose                         | What goes here                    |
| ----------- | ------------------------------- | --------------------------------- |
| **raw**     | Untouched source data           | Files exactly as downloaded       |
| **landing** | Staging zone for ingestion jobs | Temporary storage for Airflow     |
| **bronze**  | Cleaned & lightly validated     | Schema-aligned raw data           |
| **silver**  | Fully transformed               | Enrichment, joins, business logic |
| **gold**    | Final analytics layer           | Fact tables, dimensions, KPIs     |

---

# ğŸ”¥ **Step 2C â€” Folder Structure After This Step**

Your repo should look like this now:

```
de-ecommerce-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ olist/
â”‚   â”‚       â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_order_items_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_order_reviews_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_order_payments_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_products_dataset.csv
â”‚   â”‚       â”œâ”€â”€ olist_sellers_dataset.csv
â”‚   â”‚       â””â”€â”€ olist_geolocation_dataset.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ landing/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
```

---

# ğŸ”¥ **Step 2D â€” What This Step Teaches You (This is IMPORTANT)**

## **1. Data Governance & Lineage**

You understand how data flows from:
**source â†’ raw â†’ cleaned â†’ modeled â†’ analytics**.

This is a *core* DE skill.

---

## **2. Reproducibility**

Anyone can pull your repo and instantly:

* See your raw data
* Reproduce your transformations
* Run your pipelines

Your repo becomes a *real* reproducible data system.

---

## **3. Multi-layer Data Architecture**

This step makes you think:

* How does data evolve?
* What transformations happen where?
* What rules must each layer follow?

This is how real DE systems think.

---

## **4. Scalability**

With properly separated layers, you can later add:

* Incremental loading
* Partitioning
* Validation layers
* BigQuery / Snowflake warehousing
* Airflow DAGs
* Streaming ingestion additions

This structure supports **enterprise scale**.

---

# ğŸ”¥ **Step 2E â€” Quick Verification Checklist**

âœ” Raw folder created
âœ” All CSVs placed inside `data/raw/olist`
âœ” Monorepo structure intact
âœ” Ready to proceed to ingestion and ETL

If all boxes are checked, we move to **Step 3.**

---

# ğŸ¯ **NEXT: Step 3 â€” Set Up Your Local Development Environment**

This includes:

* Installing Python + dependencies
* Setting up a venv
* Installing Airflow
* Setting up DB (Postgres)
* Creating the `.env` file
* Defining your ingestion configs

---

If you're ready, say:

ğŸ‘‰ **â€œLetâ€™s start Step 3â€**

==================================================================================================================================================================================

Great â€” **Step 3: Set Up a Reproducible Python Environment**

This step ensures your project is **isolated**, **clean**, and **fully reproducible** by anyone who clones your monorepo later (including future you).

---

# âœ… **Step 3 â€” Create & Activate a Virtual Environment**

### **Why this step matters**

Youâ€™re learning how real-world data teams structure projects:

| What It Teaches You            | Why It Matters                                                 |
| ------------------------------ | -------------------------------------------------------------- |
| Environment Isolation          | Prevents package conflicts (e.g., pandas 2.0 vs 1.5).          |
| Reproducibility                | Teaches how teams ensure code works the same on every machine. |
| Professional Project Practices | Every ML/data engineering job uses virtual envs.               |
| Dependency Management          | Youâ€™ll later deploy pipelines, which REQUIRE pinned versions.  |

This is foundational for building production-grade systems, not just Jupyter notebooks.

---

# ğŸ”§ **Step 3.1 â€” Create the environment**

### In Windows Command Prompt (inside your project folder):

```bash
python -m venv venv
```

This creates a folder called **venv/** that contains Python + all your project-specific packages.

---

# ğŸ”§ **Step 3.2 â€” Activate the environment**

```bash
venv\Scripts\activate
```

If successful, your terminal prompt will change to:

```
(venv) C:\path\to\project>
```

---

# ğŸ”§ **Step 3.3 â€” Install core packages**

Now install the core dependencies for early EDA and loading the dataset:

```bash
pip install pandas numpy matplotlib seaborn jupyterlab ipykernel python-dotenv
```

---

# ğŸ”§ **Step 3.4 â€” Save them in requirements.txt**

```bash
pip freeze > requirements.txt
```

This teaches you **version pinning** â€” critical in real-world production work.

---

# ğŸ”§ **Step 3.5 â€” Add the environment to Jupyter**

```bash
python -m ipykernel install --user --name=olist-env --display-name "Olist Env"
```

Now you can pick this kernel inside JupyterLab/Notebook.

---

# ğŸ‰ Step 3 Completed

Your monorepo now contains:

```
/olist-project
    â”œâ”€â”€ data/
    â”œâ”€â”€ notebooks/
    â”œâ”€â”€ scripts/
    â”œâ”€â”€ venv/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .env.example
```

---

# ğŸ‘‰ **Ready for Step 4?**

Step 4 is **Data Ingestion + Folder Organization**:

We will:

* create a `data/raw/` and `data/processed/` directory,
* download the dataset into the right place,
* explain why data versioning is important,
* write a script to load + validate the raw data.

Just say **"Yes let's do step 4"** and we move forward.

==============================================================================================================================================================================================

Perfect â€” let's execute **Step 4** in a clean, production-style, startup-MVP structure.

---

# âœ… **Step 4 â€” Dataset Setup + Folder Structure + Download Script (Startup-Ready Approach)**

This step converts your project from an empty monorepo â†’ a **real data engineering pipeline base**.

We will do THREE things:

---

# **4.1 Create the full project folder structure**

(What a real DE would set up for a new pipeline)

# **4.2 Download the Brazilian E-commerce (Olist) dataset**

(from Kaggle â€” requires API)

# **4.3 Write an automated Python script to download + unzip the dataset**

(so your entire dataset setup becomes reproducible + automated)

---

# ğŸš€ **4.1 â€” Create the Full Project Folder Structure**

Open Command Prompt **inside your repo directory**, then run:

```bash
mkdir data
mkdir data/raw
mkdir data/processed

mkdir notebooks
mkdir src
mkdir src/extract
mkdir src/transform
mkdir src/load

mkdir configs
mkdir scripts
mkdir logs
```

### **ğŸ’¡ Why this structure?**

Because it mirrors real-world DE pipelines:

| Folder            | Purpose                                                 |
| ----------------- | ------------------------------------------------------- |
| `data/raw/`       | Direct download from source (never modified)            |
| `data/processed/` | Cleaned/prepared tables                                 |
| `src/extract/`    | Scripts that fetch data (API/Kaggle/pipeline ingest)    |
| `src/transform/`  | Cleaning, normalization, modeling                       |
| `src/load/`       | Load into warehouse (DuckDB, Postgres, Snowflake later) |
| `scripts/`        | Helper scripts, automation                              |
| `configs/`        | YAML/JSON config for pipelines                          |
| `notebooks/`      | EDA + prototyping                                       |
| `logs/`           | Pipeline run logs (production habit)                    |

This teaches:

* modularity
* pipeline separation
* versionable dataset handling
* how real production ETL repos are structured

---

# ğŸš€ **4.2 â€” Download the Olist Dataset (The Brazilian E-commerce Dataset)**

We will use the official Kaggle version:

**Kaggle Dataset:**
"Olist Brazilian E-Commerce Public Dataset"

Steps to download:

### **1ï¸âƒ£ Install Kaggle CLI**

Run:

```bash
pip install kaggle
```

### **2ï¸âƒ£ Create API Token**

Go to:

**Kaggle â†’ Account â†’ API â†’ Create New Token**
This downloads: `kaggle.json`

### **3ï¸âƒ£ Put it in the correct location**

Windows:

```
C:\Users\<your-username>\.kaggle\kaggle.json
```

Then:

```bash
chmod 600 ~/.kaggle/kaggle.json
```

(Windows will ignore chmod safely.)

Now the CLI works.

---

# ğŸš€ **4.3 â€” Write Automated Dataset Download Script**

Inside:

```
scripts/download_olist.py
```

Create this file and paste:

```python
import os
import zipfile
import kaggle

RAW_DIR = "data/raw"
DATASET = "olistbr/brazilian-ecommerce"

def download_olist():
    os.makedirs(RAW_DIR, exist_ok=True)

    print("Downloading Olist dataset from Kaggle...")
    kaggle.api.dataset_download_files(DATASET, path=RAW_DIR, unzip=False)

    zip_path = os.path.join(RAW_DIR, "brazilian-ecommerce.zip")

    print("Unzipping files...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(RAW_DIR)

    os.remove(zip_path)

    print("Download complete. Raw data stored in data/raw/")

if __name__ == "__main__":
    download_olist()
```

### **ğŸ’¡ What this gives you**

âœ” Fully automated ingestion
âœ” Raw data always consistent
âœ” Anyone who clones your repo can run:

```bash
python scripts/download_olist.py
```

And instantly have the dataset.

This is **exactly how production data onboarding** works.

---

# ğŸ”¥ Step 4 Summary

You now have:

* a production-style folder structure
* automated dataset ingestion
* reproducible download script
* raw data stored safely

You're officially ready to move next to **Step 5 â€” Data Profiling & Schema Inference**.

This is where you'll:

* explore tables
* understand relationships
* detect nulls/outliers
* document schemas
* prepare for transformations

---

# ğŸ‘‰ Ready for Step 5?

Great â€” this explains everything.

Kaggle has **changed their API system**.
You are seeing the *new API token popup*, which **does NOT download `kaggle.json` anymore**.

Instead, Kaggle now provides:

* a **single API token string**
* instructions to set it as an environment variable

This replaces the old JSON file.

So **your Kaggle setup is correct** â€” nothing is broken.

Now hereâ€™s what to do next.

---

# âœ… **How to Use the New Kaggle API Token (2024â€“2025 update)**

Since Kaggle no longer gives you `kaggle.json`, you must:

### **Step 1 â€” Copy your API token**

From your popup:

```
KGAT_f39524a5ddd5b732aa9b3e846e9a5888
```

(Make sure to save it somewhere safe.)

---

# **Step 2 â€” Create a `.env` file**

Inside your project root, open:

```
.env
```

Paste this:

```
KAGGLE_API_TOKEN=KGAT_f39524a5ddd5b732aa9b3e846e9a5888
```

(This keeps your token out of your code â€” professional habit.)

---

# **Step 3 â€” Modify your Python download script**

Replace your Kaggle code with this updated version:

```python
import os
import zipfile
from kaggle.api.kaggle_api_extended import KaggleApi
from dotenv import load_dotenv

load_dotenv()

RAW_DIR = "data/raw"
DATASET = "olistbr/brazilian-ecommerce"

def download_olist():
    os.makedirs(RAW_DIR, exist_ok=True)

    api = KaggleApi()
    api.set_api_token(os.getenv("KAGGLE_API_TOKEN"))
    api.authenticate()

    print("Downloading Olist dataset from Kaggle...")
    api.dataset_download_files(DATASET, path=RAW_DIR, unzip=False)

    zip_path = os.path.join(RAW_DIR, "brazilian-ecommerce.zip")

    print("Unzipping files...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(RAW_DIR)

    os.remove(zip_path)
    
    print("Download complete. Raw data stored in data/raw/.")

if __name__ == "__main__":
    download_olist()
```

---

# **Why this works**

The new Kaggle API has shifted from:

```
~/.kaggle/kaggle.json
```

to:

```
Environment variable: KAGGLE_API_TOKEN
```

So we authenticate this way:

```python
api.set_api_token(os.getenv("KAGGLE_API_TOKEN"))
api.authenticate()
```

---

# ğŸ‰ **You Are Now Fully Ready for Step 5**

(And your dataset download will work with a single command.)

```bash
python scripts/download_olist.py
```

---

# Want to move to **Step 5 â€” Data Profiling + Schema Inference**?

==============================================================================================================================================
