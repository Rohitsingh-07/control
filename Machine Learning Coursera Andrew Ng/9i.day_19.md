## Logistic Regression

- We will continue with the example of classifying whether a tumor is malignant
- Whereas before we're going to use the label 1 or yes to the positive class to represent malignant tumors, and zero or no and negative examples to represent benign tumors.
- Here's a graph of the dataset where the horizontal axis is the tumor size and the vertical axis takes on only values of 0 and 1, because is a classification problem.
- As we saw earlier that linear regression is not a good algorithm for this problem. In contrast, what logistic regression we end up doing is fit a curve that looks like this, S-shaped curve to this dataset
- For this example, if a patient comes in with a tumor of this size, which I'm showing on the x-axis, then the algorithm will output 0.7 suggesting that is closer or maybe more likely to be malignant and benign.
![image](https://github.com/user-attachments/assets/bcf37d74-865a-4d0e-9cf0-2d491d76e2b9)

- The output label y is never 0.7, it is only ever 0 or 1
- To build out to the logistic regression algorithm, there's an important mathematical function I like to describe which is called the Sigmoid function, sometimes also referred to as the logistic function.

![image](https://github.com/user-attachments/assets/8e9df6ec-c1f6-46d6-a093-c5a9c05a94c6)

- The sigmoid Function looks like the graph on the right side
- The x-axis of the graph on the left and the right are different. In the graph to the left on the x-axis is the tumor size, so is all positive numbers. Whereas in the graph on the right, you have 0 down here, and the horizontal axis takes on both negative and positive values and have label the horizontal axis Z
- I'm showing here just a range of negative 3 to plus 3
- So the Sigmoid function outputs value is between 0 and 1. If I use g of z to denote this function, then the formula of g of z is equal to 1 over 1 plus e to the negative z

![image](https://github.com/user-attachments/assets/e08ab67c-0350-4ae9-8bad-dce68621c416)

- The sigmoid function is given as above in the mathematical terms
- If z were to be really big, so the denominator would be very close to 1 and hence the g of z that is the sigmoid function of z is going to be very close to 1
- Conversely, is z is a very small number or a large negative number, then the sigmoid function becomes very close to 0.
- That's why the sigmoid function has this shape where it starts very close to zero and slowly builds up or grows to the value of one
- Also when the z = 0, then the sigmoid function results as 0.5

![image](https://github.com/user-attachments/assets/524ee8f5-d153-4c5f-9a4c-d8dae7b76682)

- Now, let's use this to build up to the logistic regression algorithm. We're going to do this in two steps.
- In the first step, I hope you remember that a straight line function, like a linear regression function can be defined as w. product of x plus b.
- Let's store this value in a variable which I'm going to call z, and this will turn out to be the same z as the one you saw on the previous slide, but we'll get to that in a minute. The next step then is to take this value of z and pass it to the Sigmoid function, also called the logistic function, g
- Now, g of z then outputs a value computed by this formula, 1 over 1 plus e to the negative z. There's going to be between 0 and 1. When you take these two equations and put them together, they then give you the logistic regression model f of x, which is equal to g of wx plus b. Or equivalently g of z, which is equal to this formula over here
- This is the logistic regression model, and what it does is it inputs feature or set of features X and outputs a number between 0 and 1. Next, let's take a look at how to interpret the output of logistic regression

![image](https://github.com/user-attachments/assets/ea4b6251-e475-4c4b-8212-1fabd5744d5e)

- We'll return to the tumor classification example. The way I encourage you to think of logistic regressions output is to think of it as outputting the probability that the class or the label y will be equal to 1 given a certain input x. For example, in this application, where x is the tumor size and y is either 0 or 1, if you have a patient come in and she has a tumor of a certain size x, and if based on this input x, the model outputs 0.7
- Then what that means is that the model is predicting or the model thinks there's a 70 percent chance that the true label y would be equal to 1 for this patient
- In other words, the model is telling us that it thinks the patient has a 70 percent chance of the tumor turning out to be malignant.
- That's why if the chance of y being 1 is 0.7 or 70 percent chance, then the chance of it being 0 has got to be 0.3 or 30 percent chance.
- If someday you read research papers or blog pulls of all logistic regression, sometimes you see this notation that f of x is equal to p of y equals 1 given the input features x and with parameters w and b. What the semicolon here is used to denote is just that w and b are parameters that affect this computation of what is the probability of y being equal to 1 given the input feature x? For the purpose of this class, don't worry too much about what this vertical line and what the semicolon mean.

![image](https://github.com/user-attachments/assets/bc1c3135-3ee9-41f6-b1de-20be3516c97f)

---

## Optional Lab: Logistic Regression

- In this ungraded lab, you will
  - explore the sigmoid function (also known as the logistic function)
  - explore logistic regression; which uses the sigmoid function
```
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from plt_one_addpt_onclick import plt_one_addpt_onclick
from lab_utils_common import draw_vthresh
plt.style.use('./deeplearning.mplstyle')
```

## Sigmoid or Logistic Function
<img align="left" src="![image](https://github.com/user-attachments/assets/37252464-f246-4c9f-8ae7-15a33273c991)
"     style=" width:300px; padding: 10px; " >As discussed in the lecture videos, for a classification task, we can start by using our linear regression model, $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot  \mathbf{x}^{(i)} + b$, to predict $y$ given $x$. 
- However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1. 
- This can be accomplished by using a "sigmoid function" which maps all input values to values between 0 and 1. 


Let's implement the sigmoid function and see this for ourselves.

## Formula for Sigmoid function

The formula for a sigmoid function is as follows -  

$g(z) = \frac{1}{1+e^{-z}}\tag{1}$

In the case of logistic regression, z (the input to the sigmoid function), is the output of a linear regression model. 
- In the case of a single example, $z$ is scalar.
- in the case of multiple examples, $z$ may be a vector consisting of $m$ values, one for each example. 
- The implementation of the sigmoid function should cover both of these potential input formats.
Let's implement this in Python.
