## Addressing Overfitting

- Let's say we fit a model and it has high variance, it overfits.
- One way to address this problem is to collect more training data. If we are able to get more data, that is more training examples on sizes and prices of houses, then with the larger training set, the learning algorithm will learn to fit a function that is less wiggly.
- We can continue to fit a higher order polynomial or some of the function with a lot of features, and if we have enough training examples, it will still do okay.
- The number one tool we can use against overfitting is to get more training data.

![image](https://github.com/user-attachments/assets/64ea6e04-43d3-421a-a95f-eda2dc5496a5)

- At times, it happens that maybe there just isn't more data to add. In that case, a second option for addressing overfitting is to see if we can use fewer features.
- In case we have a lot of features but don't have enough training data, then the learning algorithm may also overfit to the training set. Now, instead of using all 100 features, if we were to pick just a subset of the most useful ones which seem the most relevant, then using just that smallest subset of features, you may find that the model no longer overfits as badly.
- Choosing the most appropriate set of features to use is also known as feature selection. One way we could do so is to use our intuition to choose what we think is the best set of features, what's most relevant for predicting the price
- A disadvantage of feature selection is that by using only a subset of the features, the algorithm is throwing away some of the information that we have about the houses.
- At times, it could happen that all the features of the training data set are important and we don't want to throw them away.

![image](https://github.com/user-attachments/assets/36634308-36ec-4f59-8090-5c926b657dcd)

- The third option for reducing overfitting is called regularization. We can see that the parameters are often relatively large. Now, if we were to eliminate some of these features, say if we were to eliminate the feature x4, that corresponds to setting this parameter to 0. So, setting a parameter to 0 is equivalent to eliminating the feature.
- It turns out that regularization is a way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright.
- Regularization encourage the learning algorithm to shrink the values of the parameters without necessarily demanding that the parameter is set to exactly 0.
- It turns out that even if we fit a higher order polynomial, so long as we can get the algorithm to use smaller parameter values, w1, w2, w3, w4. We would end up with a curve that fits the training data much better.
- Regularization lets us keep all the features, but they just prevent the features from having an overly large effect, which is what sometimes can cause overfitting.
- By convention, we just reduce the size of the wj parameters, that is w1 through wn. It doesn't make a huge difference whether you regularize the parameter b as well, we can do it if we want or choose to not to.
- In practice, regularizing b makes a very small impact

![image](https://github.com/user-attachments/assets/fc00d26a-89f0-4a13-8641-329491d06236)

![image](https://github.com/user-attachments/assets/c71b8f1e-787e-4268-aec7-4ebc901a475a)

---

## Optional lab: Overfitting

![image](https://github.com/user-attachments/assets/fefea417-af59-4735-b9ef-19aa3c2a35cb)

In this lab, you will explore:
- the situations where overfitting can occur
- some of the solutions

```
%matplotlib widget
import matplotlib.pyplot as plt
from ipywidgets import Output
from plt_overfit import overfit_example, output
plt.style.use('./deeplearning.mplstyle')
```

The week's lecture described situations where overfitting can arise. Run the cell below to generate a plot that will allow you to explore overfitting. There are further instructions below the cell.

```
plt.close("all")
display(output)
ofit = overfit_example(False)
```

![image](https://github.com/user-attachments/assets/19ba7980-c44b-4a77-8080-e508836005aa)

- Here, we can add points and fit the data as per that and also see how overfitting is represenated

In the plot above you can:
- switch between Regression and Categorization examples
- add data
- select the degree of the model
- fit the model to the data  

Here are some things you should try:
- Fit the data with degree = 1; Note 'underfitting'.
- Fit the data with degree = 6; Note 'overfitting'
- tune degree to get the 'best fit'
- add data:
    - extreme examples can increase overfitting (assuming they are outliers).
    - nominal examples can reduce overfitting
- switch between `Regression` and `Categorical` to try both examples.

To reset the plot, re-run the cell. Click slowly to allow the plot to update before receiving the next click.

Notes on implementations:
- the 'ideal' curves represent the generator model to which noise was added to achieve the data set
- 'fit' does not use pure gradient descent to improve speed. These methods can be used on smaller data sets.

---

## Cost Function with Regularization

- We did see that regularization tries to make the parental values W1 through WN small to reduce overfitting. We are going to build on that intuition and develop a modified cost function for the learning algorithm that can be used to actually apply regularization.
- If we fit a quadratic function to the data, it gives a pretty good fit. But if we fit a very high order polynomial, we end up with a curve that over fits the data.
- Let's consider that if we had a way to make the parameters W3 and W4 really, really small. Say close to 0.
- If we add 1000 times W3 and 1000 times W4 in the cost function, we would be penalizing the model if W3 and W4 are large. Because if we want to minimize the function, the only way to make this new cost function small is if W3 and W4 are both small.
- So, we are going to end up with W3 close to 0 and W4 close to 0. We are effectively nearly cancelling out the effects of the features X3 and X4 and getting rid of those two terms.
- When we do that, we end up with a fit to the data that's much closer to the quadratic function with just tiny contributions from the features X3 and X4.

![image](https://github.com/user-attachments/assets/0c82ae82-11d5-4529-886f-8050b9b830d2)

- More generally, the idea behind regularizartion. The idea is that if there are smaller values for the parameters, then that's a bit like having a simpler model. Maybe one with fewer features, which is therefore less prone to overfitting.
- In case we have a lot of features as in 100 features and we may not know which one to penalize, we penalize all the features or more precisely, we penalize all the Wj parameters and it's possible to show that this will usually result in fitting a smoother, simpler, less wiggly function that's less prone to overfitting.
- So, if we have data with 100 features for each house, it may be hard to pick and advance which features to include and which ones to exclude. So we have these 100 parameters W1 through W100 and the parameter B.
- As we have 100 features and we don't know which of these parameters are going to be the important ones. let's penalize all of them a bit and shrink all of them by adding this new term lambda times the sum from J equals 1 through n where n is 100. The number of features of Wj squared.
- The value lambda here is the Greek alphabet lambda and it's also called a regularization parameter. So similar to picking a learning rate alpha, you now also have to choose a number for lambda.
- Since we are adding the regularizaton parameter to the cost function, instead of using lambda times the sum of wj squared. We also divide lambda by 2m so that both the 1st and 2nd terms here are scaled by 1 over 2m.
- It turns out that by scaling both terms the same way it becomes a little bit easier to choose a good value for lambda.
- In case if the training set size grows and we find more training examples. So m the training set size is now bigger. The same value of lambda that you've picked previously is now also more likely to continue to work if we have this extra scaling by 2m.
- We are not going to penalize parameter b since in practive, it makes little to no difference in doing so. We can choose to penalize it as per our own convenience.

![image](https://github.com/user-attachments/assets/a634f24c-f8c6-4d4e-af28-d90d05d5f785)

- So to summarize in this modified cost function, we want to minimize the original cost, which is the mean squared error cost plus additionally, the second term which is called the regularization term.
- This new cost function trades off two goals that we might have. Trying to minimize the first term encourages the algorithm to fit the training data well by minimizing the squared differences of the predictions and the actual values. And trying to minimize the second term, the algorithm tries to keep the parameters Wj small, whic will tend to reduce overfitting.
- The value of lambda that we choose, specifies the relative importance of the relative trade off or how you balance between theese two goals.
- We go back to the housing price prediction example using linear regression to get an insight on what different values of lambda will cause the learning algorithm to do.
- If lambda was set to be 0, then we are not using the regularizaton term at all because the regularization term is multiplied by 0. So, if lambda was 0, we end up fitting the overly wiggly, overly complex curve and it over fits.
- If we said that lambda is a very high term say lamdbda equals 10 to the power 10, then we are placing a very heavy weight on this regularization term on the right. And the only way to minimize this is to be sure that all the values of w are pretty much close to 0.
- So, if lambda is very large, all the wj parameters become very close to 0 and the function becomes equal to b and so the learning algorithm fits a horizontal straght line and under fits.
- So we need a value of lambda that is in between and more appropriately balances these first and second terms of trading off, minimizing the mean squared error and keeping the parameters small.

![image](https://github.com/user-attachments/assets/e9485f3e-1aed-4e6e-bb10-80eaece84ee1)

---

## Regularized Linear Regression

- We are going to figure out how to get gradient descent to work with regularized linear regression.
- The cost function for the regularized linear regression is the usual squared error cost function, and now we have this additional regularization term, where lambda is the regularization parameter and we would like to find parameters w and b that minimize the regularized cost function.
- Initially, we used the gradient descent where we updated the parameters w, j and b for j equals 1 through n and b is also updated similarly. Alpha is the small positive number called the learning rate.
- The updates for a regularized linear regression look exactly the same, except that now the cost, J, is defined a bit differently.
- Previously the derivative of J with respect to w_j and similar to b has been defined and we have seen them.
- Now that we have added this additional regularization term, the only thing that changes is that the expression for the derivative with respect to w_j ends up with one additional term, whcih is represented below
- Recall that we don't regularize b, so we're not trying to shrink B. That's why the updated B remains the same as before, whereas the updated w changes because the regularization term causes us to try to shrink w_j.
- Let's take these definitions for the derivatives and put them back into the expression on the left to write out the gradient descent algorithm for regularized linear regression.

![image](https://github.com/user-attachments/assets/88dacb9a-5fbf-43c2-95b1-9ba3f3760655)

![image](https://github.com/user-attachments/assets/7424dfbb-5e2c-45ce-8d6a-e9023515e8a8)

- Above is the formula for the gradient descent algorithm for regularized linear regression

---

## Regularized Logistic Regression

- Just as the gradient update for logistic regression has seemed surprisingly similar to the gradient update for linear regression, you find that the gradient descent update for regularized logistic regression will also look similar to the update for regularized linear regression
- We saw earlier that logistic regression can be prone to overfitting if you fit it with very high order polynomial features like this. Here, z is a high order polynomial that gets passed into the sigmoid function like so to compute f. In particular, you can end up with a decision boundary that is overly complex and overfits as training set.
- More generally, when you train logistic regression with a lot of features, whether polynomial features or some other features, there could be a higher risk of overfitting. This was the cost function for logistic regression.

<img width="1815" height="941" alt="image" src="https://github.com/user-attachments/assets/d335a523-5b46-4eaf-aa89-d693c357cbce" />

- If you want to modify it to use regularization, all you need to do is add to it the following term. Let's add lambda to regularization parameter over 2m times the sum from j equals 1 through n, where n is the number of features as usual of wj squared. When you minimize this cost function as a function of w and b, it has the effect of penalizing parameters w_1, w_2 through w_n, and preventing them from being too large.
- If you do this, then even though you're fitting a high order polynomial with a lot of parameters, you still get a decision boundary that looks like this. Something that looks more reasonable for separating positive and negative examples while also generalizing hopefully to new examples not in the training set.

<img width="1919" height="939" alt="image" src="https://github.com/user-attachments/assets/e65908b3-94e9-4a6b-a5f7-5bafe3c65d60" />

- When using regularization, even when you have a lot of features. How can you actually implement this? How can you actually minimize this cost function j of wb that includes the regularization term? Well, let's use gradient descent as before. Here's a cost function that you want to minimize.
- To implement gradient descent, as before, we'll carry out the following simultaneous updates over wj and b. These are the usual update rules for gradient descent.
- Just like regularized linear regression, when you compute where there are these derivative terms, the only thing that changes now is that the derivative respect to wj gets this additional term, lambda over m times wj added here at the end.
- Again, it looks a lot like the update for regularized linear regression. In fact is the exact same equation, except for the fact that the definition of f is now no longer the linear function, it is the logistic function applied to z. Similar to linear regression, we will regularize only the parameters w, j, but not the parameter b, which is why there's no change the update you will make for b

<img width="1902" height="936" alt="image" src="https://github.com/user-attachments/assets/7fc74700-07eb-4a60-9661-6abc828cbfff" />

---

## Optional Lab: Regularization


In this lab, you will:
- extend the previous linear and logistic cost functions with a regularization term.
- rerun the previous example of over-fitting with a regularization term added.

```
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from plt_overfit import overfit_example, output
from lab_utils_common import sigmoid
np.set_printoptions(precision=8)
```

<img width="1161" height="499" alt="image" src="https://github.com/user-attachments/assets/de3ffbfe-c572-4c31-9237-03e4d5d29174" />

<img width="1199" height="525" alt="image" src="https://github.com/user-attachments/assets/12473fe1-af4b-41c2-8b1a-f3a7e16afdf1" />

```
def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m  = X.shape[0]
    n  = len(w)
    cost = 0.
    for i in range(m):
        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot
        cost = cost + (f_wb_i - y[i])**2                               #scalar             
    cost = cost / (2 * m)                                              #scalar  
 
    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar
```

```
np.random.seed(1)
X_tmp = np.random.rand(5,6)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5
b_tmp = 0.5
lambda_tmp = 0.7
cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)

print("Regularized cost:", cost_tmp)

Output: Regularized cost: 0.07917239320214275
```

<img width="1183" height="430" alt="image" src="https://github.com/user-attachments/assets/0442fc15-bf35-4d47-844f-ac9509ce1d1a" />

```
def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):
    """
    Computes the cost over all examples
    Args:
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns:
      total_cost (scalar):  cost 
    """

    m,n  = X.shape
    cost = 0.
    for i in range(m):
        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot
        f_wb_i = sigmoid(z_i)                                          #scalar
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar
             
    cost = cost/m                                                      #scalar

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                          #scalar
    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar
    
    total_cost = cost + reg_cost                                       #scalar
    return total_cost                                                  #scalar
```
```
np.random.seed(1)
X_tmp = np.random.rand(5,6)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5
b_tmp = 0.5
lambda_tmp = 0.7
cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)

print("Regularized cost:", cost_tmp)

Output: Regularized cost: 0.6850849138741673
```

<img width="1260" height="628" alt="image" src="https://github.com/user-attachments/assets/8a66c2a3-2ea2-4296-9d96-63420224949e" />

<img width="1243" height="253" alt="image" src="https://github.com/user-attachments/assets/065953f2-2193-4d5f-9ff3-93b5efe3f1ac" />

```
def compute_gradient_linear_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]                 
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]               
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m   
    
    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw
```

```
np.random.seed(1)
X_tmp = np.random.rand(5,3)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1])
b_tmp = 0.5
lambda_tmp = 0.7
dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)

print(f"dj_db: {dj_db_tmp}", )
print(f"Regularized dj_dw:\n {dj_dw_tmp.tolist()}", )

Output:
dj_db: 0.6648774569425726
Regularized dj_dw:
 [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]
```

```
def compute_gradient_logistic_reg(X, y, w, b, lambda_): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
      lambda_ (scalar): Controls amount of regularization
    Returns
      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))                            #(n,)
    dj_db = 0.0                                       #scalar

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]                       #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   #(n,)
    dj_db = dj_db/m                                   #scalar

    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]

    return dj_db, dj_dw  

```

```
np.random.seed(1)
X_tmp = np.random.rand(5,3)
y_tmp = np.array([0,1,0,1,0])
w_tmp = np.random.rand(X_tmp.shape[1])
b_tmp = 0.5
lambda_tmp = 0.7
dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)

print(f"dj_db: {dj_db_tmp}", )
print(f"Regularized dj_dw:\n {dj_dw_tmp.tolist()}", )

Output:
dj_db: 0.341798994972791
Regularized dj_dw:
 [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]
```

```
plt.close("all")
display(output)
ofit = overfit_example(True)
```
<img width="1092" height="628" alt="image" src="https://github.com/user-attachments/assets/6ed9a405-7ec8-4acf-af15-9ad421795b7e" />

<img width="1241" height="418" alt="image" src="https://github.com/user-attachments/assets/fcabec9a-7583-483f-ab69-b8e246798028" />
