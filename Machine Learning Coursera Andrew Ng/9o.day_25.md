## Addressing Overfitting

- Let's say we fit a model and it has high variance, it overfits.
- One way to address this problem is to collect more training data. If we are able to get more data, that is more training examples on sizes and prices of houses, then with the larger training set, the learning algorithm will learn to fit a function that is less wiggly.
- We can continue to fit a higher order polynomial or some of the function with a lot of features, and if we have enough training examples, it will still do okay.
- The number one tool we can use against overfitting is to get more training data.

![image](https://github.com/user-attachments/assets/64ea6e04-43d3-421a-a95f-eda2dc5496a5)

- At times, it happens that maybe there just isn't more data to add. In that case, a second option for addressing overfitting is to see if we can use fewer features.
- In case we have a lot of features but don't have enough training data, then the learning algorithm may also overfit to the training set. Now, instead of using all 100 features, if we were to pick just a subset of the most useful ones which seem the most relevant, then using just that smallest subset of features, you may find that the model no longer overfits as badly.
- Choosing the most appropriate set of features to use is also known as feature selection. One way we could do so is to use our intuition to choose what we think is the best set of features, what's most relevant for predicting the price
- A disadvantage of feature selection is that by using only a subset of the features, the algorithm is throwing away some of the information that we have about the houses.
- At times, it could happen that all the features of the training data set are important and we don't want to throw them away.

![image](https://github.com/user-attachments/assets/36634308-36ec-4f59-8090-5c926b657dcd)

- The third option for reducing overfitting is called regularization. We can see that the parameters are often relatively large. Now, if we were to eliminate some of these features, say if we were to eliminate the feature x4, that corresponds to setting this parameter to 0. So, setting a parameter to 0 is equivalent to eliminating the feature.
- It turns out that regularization is a way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright.
- Regularization encourage the learning algorithm to shrink the values of the parameters without necessarily demanding that the parameter is set to exactly 0.
- It turns out that even if we fit a higher order polynomial, so long as we can get the algorithm to use smaller parameter values, w1, w2, w3, w4. We would end up with a curve that fits the training data much better.
- Regularization lets us keep all the features, but they just prevent the features from having an overly large effect, which is what sometimes can cause overfitting.
- By convention, we just reduce the size of the wj parameters, that is w1 through wn. It doesn't make a huge difference whether you regularize the parameter b as well, we can do it if we want or choose to not to.
- In practice, regularizing b makes a very small impact

![image](https://github.com/user-attachments/assets/fc00d26a-89f0-4a13-8641-329491d06236)

![image](https://github.com/user-attachments/assets/c71b8f1e-787e-4268-aec7-4ebc901a475a)

---

## Optional lab: Overfitting

![image](https://github.com/user-attachments/assets/fefea417-af59-4735-b9ef-19aa3c2a35cb)

In this lab, you will explore:
- the situations where overfitting can occur
- some of the solutions

```
%matplotlib widget
import matplotlib.pyplot as plt
from ipywidgets import Output
from plt_overfit import overfit_example, output
plt.style.use('./deeplearning.mplstyle')
```

The week's lecture described situations where overfitting can arise. Run the cell below to generate a plot that will allow you to explore overfitting. There are further instructions below the cell.

```
plt.close("all")
display(output)
ofit = overfit_example(False)
```

![image](https://github.com/user-attachments/assets/19ba7980-c44b-4a77-8080-e508836005aa)

- Here, we can add points and fit the data as per that and also see how overfitting is represenated

In the plot above you can:
- switch between Regression and Categorization examples
- add data
- select the degree of the model
- fit the model to the data  

Here are some things you should try:
- Fit the data with degree = 1; Note 'underfitting'.
- Fit the data with degree = 6; Note 'overfitting'
- tune degree to get the 'best fit'
- add data:
    - extreme examples can increase overfitting (assuming they are outliers).
    - nominal examples can reduce overfitting
- switch between `Regression` and `Categorical` to try both examples.

To reset the plot, re-run the cell. Click slowly to allow the plot to update before receiving the next click.

Notes on implementations:
- the 'ideal' curves represent the generator model to which noise was added to achieve the data set
- 'fit' does not use pure gradient descent to improve speed. These methods can be used on smaller data sets. 
