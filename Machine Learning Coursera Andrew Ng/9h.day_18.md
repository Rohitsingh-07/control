# Week 3 

- This week, you'll learn the other type of supervised learning, classification. You'll learn how to predict categories using the logistic regression model. You'll learn about the problem of overfitting, and how to handle this problem with a method called regularization. You'll get to practice implementing logistic regression with regularization at the end of this week!

- Learning Objectives
  - Use logistic regression for binary classification
  - Implement logistic regression for binary classification
  - Address overfitting using regularization, to improve model performance

---

## Motivations

- We learn about classification where the output variable y can take on only one of a small handful of possibe values instead of any number in an infinite range of numbers.
- It turns out that Linear regression is not a good algorithm for classification problems
- In calssification problems, y can only be two values, yes or no. This type of classification problem where there are only two possible outputs is called binary classification. Where the word binary refers to there being only two possible classes or two possible categories.
- Classes and category both mean the same thing

![image](https://github.com/user-attachments/assets/f45e1fde-eaf4-42a4-a411-69ad1fd42d27)

---
![image](https://github.com/user-attachments/assets/12205201-b1d7-41dc-b808-208bdc40149f)

- The above diagram shows us what would happen if we apply a linear regression functon on a classification problem
- Linear regression predicts not just the values zero and one. But all numbers between zero and one or even less than zero or greater than one.
- But here we want to predict categories. One thing you could try is to pick a threshold of say 0.5. So that if the model outputs a value below 0.5, then you predict why equal zero or not malignant. And if the model outputs a number equal to or greater than 0.5, then predict Y equals one or malignant.
- Notice that this threshold value of 0.5 intersects the best fit straight line at this point. So if you draw this vertical line here, everything to the left ends up with a prediction of y equals zero. And everything on the right ends up with the prediction of y equals one
- But in case when our data would have one more training example te best fit line will shift towards thta data too and then the model would not be able to do good classification and would not provide a proper result or an optimal one
- We would look at Logistic regression later which always provides the output in zero or one and we will also learn about the decision boundary

---

## Optional Lab: Classification

- In this optional lab, you’ll get to take a look at what happens when you try to use linear regression for classification on categorical data.  You can see an interactive plot that attempts to classify between two categories.  And you may notice that this doesn’t work very well, which is okay, because that motivates the need for a different model to do classification tasks.

```
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_common import dlc, plot_data
from plt_one_addpt_onclick import plt_one_addpt_onclick
plt.style.use('./deeplearning.mplstyle')
```

![image](https://github.com/user-attachments/assets/eae801ef-6e70-4680-8077-642e2c700597)

```
x_train = np.array([0., 1, 2, 3, 4, 5])
y_train = np.array([0,  0, 0, 1, 1, 1])
X_train2 = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y_train2 = np.array([0, 0, 0, 1, 1, 1])
```
- These are the training examples

```
pos = y_train == 1
neg = y_train == 0

fig,ax = plt.subplots(1,2,figsize=(8,3))
#plot 1, single variable
ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label="y=1")
ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label="y=0", facecolors='none', 
              edgecolors=dlc["dlblue"],lw=3)

ax[0].set_ylim(-0.08,1.1)
ax[0].set_ylabel('y', fontsize=12)
ax[0].set_xlabel('x', fontsize=12)
ax[0].set_title('one variable plot')
ax[0].legend()

#plot 2, two variables
plot_data(X_train2, y_train2, ax[1])
ax[1].axis([0, 4, 0, 4])
ax[1].set_ylabel('$x_1$', fontsize=12)
ax[1].set_xlabel('$x_0$', fontsize=12)
ax[1].set_title('two variable plot')
ax[1].legend()
plt.tight_layout()
plt.show()
```
![image](https://github.com/user-attachments/assets/8f9e2352-8ee3-4720-9027-1907f6028957)

- This is the code to present two examples.
  - First example is one variable plot
  - The second example is teo variable plot (For Ex: for the first plot, we just see that the points are plotted. For the second plot, we can say that the Tumor and the size of tumor both are given)

Note in the plots above:
- In the single variable plot, positive results are shown both a red 'X's and as y=1. Negative results are blue 'O's and are located at y=0.
   - Recall in the case of linear regression, y would not have been limited to two values but could have been any value.
- In the two-variable plot, the y axis is not available.  Positive results are shown as red 'X's, while negative results use the blue 'O' symbol.
    - Recall in the case of linear regression with multiple variables, y would not have been limited to two values and a similar plot would have been three-dimensional.

#### Linear Regression approach
In the previous week, you applied linear regression to build a prediction model. Let's try that approach here using the simple example that was described in the lecture. The model will predict if a tumor is benign or malignant based on tumor size.  Try the following:
- Click on 'Run Linear Regression' to find the best linear regression model for the given data.
    - Note the resulting linear model does **not** match the data well. 
One option to improve the results is to apply a *threshold*. 
- Tick the box on the 'Toggle 0.5 threshold' to show the predictions if a threshold is applied.
    - These predictions look good, the predictions match the data
- *Important*: Now, add further 'malignant' data points on the far right, in the large tumor size range (near 10), and re-run linear regression.
    - Now, the model predicts the larger tumor, but data point at x=3 is being incorrectly predicted!
- to clear/renew the plot, rerun the cell containing the plot command.

```
w_in = np.zeros((1))
b_in = 0
plt.close('all') 
addpt = plt_one_addpt_onclick( x_train,y_train, w_in, b_in, logistic=False)
```

- This block of code does a presentation of how the linear regression misinterprets the data and gives incorrect classification

![image](https://github.com/user-attachments/assets/a1c4eae6-8a08-4c43-81e4-024e27c7bcd0)
