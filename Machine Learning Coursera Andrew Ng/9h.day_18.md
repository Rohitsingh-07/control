# Week 3 

- This week, you'll learn the other type of supervised learning, classification. You'll learn how to predict categories using the logistic regression model. You'll learn about the problem of overfitting, and how to handle this problem with a method called regularization. You'll get to practice implementing logistic regression with regularization at the end of this week!

- Learning Objectives
  - Use logistic regression for binary classification
  - Implement logistic regression for binary classification
  - Address overfitting using regularization, to improve model performance

---

## Motivations

- We learn about classification where the output variable y can take on only one of a small handful of possibe values instead of any number in an infinite range of numbers.
- It turns out that Linear regression is not a good algorithm for classification problems
- In calssification problems, y can only be two values, yes or no. This type of classification problem where there are only two possible outputs is called binary classification. Where the word binary refers to there being only two possible classes or two possible categories.
- Classes and category both mean the same thing

![image](https://github.com/user-attachments/assets/f45e1fde-eaf4-42a4-a411-69ad1fd42d27)

---
![image](https://github.com/user-attachments/assets/12205201-b1d7-41dc-b808-208bdc40149f)

- The above diagram shows us what would happen if we apply a linear regression functon on a classification problem
- Linear regression predicts not just the values zero and one. But all numbers between zero and one or even less than zero or greater than one.
- But here we want to predict categories. One thing you could try is to pick a threshold of say 0.5. So that if the model outputs a value below 0.5, then you predict why equal zero or not malignant. And if the model outputs a number equal to or greater than 0.5, then predict Y equals one or malignant.
- Notice that this threshold value of 0.5 intersects the best fit straight line at this point. So if you draw this vertical line here, everything to the left ends up with a prediction of y equals zero. And everything on the right ends up with the prediction of y equals one
- But in case when our data would have one more training example te best fit line will shift towards thta data too and then the model would not be able to do good classification and would not provide a proper result or an optimal one
- We would look at Logistic regression later which always provides the output in zero or one and we will also learn about the decision boundary
