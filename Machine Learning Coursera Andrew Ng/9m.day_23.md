## Gradient Descent Implementation

- To fit the parameters of a logistic regression model, we're going to try to find the values of the parameters w and b that minimize the cost function J of w and b, and we'll again apply gradient descent to perform this
- We are going to focus on how to find a good choice of the parameters w and b. Once we do it, if we give the model a new input, x , say a new patient at the hospital with a certain tumor size and age, then these are diagnosis. The model can then make a prediction, or it can try to estimate the probability that the label y is one.

![image](https://github.com/user-attachments/assets/d8d27bf9-1a2a-44e0-ab78-7a4bfb0cc451)

- The algorithm that we can use to minimize the cost function is gradient descent.
- We have to use to cost function and get tot the optimal results of parameters where cost function is the minimum
- We derive the formulas for the cost function an for updating the hyperparameters

![image](https://github.com/user-attachments/assets/dc213033-4a0d-4702-9dd6-eaad668f9c95)

- Even though some equations in the linear and logistic regression looks similar, it is not particularly. The reason is because everything core is the function. In linear regression we have the equation as wx+b
- But in logistic regression, f of x is defined to be the sigmoid function applied to wx+b.
- We can apply the same method for logistic regression to make sure it also converges. We have written out these updates as if we are updating the parameters w_j one parameter at one. We can also use vectorization to make gradient descent run faster for logistic regression.
- We used feature scaling, that is scaling all the features to take on similar ranges of values, say between -1 to +1, and how they can help gradient descent to converge faster.
- Feature scaling applied the same way to scale the different features to take on similar ranges of values can also speed up the gradient descent for logistic regression.

---

## Optional lab: Gradient descent for logistic regression

In this lab, you will:
- update gradient descent for logistic regression.
- explore gradient descent on a familiar data set

```
import copy, math
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic
from plt_quad_logistic import plt_quad_logistic, plt_prob
plt.style.use('./deeplearning.mplstyle')
```

Let's start with the same two feature data set used in the decision boundary lab.

```
X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y_train = np.array([0, 0, 0, 1, 1, 1])
```

As before, we'll use a helper function to plot this data. The data points with label  ùë¶=1 are shown as red crosses, while the data points with label  ùë¶=0 are shown as blue circles.

```
fig,ax = plt.subplots(1,1,figsize=(4,4))
plot_data(X_train, y_train, ax)

ax.axis([0, 4, 0, 3.5])
ax.set_ylabel('$x_1$', fontsize=12)
ax.set_xlabel('$x_0$', fontsize=12)
plt.show()
```

![image](https://github.com/user-attachments/assets/2c5ef5c5-fed5-4cde-a645-72cff6148c9e)

![image](https://github.com/user-attachments/assets/d8cd9a5a-d1f9-49fd-8dd7-b7d900a6d80c)

![image](https://github.com/user-attachments/assets/e138e56a-25a1-4e83-aaca-f6b9cc21762b)

```
def compute_gradient_logistic(X, y, w, b): 
    """
    Computes the gradient for logistic regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
    Returns
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))                           #(n,)
    dj_db = 0.

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]                       #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   #(n,)
    dj_db = dj_db/m                                   #scalar
        
    return dj_db, dj_dw
```

```
X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y_tmp = np.array([0, 0, 0, 1, 1, 1])
w_tmp = np.array([2.,3.])
b_tmp = 1.
dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)
print(f"dj_db: {dj_db_tmp}" )
print(f"dj_dw: {dj_dw_tmp.tolist()}" )

Output-
dj_db: 0.49861806546328574
dj_dw: [0.498333393278696, 0.49883942983996693]
```

```
def gradient_descent(X, y, w_in, b_in, alpha, num_iters): 
    """
    Performs batch gradient descent
    
    Args:
      X (ndarray (m,n)   : Data, m examples with n features
      y (ndarray (m,))   : target values
      w_in (ndarray (n,)): Initial values of model parameters  
      b_in (scalar)      : Initial values of model parameter
      alpha (float)      : Learning rate
      num_iters (scalar) : number of iterations to run gradient descent
      
    Returns:
      w (ndarray (n,))   : Updated values of parameters
      b (scalar)         : Updated value of parameter 
    """
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)  #avoid modifying global w within function
    b = b_in
    
    for i in range(num_iters):
        # Calculate the gradient and update the parameters
        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               
        b = b - alpha * dj_db               
      
        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( compute_cost_logistic(X, y, w, b) )

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]}   ")
        
    return w, b, J_history         #return final w,b and J history for graphing

```

```
w_tmp  = np.zeros_like(X_train[0])
b_tmp  = 0.
alph = 0.1
iters = 10000

w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) 
print(f"\nupdated parameters: w:{w_out}, b:{b_out}")

Iteration    0: Cost 0.684610468560574   
Iteration 1000: Cost 0.1590977666870456   
Iteration 2000: Cost 0.08460064176930081   
Iteration 3000: Cost 0.05705327279402531   
Iteration 4000: Cost 0.042907594216820076   
Iteration 5000: Cost 0.034338477298845684   
Iteration 6000: Cost 0.028603798022120097   
Iteration 7000: Cost 0.024501569608793   
Iteration 8000: Cost 0.02142370332569295   
Iteration 9000: Cost 0.019030137124109114   

updated parameters: w:[5.28 5.08], b:-14.222409982019837

```

```
fig,ax = plt.subplots(1,1,figsize=(5,4))
# plot the probability 
plt_prob(ax, w_out, b_out)

# Plot the original data
ax.set_ylabel(r'$x_1$')
ax.set_xlabel(r'$x_0$')   
ax.axis([0, 4, 0, 3.5])
plot_data(X_train,y_train,ax)

# Plot the decision boundary
x0 = -b_out/w_out[0]
x1 = -b_out/w_out[1]
ax.plot([0,x0],[x1,0], c=dlc["dlblue"], lw=1)
plt.show()
```

![image](https://github.com/user-attachments/assets/6145ec24-f3fd-4ddb-a12a-33e3961e2619)

In the plot above:
 - the shading reflects the probability y=1 (result prior to decision boundary)
 - the decision boundary is the line at which the probability = 0.5


- Let's return to a one-variable data set. With just two parameters,  ùë§,  ùëè, it is possible to plot the cost function using a contour plot to get a better idea of what gradient descent is up to.

```
x_train = np.array([0., 1, 2, 3, 4, 5])
y_train = np.array([0,  0, 0, 1, 1, 1])
```

```
fig,ax = plt.subplots(1,1,figsize=(4,3))
plt_tumor_data(x_train, y_train, ax)
plt.show()
```

![image](https://github.com/user-attachments/assets/581fddd7-833f-4ab0-9f6c-091e2c15ea7f)

```
w_range = np.array([-1, 7])
b_range = np.array([1, -14])
quad = plt_quad_logistic( x_train, y_train, w_range, b_range )
```

![image](https://github.com/user-attachments/assets/06051264-866a-4a1a-9260-bf07ad39672e)

You have:
- examined the formulas and implementation of calculating the gradient for logistic regression
- utilized those routines in
    - exploring a single variable data set
    - exploring a two-variable data set

---

## Optional lab: Logistic regression with scikit-learn

In this lab you will:
-  Train a logistic regression model using scikit-learn.

```
import numpy as np

X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y = np.array([0, 0, 0, 1, 1, 1])

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(X, y)

Output-

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
```

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression

- The code above imports the logistic regression model from scikit-learn. You can fit this model on the training data by calling fit function. The link to the logistic regression model is given as above.

- You can see the predictions made by this model by calling the predict function.

```
y_pred = lr_model.predict(X)

print("Prediction on training set:", y_pred)

Output - Prediction on training set: [0 0 0 1 1 1]
```

- You can calculate this accuracy of this model by calling the `score` function.

```
print("Accuracy on training set:", lr_model.score(X, y))

Output - Accuracy on training set: 1.0
```
