## Cost Function for Logistic Regression

- Cost function gives a way to measure how well a specific set of parameters fits the training data. Thereby gives us a way to try to choose better parameters. Here, we will see why a squared error cost function is not an ideal cost function for logistic regression
- In the training set, each row might correspond to patients that was paying a visit to the doctor and one dealt with some diagnosis. We will use m to denote the number of training examples.
- Each training example has one or more features, such as the tumor size, the patient's age, and so on for a total of n features. Let's call the features X_1 through X_n.
- Since this is a binary classification task, the target label y takes on only two values, either 0 or 1. The logistic regression model equation is as mentioned
- We need to understand that given this training set, how can we choose parameters w and b.

![image](https://github.com/user-attachments/assets/c6cb0090-9395-464a-ab4a-251694837e0d)

- We know the cost function for the squared error cost function. We also know that in case of a linear regression, where f of x is the linear function, w dot x plus b. The cost function looks like a convex function or a bowl shape or a hammock shape
- Gradient descent will be the one where it would start from one place and take steps to reach the local minimum or in other words converge at the global minimum
- If and when we try to use the same cost function for logistic regression. If I were to apply the logistic regression formula here, we would get a non-convex cost function and in such a case, if we try to use gradient descent. There are a lot of local minima that we can get stuck into. This makes the squared error cost function not a good choice for logistic regression
- Instead, there will be a different cost function that can make the cost function convex again. The gradient descent can be guaranteed to converge to the global minimum.
- In order to build a new cost function, one that is used for logistic regression, we are going to change the definition of cost function J of w and b.
- In the cost function formula, let's call the term inside the summation as the Loss function for a single training example. So Loss can be denoted now as L and as a function of the prediction of the learning algorithm, f of x as well as of the true label y. The loss given the predicted f of x and the true label y is equal in this case to 1.5 of the squared difference
- We will see shortly that by choosing a different form for this loss function, will be able to keep the overall cost function, which is 1 over n times the sum of these loss functions to be a convex function.

![image](https://github.com/user-attachments/assets/43b045ae-7b72-4453-b93e-50b679bed554)

- The loss function inputs f of x and the true label y and tells us how well we're doing on that example.
- If the label y is equal to 1, then the loss is negative log of f of x and if the label y is equal to 0, then the loss is negative log of 1 minus f of x.
- Let's first consider the case of y equals 1 and plot what this function looks like to gain some intuition about what this loss function is doing.
- Remember, the loss function measures how well you're doing on one training example and is by summing up the losses on all of the training examples that we get the cost function, which measures how well you're doing on the entire training set
- We can plot the log of f and negative log of f where f here is on the horizontal axis.

![image](https://github.com/user-attachments/assets/1a6ee1e2-e41e-49fe-8026-aab1b9ab9ec5)

- This plot of log of f and -log of f intersects the horizontal axis at f equals 1 and continues downward from there. Since, f is the output of logistic regression, f is always between zero and one because the output of logistic regression is always between zero and one. The major part of the function which is relevant is the part where f is between 0 and 1
- If the algorithm predicts a probability close to 1 and the true label is 1, then the loss is very small. It's pretty much 0 because we are very close to the right answer

![image](https://github.com/user-attachments/assets/622ace06-d4c4-4961-9afc-603016d06fbd)

- Let's continue with the example of the true label y being 1, say everything is a malignant tumor. If the algorithm predicts 0.5, then the loss is at the centre point, which is a bit higher but not that high.
- Whereas in contrast, if the algorithm were to have outputs at 0.1 if it thinks that there is only a 10 percent chance of tumor being malignant but y really is 1. If really is malignant, then the loss is much higher value.
- When y is equal to 1, the loss function incentivizes or nurtures, or helps push the algorithm to make more accurate predictions because the loss is lowest, when it predicts values close to 1.

---

- Now, let's look at the 2nd part of the loss function corresponding to when y is equal to 0.
- In this case, the loss is negative log of 1 minus f of x. The range of f is limited to 0 to 1 because logistic regression only outputs values between 0 and 1.
- When f is 0 or very close to 0, the loss is also going to be very small which means that if the true label is 0 and the model's prediction is very close to 0, so the loss is appropriately very close to 0.
- The larger the value of f of x gets, the bigger the loss because the prediction is further from the true label 0. In fact, as the prediction approaches 1, the loss actually approaches infinity.
- If the model predicts that the patient's tumor is almose certain to be malignant, say 99.9 percent chance of malignancy, that turns out to actually not be malignant, so y equals 0 then we penalize the model with a very high loss.
- In this case y equals 0, so this is in the case of y eqauls 1 on the previous slide, the further the prediction f of x is away from the true value of y, the higher the loss

![image](https://github.com/user-attachments/assets/07b12b6c-04d4-425c-b4b6-2a7ad5d5bede)
---
![image](https://github.com/user-attachments/assets/9e48289f-533a-4843-9dad-3770561bbbc3)

- Here, we saw why squared error cost function doesn't work well for the logistic regression. We defined the loss for a single training example and came up with a new definition for the loss function for the logistic regression.
- It turns out that with this choice of loss function, the overall cost function will be convex and thus you can reliably use gradient descent to take us to the global minimum. Proving that this function is convex is beyond the scope of this cost. 

---

## Optional Lab: Logistic Regression, Logistic Loss

In this ungraded lab, you will:
- explore the reason the squared error loss is not appropriate for logistic regression
- explore the logistic loss function

```
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example
from plt_logistic_loss import soup_bowl, plt_logistic_squared_error
plt.style.use('./deeplearning.mplstyle')
```

![image](https://github.com/user-attachments/assets/2dc05e41-482c-4dd6-9b3e-f291b86bab41)

This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out, $f_{wb}(x)$ now has a non-linear component, the sigmoid function:   $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$.   Let's try a squared error cost on the example from an earlier lab, now including the sigmoid.

Here is our training data:

```
x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)
y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)
plt_simple_example(x_train, y_train)
```
![image](https://github.com/user-attachments/assets/b0db6368-c6f6-4e4e-94e9-265808337d6f)

Now, let's get a surface plot of the cost using a *squared error cost*:
  $$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ 
 
where 
  $$f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$$

```
plt.close('all')
plt_logistic_squared_error(x_train,y_train)
plt.show()
```
![image](https://github.com/user-attachments/assets/ef503a55-8ce3-482f-b1d3-1b7dc28912c8)

While this produces a pretty interesting plot, the surface above not nearly as smooth as the 'soup bowl' from linear regression!    

Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below.

![image](https://github.com/user-attachments/assets/b937f82b-0b3b-4f99-ba8c-b4074c96e818)

Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. 

>**Definition Note:**   In this course, these definitions are used:  
**Loss** is a measure of the difference of a single example to its target value while the  
**Cost** is a measure of the losses over the training set


This is defined: 
* $loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:

\begin{equation}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = \begin{cases}
    - \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=1$}\\
    - \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) & \text{if $y^{(i)}=0$}
  \end{cases}
\end{equation}


*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.

*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.
* Notational convention: `log` means the natural logarithm. 

The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:

![image](https://github.com/user-attachments/assets/e6e3bd8a-b3ec-47b4-bbb3-f86ea9dd7f0e)

The loss function above can be rewritten to be easier to implement.
    $$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)$$
  
This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  
when $ y^{(i)} = 0$, the left-hand term is eliminated:
$$
\begin{align}
loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 0) &= (-(0) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 0\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \\
&= -\log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}
$$
and when $ y^{(i)} = 1$, the right-hand term is eliminated:
$$
\begin{align}
  loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), 1) &=  (-(1) \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - 1\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)\\
  &=  -\log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)
\end{align}
$$

OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:


```
plt.close('all')
cst = plt_logistic_cost(x_train,y_train)
```

![image](https://github.com/user-attachments/assets/7ed1ed56-e676-4b82-b9aa-c412499faa8a)

This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline. Reminder: you can rotate the above plots using your mouse.

You have:
  - determined a squared error loss function is not suitable for classification tasks
  - developed and examined the logistic loss function which is suitable for classification tasks.
