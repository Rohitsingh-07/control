## Cost Function for Logistic Regression

- Cost function gives a way to measure how well a specific set of parameters fits the training data. Thereby gives us a way to try to choose better parameters. Here, we will see why a squared error cost function is not an ideal cost function for logistic regression
- In the training set, each row might correspond to patients that was paying a visit to the doctor and one dealt with some diagnosis. We will use m to denote the number of training examples.
- Each training example has one or more features, such as the tumor size, the patient's age, and so on for a total of n features. Let's call the features X_1 through X_n.
- Since this is a binary classification task, the target label y takes on only two values, either 0 or 1. The logistic regression model equation is as mentioned
- We need to understand that given this training set, how can we choose parameters w and b.

![image](https://github.com/user-attachments/assets/c6cb0090-9395-464a-ab4a-251694837e0d)

- We know the cost function for the squared error cost function. We also know that in case of a linear regression, where f of x is the linear function, w dot x plus b. The cost function looks like a convex function or a bowl shape or a hammock shape
- Gradient descent will be the one where it would start from one place and take steps to reach the local minimum or in other words converge at the global minimum
- If and when we try to use the same cost function for logistic regression. If I were to apply the logistic regression formula here, we would get a non-convex cost function and in such a case, if we try to use gradient descent. There are a lot of local minima that we can get stuck into. This makes the squared error cost function not a good choice for logistic regression
- Instead, there will be a different cost function that can make the cost function convex again. The gradient descent can be guaranteed to converge to the global minimum.
- In order to build a new cost function, one that is used for logistic regression, we are going to change the definition of cost function J of w and b.
- In the cost function formula, let's call the term inside the summation as the Loss function for a single training example. So Loss can be denoted now as L and as a function of the prediction of the learning algorithm, f of x as well as of the true label y. The loss given the predicted f of x and the true label y is equal in this case to 1.5 of the squared difference
- We will see shortly that by choosing a different form for this loss function, will be able to keep the overall cost function, which is 1 over n times the sum of these loss functions to be a convex function.

![image](https://github.com/user-attachments/assets/43b045ae-7b72-4453-b93e-50b679bed554)

- The loss function inputs f of x and the true label y and tells us how well we're doing on that example.
- If the label y is equal to 1, then the loss is negative log of f of x and if the label y is equal to 0, then the loss is negative log of 1 minus f of x.
- Let's first consider the case of y equals 1 and plot what this function looks like to gain some intuition about what this loss function is doing.
- Remember, the loss function measures how well you're doing on one training example and is by summing up the losses on all of the training examples that we get the cost function, which measures how well you're doing on the entire training set
- We can plot the log of f and negative log of f where f here is on the horizontal axis.

![image](https://github.com/user-attachments/assets/1a6ee1e2-e41e-49fe-8026-aab1b9ab9ec5)

- This plot of log of f and -log of f intersects the horizontal axis at f equals 1 and continues downward from there. Since, f is the output of logistic regression, f is always between zero and one because the output of logistic regression is always between zero and one. The major part of the function which is relevant is the part where f is between 0 and 1
- If the algorithm predicts a probability close to 1 and the true label is 1, then the loss is very small. It's pretty much 0 because we are very close to the right answer

![image](https://github.com/user-attachments/assets/622ace06-d4c4-4961-9afc-603016d06fbd)

- Let's continue with the example of the true label y being 1, say everything is a malignant tumor. If the algorithm predicts 0.5, then the loss is at the centre point, which is a bit higher but not that high.
- Whereas in contrast, if the algorithm were to have outputs at 0.1 if it thinks that there is only a 10 percent chance of tumor being malignant but y really is 1. If really is malignant, then the loss is much higher value.
- When y is equal to 1, the loss function incentivizes or nurtures, or helps push the algorithm to make more accurate predictions because the loss is lowest, when it predicts values close to 1.

---

- Now, let's look at the 2nd part of the loss function corresponding to when y is equal to 0.
- In this case, the loss is negative log of 1 minus f of x. The range of f is limited to 0 to 1 because logistic regression only outputs values between 0 and 1.
- When f is 0 or very close to 0, the loss is also going to be very small which means that if the true label is 0 and the model's prediction is very close to 0, so the loss is appropriately very close to 0.
- The larger the value of f of x gets, the bigger the loss because the prediction is further from the true label 0. In fact, as the prediction approaches 1, the loss actually approaches infinity.
- If the model predicts that the patient's tumor is almose certain to be malignant, say 99.9 percent chance of malignancy, that turns out to actually not be malignant, so y equals 0 then we penalize the model with a very high loss.
- In this case y equals 0, so this is in the case of y eqauls 1 on the previous slide, the further the prediction f of x is away from the true value of y, the higher the loss

![image](https://github.com/user-attachments/assets/07b12b6c-04d4-425c-b4b6-2a7ad5d5bede)
---
![image](https://github.com/user-attachments/assets/9e48289f-533a-4843-9dad-3770561bbbc3)

- Here, we saw why squared error cost function doesn't work well for the logistic regression. We defined the loss for a single training example and came up with a new definition for the loss function for the logistic regression.
- It turns out that with this choice of loss function, the overall cost function will be convex and thus you can reliably use gradient descent to take us to the global minimum. Proving that this function is convex is beyond the scope of this cost. 
