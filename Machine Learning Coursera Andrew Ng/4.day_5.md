## Cost Function Formula

- In order to implement linear regression the first key step is first to define something called a cost function
- The cost function tells us how well the model is doing so that we can try to get it to do better.
- Recall that you have a training set that contains input features x and output targets y. The model you're going to use to fit this training set is this linear function f_w, b of x equals to w times x plus b. To introduce a little bit more terminology the w and b are called the parameters of the model. Parameters of the model are variables that we can adjust during training in order to improve the model.
- We also refer to the parameters of a model as coefficients or weights. Depending on the values we chose for w and b we get a different function f of x, which generates a different line on the graph.

![image](https://github.com/user-attachments/assets/392fc83b-5850-49be-bae7-6e534794f751)

- Above is the diagram which shows the line graph for different values of w and b incorporated in the function f. We take the vlaue of x and change it between 0 and 1 and get the points to plot for the function.
- We need to also make sure that the slope of the function should be correct and is calculated accordingly.
- For a given training set, with the help of linear regression we need to choose the values of the parameters w and b so that the straight line we get from the function f somehow fits the data well.
- A line that fits the data well can mean a line defined by f which is roughly passing through or somehwere close to the training examples as compared to other possible lines that are not as close to these points
- To remind, a training example like this point here is defined by x superscript i, y superscript i where y is the target (x^i, y^i).
- For a given input x^i, the function f also makes a predictive value for y and a value that it predicts to y is y hat i shown here. For our choice of a model f of x^i is w times x^i plus b
- Stated differently, the prediction y hat i is f of wb of x^i where for the model we're using f of x^i is equal to wx^i plus b

![image](https://github.com/user-attachments/assets/df992919-5c3e-4d8d-b202-d5c96db45af3)

---
- The goal is to find the values of w and b which keeps the prediction y^hat close to the true target y^i for many or maybe all training examples x^i, y^i. In order to find that, let's first take a look at how to measure how well a line fits the training data
- To do that, we're going to construct a cost function.
- The cost function takes the prediction y hat and compares it to the target y by taking y hat minus y. This difference is called the error. 
- We are measuring how far off to prediction is from the target. Next, let's compute the square of this error. Also, we're going to want to compute this term for different training examples i in the training set.
- When measuring the error, for example i, we'll compute this squared error term.
- Since we want to measure the error accross the entire training set, let's sum up the squared errors. We'll sum up from i equals 1 to m where m is the no of training examples.
- If we have more training examples, m is larger and the cost function will calculate a bigger number.
- To build a cost function that doesn't automatically get bigger as the training set size get larger by convention, we will compute the average square error instead of the total squared error and we do that by dividing by m.
- By convention, the cost function that machine learning people use actually divides by 2 times m. The extra division by 2 is just meant to make some of our later calculations look neater, but the cost function still works whether you include this division by 2 or not
- This cost function is referred to as J of wb. It is also called squared error cost function because we are taking square of these error terms.
- In machine learning different people will use different cost functions for different applications, but the squared error cost function is by far the most commonly used one for linear regression and for that matter, for all regression problems where it seems to give good results for many applications
  
![image](https://github.com/user-attachments/assets/6d0337c6-5f93-4eb7-a588-8720821200f8)

- Above, as you can we we have the whole convention on how we can write the function and its mathematical formula.

---
---

## Cost Function Intuition

- You want to fit a straight line to the training data, so you have this model, fw, b of x is w times x, plus b. Here, the model's parameters are w, and b. Now, depending on the values chosen for these parameters, you get different straight lines
- You want to find values for w, and b, so that the straight line fits the training data well. To measure how well a choice of w, and b fits the training data, you have a cost function J
- The cost function J measures the difference between the model's predictions, and the actual true values for y. linear regression would try to find values for w, and b, then make a J of w be as small as possible.
- This is the represention in a shorter way
  
![image](https://github.com/user-attachments/assets/f6021dd2-a798-4602-a9cd-e75e5cb80c8d)
---

#### Simplified Version
- In order to better visualize cost function J, we are going to work on a simplified version of Linear Regression Model. We're going to use the model fw of x, is w times x. It can be seen as taking the original model and getting rid of the parameter b, or setting the parameter b equal to 0. It just goes away from the equation, so f is now just w times x.
- We now have just one parameter w, and the cost function J, looks similar to what it was before. The goal becomes different as we have just one parameter, w, not w and b.
- With this simplified model, the goal is to find the value for w, that minimizes J of w
- To see this visually, what this means is that if b is set to 0, then f defines a line that looks as shown below. You see that the line passes through the origin here, because when x is 0, f of x is 0 too

![image](https://github.com/user-attachments/assets/1975627c-6099-4f98-b4b4-3a156f66cea7)

![image](https://github.com/user-attachments/assets/9d16dc6d-6fd9-4a16-9420-dab9d7ed83fa)

---

- Now, using this simplified model, let's see how the cost function changes as you choose different values for the parameter w. In particular, let's look at graphs of the model f of x, and the cost function J.
- First, notice that for f subscript w, when the parameter w is fixed, that is, is always a constant value, then fw is only a function of x, which means that the estimated value of y depends on the value of the input x
- In contrast, looking to the right, the cost function J, is a function of w, where w controls the slope of the line defined by f w. The cost defined by J, depends on a parameter, in this case, the parameter w
- We are going to mention the function f_w(x) as the left graph and J(w) as the right graph.
- On the left graph, the input feature x is on the horizontal axis, and the output variable is on the vertical axis.
- Here we don't have b or b is equal to 0 so the graph would be depending on the value of x.
- Let's say w = 1, in this case. the graph is as follows with the line of best fit.

![image](https://github.com/user-attachments/assets/0844378c-c3a7-44f1-96bd-ac4c27812451)

- The next thing that we can do it that we can clculate the cost J when w equals 1. We can substitute fw(X^i) with w times X^i. Where this expression is now w times X^i minus Y^i as follows
![image](https://github.com/user-attachments/assets/5e7c1031-1f0a-4ab9-8dd4-9810b353c251)

- Hence, we can say that for this particular data-set, when w is 1, then the cost J is equal to 0.
- Now, what we can do on the right is plot the cost function J. Notice that because the cost function is a function of the parameter w, the horizontal axis is now labeled w and not x, and the vertical axis is now J and not y
- We have J(1) equals to 0. In other words, when w equals 1, J(w) is 0
- For W, we can take on a range of values such as w can take on negative values, w can be 0, and it can take on positive values too. 

![image](https://github.com/user-attachments/assets/6a0769c1-ff66-4f8d-8c50-1485346dfc3d)

- Here, now we consider that w = 0.5 instead of 1. In that case the graph is shown above.
- On the left graph, it is seen as a line which has slope as 0.5.
- Now for x = 1, we have f(x) = 0.5 and the line follows that for different values of x and progresses ahead.
- Visually you can see that the error or difference is equal to the height of this vertical line here when x is equal to 1
- Because this lower line is the gap between the actual value of y and the value that the function f predicted, which is a bit further down here
- You can see in the diagram above, that we get the line plotted and then we calculate the cost function J(w) which comes out as 0.58 for w = 0.5

![image](https://github.com/user-attachments/assets/8575f843-2fe4-4920-9da7-db2dfc07f50a)

- When we take w = 0, the above graph shows the same methodology and gives out the graphs for both the sides.
- Since w can be any number, it can also be a negative value. if w is negative 0.5, then the line f is a downward slope. It turns out that when w is nigative 0.5, you end up with even a higher cost which is 5.25
- We can continue computing the codt function for different values of w and so on and plot these. It turns out that by computing a range of values, you can slowly trace out what the cost function J looks like and that's what J is

![image](https://github.com/user-attachments/assets/3d062866-eb02-48b2-9b97-b958342fc0e8)

---

- Choosing a value of w that causes J of w to be as small as possible seems like a good bet. J is the cost function that measures how big the squared errors are, so choosing w that minimizes these squared errors, makes them as small as possible, will give us a good model
- In this example, if you were to choose the value of w that results in the smallest possible value of J of w you'd end up picking w equals 1. This results in the line that fits the training data very well. That's how in linear regression you use the cost function to find the value of w that minimizes J.
- In the more general case where we had parameters w and b rather than just w, you find the values of w and b that minimize J. To summarize, you saw plots of both f and J and worked through how the two are related. As you vary w or vary w and b you end up with different straight lines and when that straight line passes across the data, the cost J is small
- The goal of linear regression is to find the parameters w or w and b that results in the smallest possible value for the cost function J.
