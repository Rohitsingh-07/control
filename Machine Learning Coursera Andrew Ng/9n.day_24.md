## The problem of overfitting

- In the learning algorithms, the algorithm can run into a problem called overfitting which can cause the model or the algorithm to perform poorly.
- Here, we are going to talk about overfitting and underfitting and how it affects the algorithm.
- There is also a method called regularization which is a very useful technique as it helps us reduce the overfitting problem and get the learning algorithm to work much better.

![image](https://github.com/user-attachments/assets/ef5e5339-0b4f-431c-94ac-fa1245ee7234)

- Here we are looking at three functions which are used to predict the house price
- The left most diagram is the one where we use linear function to fit the data. This results in a straight line. However, it is pretty clear that the function does not fit the data very well.
- The technical term for this is that the model is underfitting the training data. Another term is the algorithm has high bias.
- Here bias has a technical meaning, which means that it's just not even able to fit the training set that well. There's a clear pattern in the training data that the algorithm is just unable to capture.
- Another way to think of this form is as if the learning algorithm has a very strong preconception, or as we say a very strong bias, that the housing prices are going to be a completely linear function of the size despite data to the contrary.
- This preconception that the data is linear causes it to fit a straight line that fits the data poorly, leading it to underfitted data.

---

- If we look at a second variation which is if we insert for a quadratic function to the data with two features, x and x^2, then when we fit the parameters W1 and W2, we can get a curve that fits the data somewhat better.
- Also, if we get a new house, that's not in this set of five training examples. This model would probably do quite well on that new house. The idea that we want our learning algorithm to do well, even on examples that are not on the training set, that's called generalization.
- Technically we say that you want your learning algorithm to generalize well, which means to make good predictions even on brand new examples that it has never seen before
- These quadratic models seem to fit the training set not perfectly, but pretty well.

---

- Now, let's look at something more extreme which is what would happen if we were to apply a fourth order polynomial to the data. You have x, x^2, x^3, and x^4 all as features.
- With this fourth order polynomial, we can actually fir the curve that passes through all the five of the training examples exactly. We might get a curve that fits all the training data exactly.
- In fact, you'd be able to choose parameters that will result in the cost function being exactly equal to zero because the errors are zero on all five training examples.
- But this curve is very wiggly and it goes up and down all over the place. This is what makes this not so suitable for doing predictions. Technically, we would say that the model has overfit the data, or this model has an overfitting problem.
- It does not look like this model will generalize to new examples that's never seen before. Another term for this is that the algorithm has high variance. In ML, many people will use the term over-fit and high variance almost interchangeably. We'll use the terms underfit and high bias interchangeably. The intuition behind overfitting or high variance is that the algorithm is trying very hard to fir every single training example
- It turns out that if your training set were just even a little bit different, say one house was priced just a little bit more little bit less, then the function that the algorithm fits could end up being totally different
- If two different machine learning engineers were to fit this fourth-order polynomial model, to just slightly different datasets, they could end up with totally different predictions or highly variable predictions. That's why we say the algorithm has high variance.
- The middle model is the one which seems most suitable for making predictions. We can say that the goal of ML is to find a model that's not overfitting nor underfitting. A model which doesn't have high bias or high variance.

--- 

- In a similar manner as linear regression, overfitting applies to classification as well.

![image](https://github.com/user-attachments/assets/55826880-9f68-4c29-8810-b2b6878865c9)

- The image above shows the three different models
- In classification problems, we start with logistic regression models and we try to classify whether the tumor is benign or malignent. Here g is the sigmoid function and the term inside is z.
- If we do that, we end up with a straight line as the decision boundary. This is the line where z is equal to 0 that separates the positive and negative examples.
- This straight line is okay but it doesn't look like a very good fit to the data. This is the example of underfitting or high bias.

---

- If you were to add to your features these quadratic terms, then z becomes this new term in the middle and the decision boundary, that is where z equals zero can look more like this, more like an ellipse or part of an ellipse. This is a pretty good fit for the data, even though it does not perfectly classify every single training example in the training set.
- It looks like generalized pretty well to new patients.

--- 

- If we were to fit a very high-order polynomial with many features, then the model may try really hard and contoured or twist itself to find a decision boundary that fits your training data perfectly
- Having all these higher-order polynomial features allows the algorithm to choose this really over the complex decision boundary. If the features are tumor size in age, and you're trying to classify tumors as malignant or benign, then this doesn't really look like a very good model for making predictions
- Once again, this is an instance of overfitting and high variance because its model, despite doing very well on the training set, doesn't look like it'll generalize well to new examples. Now you've seen how an algorithm can underfit or have high bias or overfit and have high variance. You may want to know how you can give get a model that is just right
  
