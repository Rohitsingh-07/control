## Gradient Descent for Multiple Linear Reggression

- We saw gradient descents, multiple linear regression and vectorization in the last lectures. Let's take refernece of those and implement gradient descent for multiple linear regression with vectorization.

![image](https://github.com/user-attachments/assets/047b5da8-51ea-4196-8de4-602a30b21216)

- Above is a revision of the previous lectures
- So in gradient descent with one feature we just use the update rule to update the weights for a single feature
- When we have more than 1 feature to update, we update it one by one with respect to a given example and a given weight

![image](https://github.com/user-attachments/assets/c5ad0025-3f95-4558-b2f2-f9243b6d7f29)

- Here is a represenation of the given lecture for Gradient Descent for Multiple Linear Regression
---

- We also have an alternate way on finding w and b for linear regression
- Whereas it turns out gradient descent is a great method for minimizing the cost function J to find w and b, there is one other algorithm that works only for linear regression and pretty much none of the other algorithms you see in this specialization for solving for w and b and this other method does not need an iterative gradient descent algorithm. Called the normal equation method, it turns out to be possible to use an advanced linear algebra library to just solve for w and b all in one goal without iterations
- Some disadvantages of the normal equation method are; first unlike gradient descent, this is not generalized to other learning algorithms, such as the logistic regression algorithm that you'll learn about next week or the neural networks or other algorithms you see later in this specialization
- The normal equation method is also quite slow if the number of features and this large. Almost no machine learning practitioners should implement the normal equation method themselves but if you're using a mature machine learning library and call linear regression, there is a chance that on the backend, it'll be using this to solve for w and b
- Just be aware that some machine learning libraries may use this complicated method in the back-end to solve for w and b. But for most learning algorithms, including how you implement linear regression yourself, gradient descents offer a better way to get the job done.

![image](https://github.com/user-attachments/assets/e78cf4aa-b02a-49fd-b3ab-e4bf640a178c)
