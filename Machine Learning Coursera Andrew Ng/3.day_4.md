# Linear Regression Model Part 2
---
- Recall that a training set in supervised learning includes both the input features, such as the size of the house and also the output targets, such as the price of the house.
- The output targets are the right answers to the model we'll learn from. To train the model, you feed the training set, both the input features and the output targets to your learning algorithm.
- Then the supervised learning algorithm produces some function which can be denoted as "f". Initially, this function used to be called as hypothesis, but we are going to denote it with "f".
- The function f takes a new input x and output and estimate or predicts, which is called as y-hat. In machine learning, the convention is that y-hat is the estimate or the prediction of y.
- The function f is called the model. X is called the input or the input feature, and the output of the model is the prediction, y-hat. The model's prediction is the estimated value of y.
- When the symbol is just the letter y, then that refers to the target, which is the actual true value in the training set. In contrast, y-hat is an estimate. y-hat may or may not be the actual true value.
- When we design a learning algorithm, an important question is how are we going to represent the function f, or in other words, what is the math formula we're going to use to compute f.
- For now, let's stick with f being a straight line. the function can be written as f_w,b of x equals w times x plus b. I'll define w and b soon.
- But for now, just know that w and b are numbers, and the values chosen for w and b will determine the prediction y-hat based on the input feature x

- This f_w b of x means f is a function that takes x as input, and depending on the values of w and b, f will output some value of a prediction y-hat. As an alternative to writing this, f_w, b of x, I'll sometimes just write f of x without explicitly including w and b into subscript

![image](https://github.com/user-attachments/assets/3f6943c0-85b6-467b-8c04-7cbe0e2e67aa)

- The diagram above shows all the plots and the representation of the equation.
- In the diagram above, the input feature x is on the horizontal axis and the output target y is on the vertical axis. The algorithm learns from this data and generates the best-fit line.
- This straight line as shown in the figure is the linear function. Here it's making predictions for the value of y using a streamline function of x.
- A question may arise that why are we using a linear function where linear function is just a fancy term for a straight line instead of some non linear function like a curve or a parabola.
- A linear function is relatively simple and easy to work with and it can be used as a foundation that will eventually help us to get to more complex models that are non-linear.
- This particular model has a name, it's called linear regression. More specifically, this is linear regression with one input variable. Another name for a linear model with one variable is univariate linear regression.
---

# Optional Lab
---
File Loc: D:\ML Andrew Ng\Linear Regression Lab.ipynb

- The first line in the lab imports numpy and matplotlib

```
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
```

- Here the line ```plt.style.use('./deeplearning.mplstyle')``` refers to
-  Sets a custom style for plots using Matplotlib's style.use() function.

./deeplearning.mplstyle refers to a local file (in the same directory as your script) that contains styling configurations.

.mplstyle files can define things like:

- Font sizes
- Background and grid color
- Line widths and colors
- Figure sizes
- And other visual preferences

Use this prompt in Chatgpt to get a clear idea

```
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')

Explain what is hapenning here
```

2. The next step is we create two arrays of the given training data which contains input features of training data x_train and target y_train. Python f string is used for better code understanding

```
# x_train is the input variable (size in 1000 square feet)
# y_train is the target (price in 1000s of dollars)
x_train = np.array([1.0, 2.0])
y_train = np.array([300.0, 500.0])
print(f"x_train = {x_train}")
print(f"y_train = {y_train}")

Output-

x_train = [1. 2.]
y_train = [300. 500.]
```

##### Number of training examples m¶
- You will use m to denote the number of training examples. Numpy arrays have a .shape parameter. x_train.shape returns a python tuple with an entry for each dimension. x_train.shape[0] is the length of the array and number of examples as shown below.

```
# m is the number of training examples
print(f"x_train.shape: {x_train.shape}")
m = x_train.shape[0]
print(f"Number of training examples is: {m}")

Output -
x_train.shape: (2,)
Number of training examples is: 2
```

- We can also use the Python len() function as shown below

```
# m is the number of training examples
m = len(x_train)
print(f"Number of training examples is: {m}")

Output -
Number of training examples is: 2
```

- Training example x_i, y_i
You will use (x (𝑖), y (𝑖)) to denote the  i𝑡ℎ training example. Since Python is zero indexed, (x (0), y (0)) is (1.0, 300.0) and (x (1), y (1)) is (2.0, 500.0).

- To access a value in a Numpy array, one indexes the array with the desired offset. For example the syntax to access location zero of x_train is x_train[0]. Run the next code block below to get the  i𝑡ℎ training example.

```
i = 0 # Change this to 1 to see (x^1, y^1)

x_i = x_train[i]
y_i = y_train[i]
print(f"(x^({i}), y^({i})) = ({x_i}, {y_i})")

Output-
(x^(0), y^(0)) = (1.0, 300.0)
```
---

##### Plotting the data
- You can plot these two points using the scatter() function in the matplotlib library, as shown in the cell below.

- The function arguments marker and c show the points as red crosses (the default is blue dots). You can use other functions in the matplotlib library to set the title and labels to display

```
# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r')
# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 1000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (1000 sqft)')
plt.show()
```
Output - 
![image](https://github.com/user-attachments/assets/fd352908-6059-4c13-8a66-6bd3a1eac637)

![image](https://github.com/user-attachments/assets/c9e5a5c3-d169-48e4-b0ad-2bc6c96e25e2)

---
```
w = 100
b = 100
print(f"w: {w}")
print(f"b: {b}")

Output -

w: 100
b: 100
```

- Now, let's compute the value of  𝑓𝑤,𝑏(𝑥(𝑖)) for your two data points. You can explicitly write this out for each data point as -

for  𝑥(0), f_wb = w * x[0] + b
for  𝑥(1), f_wb = w * x[1] + b

- For a large number of data points, this can get unwieldy and repetitive. So instead, you can calculate the function output in a for loop as shown in the compute_model_output function below.
```
x = (np.ndarray(m,))
print(x)

Output-
[0.5 1. ]

Explanation: This line is creating a NumPy array using the np.ndarray constructor — but there's a better way to write this. Let’s explain what’s happening and how you should do it.
It just provides random float values for the given n dimensional array with the shape m in this case 2
```

```
def compute_model_output(x, w, b):
    """
    Computes the prediction of a linear model
    Args:
      x (ndarray (m,)): Data, m examples 
      w,b (scalar)    : model parameters  
    Returns
      f_wb (ndarray (m,)): model prediction
    """
    m = x.shape[0]
    f_wb = np.zeros(m)
    for i in range(m):
        f_wb[i] = w * x[i] + b
        
    return f_wb
```

- Function call

```
tmp_f_wb = compute_model_output(x_train, w, b,)
print(tmp_f_wb)

Output -
[200. 300.]
```

```
tmp_f_wb = compute_model_output(x_train, w, b,)

# Plot our model prediction
plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')

# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')

# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 1000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.show()
```

Ouput -
![image](https://github.com/user-attachments/assets/03195d5a-5da0-4d70-9e14-77da253e15a0)

As you can see, setting  𝑤=100 and 𝑏=100 does not result in a line that fits our data.

- Once we try to find the right value for the w and b we get the line that fits our data. trying w = 200 and b = 100

This gives the output as

![image](https://github.com/user-attachments/assets/c2a1bdd5-3bf2-4289-af3b-8e446320c996)

- Prediction
Now that we have a model, we can use it to make our original prediction. Let's predict the price of a house with 1200 sqft. Since the units of  𝑥 are in 1000's of sqft,  𝑥 is 1.2.

```
w = 200                         
b = 100    
x_i = 1.2
cost_1200sqft = w * x_i + b    

print(f"${cost_1200sqft:.0f} thousand dollars")

Output-
$340 thousand dollars
```

Congratulations!
In this lab you have learned:

- Linear regression builds a model which establishes a relationship between features and targets
  - In the example above, the feature was house size and the target was house price
  - for simple linear regression, the model has two parameters  𝑤 and  𝑏 whose values are 'fit' using training data.
  - once a model's parameters have been determined, the model can be used to make predictions on novel data.
 
## Roadmap to solution

get the training data x_trian, y_train >> No of Training Examples >> Training example x_i, y_i >> Plot the training data >> Set the model parameters In case of line y = mx+c or here f(x) = wx+b >> Calculate the function for every training example >> Create a loop for that which would be much easier >> Set the parameters w and b in such a way that it fits our data >> As it fits predict with our new data >>

.
