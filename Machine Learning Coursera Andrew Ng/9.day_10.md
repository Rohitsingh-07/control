# Regression with Multiple Input Variables

- This week, you'll extend linear regression to handle multiple input features. You'll also learn some methods for improving your model's training and performance, such as vectorization, feature scaling, feature engineering and polynomial regression. At the end of the week, you'll get to practice implementing linear regression in code.
---
- Learning Obejectives
  1. Use vectorization to implement multiple linear regression
  2. Use feature scaling, feature engineering, and polynomial regression to improve model training
  3. Implement linear regression in code
 
---

## Multiple Features

- In this lecture, we will look at the linear regression, that look at not just one feature, but a lot of different features.
- In the original version of linear regression, we had a single feature x, the size of the house and we were able to predict y, the price of the house. The model was fwb of x equals wx plus b.
- But now, what if we knew about the size of the house, the number of bedrooms, the number of floors and the age of the home in years. This information would give us a lot more information with which to predict the price
- This information is called the features and to denote these features, we are going to use the variables X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> and X<sub>4</sub>
- We would write X<sub>j</sub> to represent the list of features and j should be the feature number
- Here j would go from 1 to 4 since we have 4 features and n should be the number of feature and here n = 4
- We will keep the same notation for the example X<sup>i</sup>
- Here X<sup>i</sup> is going to be a list of 4 numbers, or a vector that includes all the features of the i<sup>th</sup> training example
- So a given example number would be the entire row of the data and that wouldbe a row vector rather tahn a column vector.
- To refer to a specific feature in the ith training example, we can write X<sub>j</sub><sup>(i)</sup>
- So for example X<sub>3</sub><sup>(2)</sup>  = 2 as per the image below
- Sometimes, we would also have a small arrow with the supercript X just to show that that example is not a feature value but the entire example list or the row vector

![image](https://github.com/user-attachments/assets/37be5dcf-1dfb-4c2f-a309-3e196bf9d074)

---
- Previously since we had one example, so we defined the model f = wx + b. However, now we have multiple examples and hence we are going to use all of the examples and their respective weights to reach the efficient value of the model

![image](https://github.com/user-attachments/assets/1c5c35d7-7d7f-47bf-a983-6d0250042758)

- So with reference to the above figure, it says that the price may increase with a given weight for the first feature that means the price may increase 0.1 times of X1 of whatever the size is and all other features respective increases of price.

![image](https://github.com/user-attachments/assets/1a5c5b75-643a-47a7-ba53-58865648269b)

- In general, when we have n features, the model looks like the first equation
- Here w is the parameters which will have multiple values and hence it is also a vector. b is a single number or a scalar and not a vector
- W and b as usual are the parameters of the model
- X the examples of the training data is also a vector and it is represented above
- The model would now be as w.x where the dot represents the dot product between the two vectors w and x
- The name of this type of model with multiple input features is multiple linear regression and this is in contrast to univariate regression which has just one feature
- This algorithm is not called multivariate regression since that term refers to something else
- This is linear regression with multiple features aka muliple linear regression. In order to implement this, there's a really neat trick called vectorization, which will make it much simpler to implement this and many other learning algorithms.

  
