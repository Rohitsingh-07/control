# Regression with Multiple Input Variables

- This week, you'll extend linear regression to handle multiple input features. You'll also learn some methods for improving your model's training and performance, such as vectorization, feature scaling, feature engineering and polynomial regression. At the end of the week, you'll get to practice implementing linear regression in code.
---
- Learning Obejectives
  1. Use vectorization to implement multiple linear regression
  2. Use feature scaling, feature engineering, and polynomial regression to improve model training
  3. Implement linear regression in code
 
---

- In this lecture, we will look at the linear regression, that look at not just one feature, but a lot of different features.
- In the original version of linear regression, we had a single feature x, the size of the house and we were able to predict y, the price of the house. The model was fwb of x equals wx plus b.
- But now, what if we knew about the size of the house, the number of bedrooms, the number of floors and the age of the home in years. This information would give us a lot more information with which to predict the price
- This information is called the features and to denote these features, we are going to use the variables X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub> and X<sub>4</sub>
- We would write X<sub>j</sub> to represent the list of features and j should be the feature number
- Here j would go from 1 to 4 since we have 4 features and n should be the number of feature and here n = 4
- We will keep the same notation for the example X<sup>i</sup>
- Here X<sup>i</sup> is going to be a list of 4 numbers, or a vector that includes all the features of the i<sup>th</sup> training example
