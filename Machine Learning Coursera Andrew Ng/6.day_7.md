# Gradient Descent
---

- It woudl be great to have a more systematic way to find the values of w and b, the results in the smallest possible cost, j of w, b. It turns out there's an algorithm called gradient descent that we can use.
- Gradient descent is used in Linear regression and also in some of the most advanced neural network models such as deep learning models.
- We have a cost function j of w,b right here that we want to minimize.
- Gradient descent is an algorithm that we can use to minimize any function, not just cost function for linear regression.
- Gradient descent applies to more general functions, including other cost functions that work with models that have more than two parameters.
- For instance, if you have a cost function J as a function of w_1, w_2 up to w_n and b, your objective is to minimize j over the parameters w_1 to w_n and b. In other words, you want to pick values for w_1 through w_n and b, that gives you the smallest possible value of j. It turns out that gradient descent is an algorithm that you can apply to try to minimize this cost function j as well
- Just to start off, we would need some initial guesses for w and b. In linear regression, it won't matter too much what the initial value are, so a common choice is to set them both to 0.
- For Example, we can set w to 0 and b to 0 as the initial guess. With the gradient descent algorithm, we will keep on changing the parameters w and b a bit everytime to reduce the cost of j of w,b until hopefully j settles at or near a minimum.
- One thing is possible is that for some functions j that may not be a bow shape or a hammock shape, it is possible for there to be more than one possible minimum.

![image](https://github.com/user-attachments/assets/56730058-fc9d-4a01-9b91-beb5818d78ae)

---

![image](https://github.com/user-attachments/assets/404329af-011e-4311-be82-b69f37bd6912)

- Above is an example of a more complex surface plot j.
- This function is not a squarred error cost function. For linear regression with squared error cost function, you always end up with a bow shape or a hammock shape.
- The above figure is a type of cost function we might get if we are training a neural network model.
- The axes are w and b on the bottom axis. For different values of w and b, you get different points on this surface, j of w, b, where the height of the surface at some point is the value of the cost function
- So we can take an exam of the shape to match it with a hilly area pasture. We start of with any point and then the focus is to start up at that point and get to the bottom of one of these valleys as efficiently as possible.
- What the gradient descent algorithm does is, you're going to spin around 360 degrees and look around and ask yourself, if I were to take a tiny little baby step in one direction, and I want to go downhill as quickly as possible to or one of these valleys. What direction do I choose to take that baby step?
-  Well, if you want to walk down this hill as efficiently as possible, it turns out that if you're standing at this point in the hill and you look around, you will notice that the best direction to take your next step downhill is roughly that direction. Mathematically, this is the direction of steepest descent
-  It means that when you take a tiny baby little step, this takes you downhill faster than a tiny little baby step you could have taken in any other direction
-  After taking this first step, you're now at this point on the hill over here. Now let's repeat the process.
-  Take another step, another step, and so on, until you find yourself at the bottom of this valley, at the local minimum. What we did was go through multiple steps of gradient descent
-  We can choose the starting point at the surface by choosing the starting values for the parameters w and b.
-  Let's say we start the gradient descent again from a different starting point now. If we then repeat the gradient descent process, we can end up in a totally different valley.
-  The bottoms of both the first and second valleys are called local minima.

![image](https://github.com/user-attachments/assets/ba7169c3-98fb-4c2e-9bee-af0050bb3e34)

- Below we can see the entire representation of the lecture and the gradient descent formula

---

# Implementing Gradient Descent

- On each step, w, the parameter

$$
\omega = \omega - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$

- This expression says that, update the parameter w by taking the current value of w and adjusting it a small amount, which is the expression on the right, minus Alpha times the derivative term
- Here the equal sign '=' is an assignment operator which we use in python where we assign a value to a given variable. In Truth Assertion, which is used in mathematics we get to see the equal and it means that value is equal to the other value. So let's be vary of that representation

- In the equation above </br>

&alpha; - Learning rate (This is a small positive number between 0 and 1 and it might be say, 0.01) </br>

- Learning rate basically controls how big of a step we take downhill.  If Alpha is very large, then that corresponds to a very aggressive gradient descent procedure where you're trying to take huge steps downhill. If Alpha is very small, then you'd be taking small baby steps downhill.

$$
\frac{\partial}{\partial \omega} J(\omega, b)
$$

- This in simpler terms mean that in which direction we want to take the baby step
- In combination of the learning rate Alpha, it also determines the size of the steps you want to take downhill.
- We have two parameters, not just w, but also b. We also have an assignment operations update the parameter b that looks very similar.

$$
b = b - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$
