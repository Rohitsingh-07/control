## Optional Lab Gradient Descent

- In this lab, we automate the process of optimizing w and b using gradient descent
- The tools that we use are Numpy, Matplotlib and labs_util.py file

```
import math, copy
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients
```

- We import the required files and the libraries
- We are using the same examples that we used previously in our old labs
```
# Load our data set
x_train = np.array([1.0, 2.0])   #features
y_train = np.array([300.0, 500.0])   #target value
```

- The compute cost function that we created last time and we are using this time

```
#Function to calculate the cost
def compute_cost(x, y, w, b):
   
    m = x.shape[0] 
    cost = 0
    
    for i in range(m):
        f_wb = w * x[i] + b
        cost = cost + (f_wb - y[i])**2
    total_cost = 1 / (2 * m) * cost

    return total_cost
```

<a name="toc_40291_2.1"></a>
### Gradient descent summary
So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:
$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$
In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples</br>
$x^{(i)},y^{(i)}$ </br>
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\tag{2}$$ 


In lecture, *gradient descent* was described as:

$$
\text{repeat until convergence:} \quad
\begin{cases}
w = w - \alpha \frac{\partial J(w, b)}{\partial w} \\
b = b - \alpha \frac{\partial J(w, b)}{\partial b}
\end{cases}
$$

where parameters \( w \), \( b \) are updated **simultaneously**.

The gradient is defined as:

$$
\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) x^{(i)}
$$

$$
\frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})
$$

Here, **simultaneously** means that you calculate the partial derivatives for all the parameters **before updating any** of the parameters.
