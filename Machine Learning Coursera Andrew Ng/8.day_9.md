## Optional Lab Gradient Descent

- In this lab, we automate the process of optimizing w and b using gradient descent
- The tools that we use are Numpy, Matplotlib and labs_util.py file

```
import math, copy
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients
```

- We import the required files and the libraries
- We are using the same examples that we used previously in our old labs
```
# Load our data set
x_train = np.array([1.0, 2.0])   #features
y_train = np.array([300.0, 500.0])   #target value
```

- The compute cost function that we created last time and we are using this time

```
#Function to calculate the cost
def compute_cost(x, y, w, b):
   
    m = x.shape[0] 
    cost = 0
    
    for i in range(m):
        f_wb = w * x[i] + b
        cost = cost + (f_wb - y[i])**2
    total_cost = 1 / (2 * m) * cost

    return total_cost
```

<a name="toc_40291_2.1"></a>
### Gradient descent summary
So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:
$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$
In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples</br>
$x^{(i)},y^{(i)}$ </br>
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\tag{2}$$ 


In lecture, *gradient descent* was described as:

$$
\text{repeat until convergence:} \quad
\begin{cases}
w = w - \alpha \frac{\partial J(w, b)}{\partial w} \\
b = b - \alpha \frac{\partial J(w, b)}{\partial b}
\end{cases}
$$

where parameters \( w \), \( b \) are updated **simultaneously**.

The gradient is defined as:

$$
\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) x^{(i)}
$$

$$
\frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})
$$

Here, **simultaneously** means that you calculate the partial derivatives for all the parameters **before updating any** of the parameters.

### Implement Gradient Descent
- You will implement gradient descent algorithm for one feature. You will need three functions.
- We will compute the partial derivative functions by creating a function out of it and then we will move ahead

- Compute Gradient Function
```
def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
    
    # Number of training examples
    m = x.shape[0]    
    dj_dw = 0
    dj_db = 0
    
    for i in range(m):  
        f_wb = w * x[i] + b 
        dj_dw_i = (f_wb - y[i]) * x[i] 
        dj_db_i = f_wb - y[i] 
        dj_db += dj_db_i
        dj_dw += dj_dw_i 
    dj_dw = dj_dw / m 
    dj_db = dj_db / m 
        
    return dj_dw, dj_db
```

![image](https://github.com/user-attachments/assets/14a24e23-4f77-43fb-80ba-bbf4a9d54401)
---

![image](https://github.com/user-attachments/assets/ca5c2767-43c0-4234-a86f-3ca44b4d67f9)

- Above, the left plot shows  ∂𝐽(𝑤,𝑏)/∂𝑤 or the slope of the cost curve relative to  𝑤 at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero.
- The left plot has fixed  𝑏=100. Gradient descent will utilize both  ∂𝐽(𝑤,𝑏)/∂𝑤 and  ∂𝐽(𝑤,𝑏)/∂𝑏 to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of  ∂𝐽(𝑤,𝑏)/∂𝑤 and  ∂𝐽(𝑤,𝑏)/∂𝑏 at that point.
- Note that the gradient points away from the minimum. Review equation (3) above. The scaled gradient is subtracted from the current value of  𝑤 or  𝑏. This moves the parameter in a direction that will reduce cost.
---
- The quiver plot is used to see the gradient direction and its magnitude. We should focus on the point where the magnitude is minimum and the direction is just a point. It represents the points as vector and hence we see that the position of w = 200 with is the best as it has the most minimum gradient. There are also other points with similar point but it is on the negative Y axis

---
- So, once the gradients (the partial derivative terms) are computed. We then move towards the gradient descent implementation that is to update w and b until convergence
```
def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): 
    """
    Performs gradient descent to fit w,b. Updates w,b by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      x (ndarray (m,))  : Data, m examples 
      y (ndarray (m,))  : target values
      w_in,b_in (scalar): initial values of model parameters  
      alpha (float):     Learning rate
      num_iters (int):   number of iterations to run gradient descent
      cost_function:     function to call to produce cost
      gradient_function: function to call to produce gradient
      
    Returns:
      w (scalar): Updated value of parameter after running gradient descent
      b (scalar): Updated value of parameter after running gradient descent
      J_history (List): History of cost values
      p_history (list): History of parameters [w,b] 
      """
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    p_history = []
    b = b_in
    w = w_in
    
    for i in range(num_iters):
        # Calculate the gradient and update the parameters using gradient_function
        dj_dw, dj_db = gradient_function(x, y, w , b)     

        # Update Parameters using equation (3) above
        b = b - alpha * dj_db                            
        w = w - alpha * dj_dw                            

        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( cost_function(x, y, w , b))
            p_history.append([w,b])
        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.3e}, b:{b: 0.5e}")
 
    return w, b, J_history, p_history #return w and J,w history for graphing
```
```
if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.3e}, b:{b: 0.5e}")
```
- This part particularly prints the values with restricted number of character prior to decimals in some case and port decimals in some case
```
# initialize parameters
w_init = 0
b_init = 0
# some gradient descent settings
iterations = 10000
tmp_alpha = 1.0e-2
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, 
                                                    iterations, compute_cost, compute_gradient)
print(f"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})")
```
- Here we then place the intial values of w and b that is to 0 and then calculate the given terms
---
- We take learning rate with a given guideline </br>
| Model Type | Recommended Initial α |
|------------|-----------------------|
| Simple linear regression	| 0.01 or 0.001 |
| Neural networks |	0.001 or 0.0001 |
| Logistic regression	| 0.01 or 0.001 |
| Large-scale datasets	| 0.0001 to 0.01 |
---
![image](https://github.com/user-attachments/assets/5a0295d9-3e90-4312-8db2-c2ab01e9c20f)

![image](https://github.com/user-attachments/assets/4ca6f4b0-2912-4942-bd1d-9329842ffae4)

```
# plot cost versus iteration  
fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))
ax1.plot(J_hist[:100])
ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])
ax1.set_title("Cost vs. iteration(start)");  ax2.set_title("Cost vs. iteration (end)")
ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') 
ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') 
plt.show()
```

![image](https://github.com/user-attachments/assets/d7b5a87a-5857-4d02-8827-c9bca1ec9f35)

- Now that you have discovered the optimal values for the parameters  𝑤 and  𝑏, you can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value.

```
print(f"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars")
print(f"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars")
print(f"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars")
```
