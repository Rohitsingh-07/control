#### Optional Lab 2

- In the 2nd optional lab we use custome functions from the python file of labs_util_uni and labs_util_common

- In the common file,
  - we are creating a dictionary with the given colors that we need
  - we are creating a custom cost function to call whenever we need
  - In this cost function, we use verbose which is usually used to print the value of the predicted function f_wb. By default the verbose is false but still we call it because we can use it to debuf when required

lab_util_common_code

```
import numpy as np
import matplotlib.pyplot as plt

dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0';
dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]
dlc = dict(dlblue = "#0096ff", dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')

# Function to compute the cost
def compute_cost_matrix(x,y,w,b,verbose=False):
    m = x.shape[0]

    f_wb = x@w + b

    total_cost = (1/(2*m))*np.sum((f_wb-y)**2)

    if verbose : print('f_wb:')
    if verbose : print(f_wb)

    return total_cost

# Function to compute the partial derivative functions

def compute_gradient_matrix(x,y,w,b):
    m,n=x.shape
    f_wb = x @ w + b
    e = f_wb - y
    dj_dw = (1/m)*(x.T @ e)
    dj_db = (1/m)*np.sum(e)

    return dj_dw, dj_db

'''Here you can see that we are using the x.T and not just multiplying with x because when it is a single example we just multiply with x but when it is a vector that is 
the training example is a vector then we use the transpose so it is safe to always use a transpose
'''

# Loop version of multi-variable compute_cost

def compute_cost(x,y,w,b):
    m = x.shape[0]
    cost = 0.0
    for i in range(m):
        f_wb_i: np.dot(x[i],w)+b
        cost = cost+(f_wb_i - y[i])**2
    cost = cost/(2*m)
    return cost

def compute_gradient(x,y,w,b):
    m,n=x.shape
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):
        err = (np.dot(x[i],w)+b)-y[i]
        for j in range(n):
            dj_dw=dj_dw[j] + err * x[i,j]
        dj_db = dj_db + err
    dj_dw = dj_dw/m
    dj_db = dj_db/m
```

This code is for single cost computation and multi variable cost computation along with the calculation of the partial derivatives
