#### Optional Lab 2

- In the 2nd optional lab we use custome functions from the python file of labs_util_uni and labs_util_common

- In the common file,
  - we are creating a dictionary with the given colors that we need
  - we are creating a custom cost function to call whenever we need
  - In this cost function, we use verbose which is usually used to print the value of the predicted function f_wb. By default the verbose is false but still we call it because we can use it to debuf when required

lab_util_common_code

```
import numpy as np
import matplotlib.pyplot as plt

dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0';
dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]
dlc = dict(dlblue = "#0096ff", dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')

# Function to compute the cost
def compute_cost_matrix(x,y,w,b,verbose=False):
    m = x.shape[0]

    f_wb = x@w + b

    total_cost = (1/(2*m))*np.sum((f_wb-y)**2)

    if verbose : print('f_wb:')
    if verbose : print(f_wb)

    return total_cost

# Function to compute the partial derivative functions

def compute_gradient_matrix(x,y,w,b):
    m,n=x.shape
    f_wb = x @ w + b
    e = f_wb - y
    dj_dw = (1/m)*(x.T @ e)
    dj_db = (1/m)*np.sum(e)

    return dj_dw, dj_db

'''Here you can see that we are using the x.T and not just multiplying with x because when it is a single example we just multiply with x but when it is a vector that is 
the training example is a vector then we use the transpose so it is safe to always use a transpose
'''

# Loop version of multi-variable compute_cost

def compute_cost(x,y,w,b):
    m = x.shape[0]
    cost = 0.0
    for i in range(m):
        f_wb_i: np.dot(x[i],w)+b
        cost = cost+(f_wb_i - y[i])**2
    cost = cost/(2*m)
    return cost

def compute_gradient(x,y,w,b):
    m,n=x.shape
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):
        err = (np.dot(x[i],w)+b)-y[i]
        for j in range(n):
            dj_dw=dj_dw[j] + err * x[i,j]
        dj_db = dj_db + err
    dj_dw = dj_dw/m
    dj_db = dj_db/m
```

This code is for single cost computation and multi variable cost computation along with the calculation of the partial derivatives

---

#### Optional Lab: Linear Regression using Scikit-Learn

- Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize a loss (or cost) function — basically, to find the "best" parameters for a model.
Here’s the breakdown in plain terms:

1️⃣ Gradient Descent – The Big Picture
Goal: Find parameter values (like weights in regression) that minimize the error between your predictions and actual values.

Idea:

Start with some guess for the parameters.

Calculate the gradient (slope) of the loss function — tells you the direction of steepest increase.

Move in the opposite direction of the gradient to reduce the loss.

Repeat until the loss is small enough or we run out of patience.

2️⃣ Why “Stochastic” Gradient Descent?
In batch gradient descent, you calculate the gradient using the entire dataset — accurate but slow for large datasets.

SGD speeds this up by:

Using one random training example (or a small random batch) at a time to update the weights.

This makes updates noisier but often much faster and sometimes even better at escaping local minima.

- An epoch is one pass of the entire dataset
- Unlike batch gradient descent, which waits until it sees the whole dataset to make one update per epoch, SGD updates after each sample (or mini-batch).
- This adds randomness (“stochasticity”), which can help it escape local minima but also makes the path noisier.
- n_iter is the number of time the algorithm goes through the entire dataset, t_ is the no of times the algorithm updates the weights.
- print(sgdr) we basically get the configuration we ended up after fitting

```
b_norm = sgdr.intercept_
w_norm = sgdr.coef_
print(f"model parameters:                   w: {w_norm}, b:{b_norm}")
print( "model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16")
```

- sgdr.intercept_ gives the value of b after updating the weights
- sgdr.coef_ gives the values of w after updating the weight

- sgdr.predict is the function which is used for presicting the new values, when we matched it with the the tradition wx+b results the return was true and each weight matched.
- 
