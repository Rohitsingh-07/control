## Vectorization - Part 1

- Vectorization is used to make code shorter and run it more efficiently
- Learning how to write vectorized code will allow you to also take advantage of modern numerical linear algebra libraries, as well as maybe even GPU hardware
- This is hardware objectively designed to speed up computer graphics in your computer, but turns out can be used when you write vectorized code to also help you execute your code much more quickly
- So, when we have a vector of data or say we have the data in an array, we do the computation manually or with the help of For loop. However, this is not an optimized way of coding and sometimes can be difficult to read
- These are the methods which are without vectorization
---
- When we talk about vectorization, it computes a dot product of the vectors in a single line of code with a numpy function dot
- Vectorization makes the code shorter and makes it run faster which is beneficial while computing
![image](https://github.com/user-attachments/assets/761870c4-8aac-4345-b6b2-11219de12706)

- Above is the representation of what I am trying to explain

---

## Vectorization - Part 2

- Without Vecotrization, when we need to calculate gradient descent or any computing, the machine performs first index calculation then second then third as in it goes from one index to another
- With Vectorization, the computer gets the multiple values of w and x and in a single step, it would multiply all the values of w and x at the same time in parallel
- This means that codes with vectorization can perform calculations in much less time than codes without vectorization.
- That's why being able to vectorize implementations of learning algorithms, has been a key step to getting learning algorithms to run efficiently, and therefore scale well to large datasets that many modern machine learning algorithms now have to operate on
![image](https://github.com/user-attachments/assets/2055153d-e231-4e04-bde2-16e774a35a2b)

- This is another example of why vectorization is better than other conventional methods

![image](https://github.com/user-attachments/assets/d8c39c22-3948-4ab4-a564-57a293cca05e)

---

## Optional lab: Python, NumPy and vectorization

- The first thing that we do is import the numpy library along with time
![image](https://github.com/user-attachments/assets/198fe5ca-2071-445a-8779-1b92c15c7936)

- NumPy Documentation including a basic introduction: NumPy.org
- A challenging feature topic: https://numpy.org/doc/stable/user/basics.broadcasting.html

- Python is the programming language we will be using in this course. It has a set of numeric data types and arithmetic operations. NumPy is a library that extends the base capabilities of python to add a richer data set including more numeric types, vectors, matrices, and many matrix functions. NumPy and python work together fairly seamlessly. Python arithmetic operators work on NumPy data types and many NumPy functions will accept python data types.

![image](https://github.com/user-attachments/assets/ce766f6e-268c-46ab-b0a8-b44b80dcd8b3)

```
# NumPy routines which allocate memory and fill arrays with value
a = np.zeros(4);                print(f"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}")
a = np.zeros((4,));             print(f"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}")
a = np.random.random_sample(4); print(f"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}")
```
![image](https://github.com/user-attachments/assets/3df98687-36a6-421e-96ee-cf5eb323b19d)

- We see the three different methods through which we can specify an array
```
# NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument
a = np.arange(4.);              print(f"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}")
a = np.random.rand(4);          print(f"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}")

# NumPy routines which allocate memory and fill with user specified values
a = np.array([5,4,3,2]);  print(f"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}")
a = np.array([5.,4,3,2]); print(f"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}")
```

- These have all created a one-dimensional vector a with four elements. a.shape returns the dimensions. Here we see a.shape = (4,) indicating a 1-d array with 4 elements.

```
#vector indexing operations on 1-D vectors
a = np.arange(10)
print(a)

#access an element
print(f"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar")

# access the last element, negative indexes count from the end
print(f"a[-1] = {a[-1]}")

#indexs must be within the range of the vector or they will produce and error
try:
    c = a[10]
except Exception as e:
    print("The error message you'll see is:")
    print(e)
```
- A code to understand indexing

![image](https://github.com/user-attachments/assets/9908ed14-d334-40d7-85eb-6a1a1d5d4c7d)

- A code to understand Slicing
```
#vector slicing operations
a = np.arange(10)
print(f"a         = {a}")

#access 5 consecutive elements (start:stop:step)
c = a[2:7:1];     print("a[2:7:1] = ", c)

# access 3 elements separated by two 
c = a[2:7:2];     print("a[2:7:2] = ", c)

# access all elements index 3 and above
c = a[3:];        print("a[3:]    = ", c)

# access all elements below index 3
c = a[:3];        print("a[:3]    = ", c)

# access all elements
c = a[:];         print("a[:]     = ", c)
```

![image](https://github.com/user-attachments/assets/72ee8093-e636-41ec-94cf-839ae30c2637)
---
![image](https://github.com/user-attachments/assets/67686e8d-b892-4c86-98cd-8285403365d0)
---

![image](https://github.com/user-attachments/assets/4c6d78f9-d600-4b04-8ffe-2d44b63d9fcc)
---
![image](https://github.com/user-attachments/assets/689b9720-2837-4a80-9f44-4535530dd865)
---
![image](https://github.com/user-attachments/assets/13712e66-327a-4563-a88a-5603f2a246c6)
---
- The dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same.
- Using a for loop, implement a function which returns the dot product of two vectors. The function to return given inputs  ùëé and  ùëè </br>
$x = \sum_{i=0}^{n-1} a_i b_i$

```
def my_dot(a, b): 
    """
   Compute the dot product of two vectors
 
    Args:
      a (ndarray (n,)):  input vector 
      b (ndarray (n,)):  input vector with same dimension as a
    
    Returns:
      x (scalar): 
    """
    x=0
    for i in range(a.shape[0]):
        x = x + a[i] * b[i]
    return x
```
- A basic dot function

```
# test 1-D
a = np.array([1, 2, 3, 4])
b = np.array([-1, 4, 3, 2])
print(f"my_dot(a, b) = {my_dot(a, b)}")
```

- The dot product is expected to return a scalar value

```
# test 1-D
a = np.array([1, 2, 3, 4])
b = np.array([-1, 4, 3, 2])
c = np.dot(a, b)
print(f"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} ") 
c = np.dot(b, a)
print(f"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} ")

Output-
NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () 
NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () 
```

- The shape function of the dot product is 0

```
np.random.seed(1)
a = np.random.rand(10000000)  # very large arrays
b = np.random.rand(10000000)

tic = time.time()  # capture start time
c = np.dot(a, b)
toc = time.time()  # capture end time

print(f"np.dot(a, b) =  {c:.4f}")
print(f"Vectorized version duration: {1000*(toc-tic):.4f} ms ")

tic = time.time()  # capture start time
c = my_dot(a,b)
toc = time.time()  # capture end time

print(f"my_dot(a, b) =  {c:.4f}")
print(f"loop version duration: {1000*(toc-tic):.4f} ms ")

del(a);del(b)  #remove these big arrays from memory

Output-
np.dot(a, b) =  2501072.5817
Vectorized version duration: 193.5370 ms 
my_dot(a, b) =  2501072.5817
loop version duration: 12025.8312 ms 
```

- This is a code which tells us the time taken by the dot product and the time taken by the function we just created and saw and dot product function is much faster
- So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large.

![image](https://github.com/user-attachments/assets/d547ee49-fc83-4332-945d-e16f9f322e27)

---
![image](https://github.com/user-attachments/assets/d396fe88-7314-42f1-8b11-ad945187d57b)

![image](https://github.com/user-attachments/assets/3717aa1e-21b4-4133-8fb4-326b90412b48)

---
![image](https://github.com/user-attachments/assets/63b9ea00-c602-4073-ad5a-68e1114d7606)

- Operations on matrices

```
#vector indexing operations on matrices
a = np.arange(6).reshape(-1, 2)   #reshape is a convenient way to create matrices
print(f"a.shape: {a.shape}, \na= {a}")

#access an element
print(f"\na[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\n")

#access a row
print(f"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}")

Output-
a.shape: (3, 2), 
a= [[0 1]
 [2 3]
 [4 5]]

a[2,0].shape:   (), a[2,0] = 4,     type(a[2,0]) = <class 'numpy.int64'> Accessing an element returns a scalar

a[2].shape:   (2,), a[2]   = [4 5], type(a[2])   = <class 'numpy.ndarray'>
```

![image](https://github.com/user-attachments/assets/7b348dff-bf2d-461b-aed0-a201062aa296)

- It is worth drawing attention to the last example. Accessing a matrix by just specifying the row will return a 1-D vector.
![image](https://github.com/user-attachments/assets/dd5a0623-1da4-4174-b554-c0292210b092)

![image](https://github.com/user-attachments/assets/d8c75515-7b52-47ad-9d2b-41744090563c)
- Output -
![image](https://github.com/user-attachments/assets/c2b5f576-fe04-4f97-9ca4-a92a2742a835)
