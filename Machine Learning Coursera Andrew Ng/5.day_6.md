## Visualizing the Cost Function

- The model's parameters w and b, the cost function J of w and b, as well as the goal of the linear regression, which is to minimize the cost function J of w and b over parameters w and b. In the last video, we had temporarily set b to zero in order to simplify the visualizations.
- Here we have a training set of house sizes and prices. I've set w to 0.06 and b to 50. f of x is 0.06 times x plus 50. We can see that the model seems to consistently underestimate housing prices.
- When we had only w, because we temporarily set b to zero to simplify things, but then we had come up with a plot of the cost function that look like a U shaped bowl as a function of w only.
- When we had only one parameter, w, the cost function had this U-shaped curve, shaped a bit like a soup bowl.

![image](https://github.com/user-attachments/assets/2a5ed1ae-1358-475a-8f93-9b3a6a7b649c)
---

- In this housing price example, we have two parameters, w and b. The plots becomes a little more complex. It turns that the cost function also has a similar shape like a soup bowl, except in three dimensions instead of two.

![image](https://github.com/user-attachments/assets/e3392bde-64d4-41b3-8330-0d6fd329ec84)

- We see here a 3D surface plot where the axes are labeled w and b.
- As we vary w and b, which are the two parameters of the model, you get different values for the cost function J of w, and b.
- This is a lot like the U-shaped curve that we saw earlier, except instead of having one parameter w as input for the j, we now have two parameters, w and b as inputs.
- For example, if w was minus 10 and b was minus 15, then the height of the surface above this point is the value of j when w is minus 10 and b is minus 15
- In order to look more closely to specific points, there's another way of plotting the cost function J that would be useful for the visualization, which is rather than using these 3D-surface plots, we take this exact same function J and plotting it using a contour plot.

![image](https://github.com/user-attachments/assets/397dcac9-67a7-44af-80e1-0dc37dd84054)

- This is one of the example of the contour plot where we are looking at Mount Fuji from the top

![image](https://github.com/user-attachments/assets/ec10ae8c-1c06-40ef-99e9-9caae46b63db)

- Here at the bottom, we can see a 3D-surface plot of the cost function J.
- On the top right corner is a contour plot of this exact same cost function as that shown at the bottom. The two axes on this contour plots are b, on the vertical axis, and w on the horizontal axis.
- Each of these ovals, also called ellipses, show, is the center points on the 3D surface which are at the exact same height. In other words, the set of points which have the same value for the cost function J.
- If you take the 3 given points shown in the figure, all of these points have the same value of the cost function J, even though they have different values for w and b.
- In the upper left, we also see that these three points correspond to different functions, f, all three of which are actually pretty bad for predicting houses in this case.
- The bottom of the bowl, where the cost function J is at a minimum, is this point right here, at the center of this concentric ovals.

![image](https://github.com/user-attachments/assets/069daa8d-ffe0-4f1c-9cac-126693197570)

 ---
 ---
 
## Visualization Examples

![image](https://github.com/user-attachments/assets/5c0031a6-6726-4a2c-9825-c0ead837f923)

- For this point, w equals about negative 0.15 and b equals about 800. This point corresponds to one pair of values for w and b that use a particular cost j.
- In fact, this booklet pair of values for w and b corresponds to this function f of x, which is this line you can see on the left. This line intersects the vertical axis at 800 because b equals 800 and the slope of the line is negative 0.15, because w equals negative 0.15
- For this function f of x, with these values of w and b, many of the predictions for the value of y are quite far from the actual target value of y that is in the training data
- Since, this line is not a good fit, if we look at the graph of j, the cost of this line is out here, which is pretty far from the minimum. There's a pretty high cost because this choice of w and b is just not that good a fit to the training set.
  
- Another Example
![image](https://github.com/user-attachments/assets/06dff6cc-d6ab-4553-9286-29205c04c867)

- The value of w is equal to 0 and the value b is about 360. This pair of parameters corresponds to this function, which is a flat line, because f of x equals 0 times x plus 360

- Another Example
![image](https://github.com/user-attachments/assets/4d5eb666-fd35-4e80-b633-d72d30b17888)

- The minimum is at the center of the smallest ellipse. If we look at f of x on the left, this looks like a pretty good fit to the training set. We can see on the right, that this point representing the cost is very close to the center of the smaller ellipse. It is not exactly the minimum, but it's pretty close.
- For these values of w and b, you get to this line, f of x. We can see that if we measure the vertical distance between the data points and the predicted values on the straight line, we would get the error for each data point.
- The sum of squared errors for all of these data points is pretty close to the minimum possible sum of squared errors among all possible straight line fits

---

- In linear regression, rather than having to manually try to read a contour plot for the best value for w and b, which isn't really a good procedure and also won't work once we get to more complex machine learning models
- We really want an efficient algorithm that we can write in code for automatically finding the values of parameters w and b which give you the best fit line. That minimizes the cost function J.
- Gradient Descent is the algorithm which is used to do this. This is one of the most important algorithm in machine learning. Gradient descent and variations on gradient descent are used to train, not just linear regression, but some of the biggest and most complex models in all of AI.

---


