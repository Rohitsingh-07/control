## Visualizing the Cost Function

- The model's parameters w and b, the cost function J of w and b, as well as the goal of the linear regression, which is to minimize the cost function J of w and b over parameters w and b. In the last video, we had temporarily set b to zero in order to simplify the visualizations.
- Here we have a training set of house sizes and prices. I've set w to 0.06 and b to 50. f of x is 0.06 times x plus 50. We can see that the model seems to consistently underestimate housing prices.
- When we had only w, because we temporarily set b to zero to simplify things, but then we had come up with a plot of the cost function that look like a U shaped bowl as a function of w only.
- When we had only one parameter, w, the cost function had this U-shaped curve, shaped a bit like a soup bowl.

![image](https://github.com/user-attachments/assets/2a5ed1ae-1358-475a-8f93-9b3a6a7b649c)
---

- In this housing price example, we have two parameters, w and b. The plots becomes a little more complex. It turns that the cost function also has a similar shape like a soup bowl, except in three dimensions instead of two.

![image](https://github.com/user-attachments/assets/e3392bde-64d4-41b3-8330-0d6fd329ec84)

- We see here a 3D surface plot where the axes are labeled w and b.
- As we vary w and b, which are the two parameters of the model, you get different values for the cost function J of w, and b.
- This is a lot like the U-shaped curve that we saw earlier, except instead of having one parameter w as input for the j, we now have two parameters, w and b as inputs.
- For example, if w was minus 10 and b was minus 15, then the height of the surface above this point is the value of j when w is minus 10 and b is minus 15
- In order to look more closely to specific points, there's another way of plotting the cost function J that would be useful for the visualization, which is rather than using these 3D-surface plots, we take this exact same function J and plotting it using a contour plot.

![image](https://github.com/user-attachments/assets/397dcac9-67a7-44af-80e1-0dc37dd84054)

- This is one of the example of the contour plot where we are looking at Mount Fuji from the top

![image](https://github.com/user-attachments/assets/ec10ae8c-1c06-40ef-99e9-9caae46b63db)

- Here at the bottom, we can see a 3D-surface plot of the cost function J.
- On the top right corner is a contour plot of this exact same cost function as that shown at the bottom. The two axes on this contour plots are b, on the vertical axis, and w on the horizontal axis.
- Each of these ovals, also called ellipses, show, is the center points on the 3D surface which are at the exact same height. In other words, the set of points which have the same value for the cost function J.
- If you take the 3 given points shown in the figure, all of these points have the same value of the cost function J, even though they have different values for w and b.
- In the upper left, we also see that these three points correspond to different functions, f, all three of which are actually pretty bad for predicting houses in this case.
- The bottom of the bowl, where the cost function J is at a minimum, is this point right here, at the center of this concentric ovals.

![image](https://github.com/user-attachments/assets/069daa8d-ffe0-4f1c-9cac-126693197570)

 ---
 ---
 
## Visualization Examples

![image](https://github.com/user-attachments/assets/5c0031a6-6726-4a2c-9825-c0ead837f923)

- For this point, w equals about negative 0.15 and b equals about 800. This point corresponds to one pair of values for w and b that use a particular cost j.
- In fact, this booklet pair of values for w and b corresponds to this function f of x, which is this line you can see on the left. This line intersects the vertical axis at 800 because b equals 800 and the slope of the line is negative 0.15, because w equals negative 0.15
- For this function f of x, with these values of w and b, many of the predictions for the value of y are quite far from the actual target value of y that is in the training data
- Since, this line is not a good fit, if we look at the graph of j, the cost of this line is out here, which is pretty far from the minimum. There's a pretty high cost because this choice of w and b is just not that good a fit to the training set.
  
- Another Example
![image](https://github.com/user-attachments/assets/06dff6cc-d6ab-4553-9286-29205c04c867)

- The value of w is equal to 0 and the value b is about 360. This pair of parameters corresponds to this function, which is a flat line, because f of x equals 0 times x plus 360

- Another Example
![image](https://github.com/user-attachments/assets/4d5eb666-fd35-4e80-b633-d72d30b17888)

- The minimum is at the center of the smallest ellipse. If we look at f of x on the left, this looks like a pretty good fit to the training set. We can see on the right, that this point representing the cost is very close to the center of the smaller ellipse. It is not exactly the minimum, but it's pretty close.
- For these values of w and b, you get to this line, f of x. We can see that if we measure the vertical distance between the data points and the predicted values on the straight line, we would get the error for each data point.
- The sum of squared errors for all of these data points is pretty close to the minimum possible sum of squared errors among all possible straight line fits

---

- In linear regression, rather than having to manually try to read a contour plot for the best value for w and b, which isn't really a good procedure and also won't work once we get to more complex machine learning models
- We really want an efficient algorithm that we can write in code for automatically finding the values of parameters w and b which give you the best fit line. That minimizes the cost function J.
- Gradient Descent is the algorithm which is used to do this. This is one of the most important algorithm in machine learning. Gradient descent and variations on gradient descent are used to train, not just linear regression, but some of the biggest and most complex models in all of AI.

---
---

## Optional Lab - Cost Function

- This optional lab will show you how the cost function is implemented in code. And given a small training set and different choices for the parameters you’ll be able to see how the cost varies depending on how well the model fits the data. 

- In the optional lab, you'll also get to play with an interactive contour plot.  You can use your mouse cursor to click anywhere on the contour plot, and you see the straight line defined by the values you chose, for parameters w and b.

- Finally the optional lab also has a 3d surface plot that you can manually rotate and spin around, using your mouse cursor, to take a better look at what the cost function looks like.

```
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl
plt.style.use('./deeplearning.mplstyle')
```

- %matplotlib widget
- This is a magic command specific to Jupyter Notebooks.
- It enables interactive Matplotlib plots in notebook cells — meaning you can click, zoom, and interact with plots directly.
- Useful for things like dynamic updates or user-driven inputs (e.g., clicking on a graph).
---
- from lab_utils_uni import ...
- You're importing a set of custom functions or visual tools from a Python module called lab_utils_uni.
- Let’s briefly guess what each might do:
---
- Function
- plt_intuition	Possibly shows the intuition behind linear regression or cost functions.
- plt_stationary	Could plot a graph to show stationary points or gradients.
- plt_update_onclick	Likely an interactive plot that updates when you click (e.g., to show how parameters change).
- soup_bowl	Could be a metaphorical term for plotting the cost function surface (like a 3D bowl) often used in gradient descent visualizations.
---
-  plt.style.use('./deeplearning.mplstyle')
-  Applies a custom Matplotlib style sheet for consistent and beautiful plots.
-  deeplearning.mplstyle probably defines color schemes, font sizes, line widths, etc.
-  Located in the same folder as your notebook (./ means current directory).

```
x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)
y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)
```

## Computing Cost
The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.

The equation for cost with one variable is:
  $$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}$$ 
 
where 
  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}$$
  
- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.  
- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   
- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.  
>Note, in lecture summation ranges are typically from 1 to m, while code will be from 0 to m-1.

The code below calculates cost by looping over each example. In each loop:
- `f_wb`, a prediction is calculated
- the difference between the target and the prediction is calculated and squared.
- this is added to the total cost.

```
def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    m = x.shape[0] 
    
    cost_sum = 0 
    for i in range(m): 
        f_wb = w * x[i] + b   
        cost = (f_wb - y[i]) ** 2  
        cost_sum = cost_sum + cost  
    total_cost = (1 / (2 * m)) * cost_sum  

    return total_cost
```

- Pretty self explanatory code

![image](https://github.com/user-attachments/assets/b5197e0a-09d8-47f1-b878-c311c73979f7)

```
plt_intuition(x_train,y_train)
```

- This line of code is calling a custom visualization function which is using a slider to change values of w and the plots change as per that

![image](https://github.com/user-attachments/assets/fc7de42b-5628-4300-be24-04eaa05556a6)

- The plot contains a few points that are worth mentioning.
- cost is minimized when $w = 200$, which matches results from the previous lab
- Because the difference between the target and pediction is squared in the cost equation, the cost increases rapidly when $w$ is either too large or too small.
- Using the `w` and `b` selected by minimizing cost results in a line which is a perfect fit to the data.

#### Cost Function Visualization- 3D

- You can see how cost varies with respect to *both* `w` and `b` by plotting in 3D or using a contour plot.   
- It is worth noting that some of the plotting in this course can become quite involved. The plotting routines are provided and while it can be instructive to read through the code to become familiar with the methods, it is not needed to complete the course successfully. The routines are in lab_utils_uni.py in the local directory.
---

#### We try with a larger dataset

```
x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])
y_train = np.array([250, 300, 480,  430,   630, 730,])
```

```
plt.close('all') 
fig, ax, dyn_items = plt_stationary(x_train, y_train)
updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)
```

- Here we build an interactive Contour plot which would change as we select a given co ordinates and will show us the visualization for that
```
plt.close('all') 

Closes any existing Matplotlib figures/windows.
Keeps the notebook or plotting area clean before opening a new interactive plot.
```

```
fig, ax, dyn_items = plt_stationary(x_train, y_train)

plt_stationary is a custom function (from lab_utils_uni.py) that likely:
Initializes a Matplotlib figure (fig) and axes (ax).
Plots the data points (x_train, y_train).
Possibly shows a "stationary" or base version of a cost function landscape or line fit.
Returns dyn_items, which are likely interactive elements — such as the line, text labels, or cost markers — that can be dynamically updated later.
```

```
updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)

This sets up an interactive behavior where clicking on the plot updates something.
Likely usage:
You click somewhere in the plot. It interprets that click as a new slope (w) or intercept (b).
Then it updates the regression line, recalculates the cost, and updates the plot in real time. The function plt_update_onclick() probably: Registers a click handler to the figure.
Modifies dyn_items each time based on user input.
```

![image](https://github.com/user-attachments/assets/3aaa104d-f6bf-413f-8688-4fa1c0c8a1e4)

- Above, note the dashed lines in the left plot. These represent the portion of the cost contributed by each example in your training set. In this case, values of approximately $w=209$ and $b=2.4$ provide low cost. Note that, because our training examples are not on a line, the minimum cost is not zero.

---

- Convex Cost surface
- The fact that the cost function squares the loss ensures that the 'error surface' is convex like a soup bowl. It will always have a minimum that can be reached by following the gradient in all dimensions. In the previous plot, because the $w$ and $b$ dimensions scale differently, this is not easy to recognize. The following plot, where $w$ and $b$ are symmetric, was shown in lecture:

```soup_bowl()```

![image](https://github.com/user-attachments/assets/f46a396e-7aa7-47b1-bcb2-8e9e568ccb83)
