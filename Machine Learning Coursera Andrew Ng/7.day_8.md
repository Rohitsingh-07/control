## Gradient Descent Intuition

$$
\omega = \omega - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$

- Above is the gradient descent algortihm
- The learning rate alpha controls how big of a step we take when updating the model's parameters, w and b.
- The term d/dw is a partial derivative term that is shown above with a curvy font.
- We are going to look at what these two terms, alpha and derviative term are doing which updates the parameters w and b.
- To simplify, let's work on minimizing just one parameter.
---

- Let's initialize gradient descent with some starting value of w.
- A way to thin
