## Gradient Descent Intuition

$$
\omega = \omega - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$

- Above is the gradient descent algortihm
- The learning rate alpha controls how big of a step we take when updating the model's parameters, w and b.
- The term d/dw is a partial derivative term that is shown above with a curvy font.
- We are going to look at what these two terms, alpha and derviative term are doing which updates the parameters w and b.
- To simplify, let's work on minimizing just one parameter.
---

- Let's initialize gradient descent with some starting value of w.
- A way to think about the derivative term at this point on the line is to draw a tangent line, which is a straight line that touches the curve at that given initial point.
- The slope of the line is the derivative of the function j at this point. If we compute the height divided by the width of the triangle, that is the slope of the line
- When the tangent line is pointing up to the right, that says that the slope is positive. which says the derivative is a positive term so is greater than 0
- When we subtract a positive term from the w it will shift towards the left or the position where the cost function J(w) is minimum

![image](https://github.com/user-attachments/assets/56074c10-4789-4f81-b0b6-1618517f16dc)

- The same thing would happen if we choose the w at the position where the slope is negative, in that case the weight would move towards the right again making cost function J(w) minimum

![image](https://github.com/user-attachments/assets/7df56974-84c8-43b8-b383-727579f6197f)

---


