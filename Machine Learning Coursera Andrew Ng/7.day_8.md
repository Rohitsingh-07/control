## Gradient Descent Intuition.

$$
\omega = \omega - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$

- Above is the gradient descent algortihm
- The learning rate alpha controls how big of a step we take when updating the model's parameters, w and b.
- The term d/dw is a partial derivative term that is shown above with a curvy font.
- We are going to look at what these two terms, alpha and derviative term are doing which updates the parameters w and b.
- To simplify, let's work on minimizing just one parameter.
---

- Let's initialize gradient descent with some starting value of w.
- A way to think about the derivative term at this point on the line is to draw a tangent line, which is a straight line that touches the curve at that given initial point.
- The slope of the line is the derivative of the function j at this point. If we compute the height divided by the width of the triangle, that is the slope of the line
- When the tangent line is pointing up to the right, that says that the slope is positive. which says the derivative is a positive term so is greater than 0
- When we subtract a positive term from the w it will shift towards the left or the position where the cost function J(w) is minimum

![image](https://github.com/user-attachments/assets/56074c10-4789-4f81-b0b6-1618517f16dc)

- The same thing would happen if we choose the w at the position where the slope is negative, in that case the weight would move towards the right again making cost function J(w) minimum

![image](https://github.com/user-attachments/assets/7df56974-84c8-43b8-b383-727579f6197f)

---

## Learning Rate

- The choice of the learning rate, alpha will have a huge impact on the efficiency of the implementation of gradient descent.
- If alpha, the learning rate is chosen poorly rate of descent may not even work at all.
- W is updated to be W minus the learning rate, alpha times the derivative term. If the learning rate is too small, we multiply the learning rate alpha with a really small number that we got from the derivative term
- Let's say we multiply alpha with a number which is approximately equal to 0.000001 and so we end up taking a very small step towards the local minimum. And it would just keep going where we take small steps continuosly to reach the local minimum
- The outcome of this process is that we do end up decreasing the cost J but incredibly slowly. We would need a lot of steps to get to the minimum. So, in this case, the gradient descent will work but it will be slow.
- The graph below is a representation of a slow learning rate
![image](https://github.com/user-attachments/assets/a4a46ff6-de6a-4556-b1f6-f55894f26961)
---

- Now, let's take a look where the learning rate is too large. If the learning rate is too large, it would move from the initial point towards another point and it would take a big step and in this case the cost instead of decreasing has now increased
- If the learning rate is too big. Then we take another huge step with an acceleration and way overshoot the minimum again. This process keeps on happening when the learning rate is too large and the gradient descent may overshoot and may never reach the minimum. And another way to say that is that the gradient descent may fail to converge and may even diverge.
- Below is the representation of the given process

![image](https://github.com/user-attachments/assets/a9fe83f2-4f0a-495d-a8b8-84b276053845)

---

- In case of a function which is not a square error cost funtion and it has two local minima. Let's suppose, our parameter W is at a point where there is no slope or say it is at one of the local minima. In this case since we don't have any slope, the derviative term turns to 0 and because of which the W doesn't get updated. So this means, that if we are already at a local minimum, gradient descent leaves W unchanged. Because it just updates the new value of w to be the exact same old value of W.
- So if your parameters have already brought you to a local minimum, then further gradient descent steps to absolutely nothing. It doesn't change the parameters which is what you want because it keeps the solution at that local minimum

![image](https://github.com/user-attachments/assets/5d44fda7-056b-40a5-b67a-994039eee30f)
---

- When we intialize a gradient descent, the change in the derivative term would decrease/ increase the position of W and we would reach the local minima with a fixed learning rate too.

![image](https://github.com/user-attachments/assets/63c168b5-6774-4279-90c1-d27d072b023a)

- So, that's the gradient descent algorithm, we can use to try to miimize the cost function J. Not just the mean squarred error cost function.

---

## Gradient Descent for Linear Regression

- We took a look at the linear regression model and then the cost function, and then the gradient descent algorithm.
- Now, we are going to pull out together and use the squared error cost function for the linear regression model with gradient descent. This will allow us to train the linear regression model to fit a straight line to achieve the training data

![image](https://github.com/user-attachments/assets/15ae5da2-c1bf-4ffa-a585-0e703eb82e4c)

- These are the formulas that we are going to refer which contains the linear regression model, the mean squared error cost function and the gradient descent algorithm.

---
![image](https://github.com/user-attachments/assets/8f674ac9-9f22-4ffd-b62f-2120498c4ca3)

- Above we see the mathematics behind the formulas getting generated

![image](https://github.com/user-attachments/assets/5d247a1d-0965-4775-9060-cccfbf94e048)

- The initial example of a neural network that we saw which had two local minima. However, when we are using a mean squared error cost function with linear regression, the cost function does not and will never have multiple local minima. It has a single global minimum because of this bowl-shape.
- The technical term for this is that this cost function is a convex function. Informally, a convex function is of bowl-shaped function and it cannot have any local minima other than the single global minimum
- When you implement gradient descent on a convex function, one nice property is that so long as you're learning rate is chosen appropriately, it will always converge to the global minimum

![image](https://github.com/user-attachments/assets/fb608d97-1462-4082-abb0-adff1b9d35c7)

---

## Running gradient descent

![image](https://github.com/user-attachments/assets/cb91f031-ed23-458f-8239-ecd7f306bc78)

- Here's a plot of the model and data on the upper left and a contour plot of the cost function on the upper right and at the bottom is the surface plot of the same cost function. Often w and b will both be initialized to 0, but for this demonstration, lets initialized w = -0.1 and b = 900. So this corresponds to f(x) = -0.1x + 900.
- Now, if we take one step using gradient descent, we end up going from one point of cost function to the other where the cost function gets decreased
- As you take more of these steps, the cost is decreasing at each update. So the parameters w and b are following this trajectory.
- And if you look on the left, you get this corresponding straight line fit that fits the data better and better until we've reached the global minimum.
- The global minimum corresponds to this straight line fit, which is a relatively good fit to the data
- That is gradient descent and we're going to use this to fit a model to the holding data and we can now use this f(x) model to predict the price of your clients house or anyone else's house.
- This gradient descent process is called batch gradient descent. The term batch gradient descent refers to the fact that on every step of gradient descent, we're looking at all the training examples, instead of just a subset of the training data
- So in computing grading descent, when computing derivatives, when computing the sum from i =1 to m. And batch gradient descent is looking at the entire batch of training examples at each update
- And then it turns out that there are other versions of gradient descent that do not look at the entire training set, but instead looks at smaller subsets of the training data at each update step. But we'll use batch gradient descent for linear regression.

![image](https://github.com/user-attachments/assets/b739bb5c-0027-4d3d-9fc2-ad60b5f8f649)


