## Gradient Descent Intuition

$$
\omega = \omega - \alpha \frac{\partial}{\partial \omega} J(\omega, b)
$$

- Above is the gradient descent algortihm
- The learning rate alpha controls how big of a step we take when updating the model's parameters, w and b.
- The term d/dw is a partial derivative term that is shown above with a curvy font.
- We are going to look at what these two terms, alpha and derviative term are doing which updates the parameters w and b.
- To simplify, let's work on minimizing just one parameter.
---

- Let's initialize gradient descent with some starting value of w.
- A way to think about the derivative term at this point on the line is to draw a tangent line, which is a straight line that touches the curve at that given initial point.
- The slope of the line is the derivative of the function j at this point. If we compute the height divided by the width of the triangle, that is the slope of the line
- When the tangent line is pointing up to the right, that says that the slope is positive. which says the derivative is a positive term so is greater than 0
- When we subtract a positive term from the w it will shift towards the left or the position where the cost function J(w) is minimum

![image](https://github.com/user-attachments/assets/56074c10-4789-4f81-b0b6-1618517f16dc)

- The same thing would happen if we choose the w at the position where the slope is negative, in that case the weight would move towards the right again making cost function J(w) minimum

![image](https://github.com/user-attachments/assets/7df56974-84c8-43b8-b383-727579f6197f)

---

## Learning Rate

- The choice of the learning rate, alpha will have a huge impact on the efficiency of the implementation of gradient descent.
- If alpha, the learning rate is chosen poorly rate of descent may not even work at all.
- W is updated to be W minus the learning rate, alpha times the derivative term. If the learning rate is too small, we multiply the learning rate alpha with a really small number that we got from the derivative term
- Let's say we multiply alpha with a number which is approximately equal to 0.000001 and so we end up taking a very small step towards the local minimum. And it would just keep going where we take small steps continuosly to reach the local minimum
- The outcome of this process is that we do end up decreasing the cost J but incredibly slowly. We would need a lot of steps to get to the minimum. So, in this case, the gradient descent will work but it will be slow.
- The graph below is a representation of a slow learning rate
![image](https://github.com/user-attachments/assets/a4a46ff6-de6a-4556-b1f6-f55894f26961)
---

- Now, let's take a look where the learning rate is too large. If the learning rate is too large, it would move from the initial point towards another point and it would take a big step and in this case the cost instead of decreasing has now increased
- If the learning rate is too big. Then we take another huge step with an acceleration and way overshoot the minimum again. This process keeps on happening when the learning rate is too large and the gradient descent may overshoot and may never reach the minimum. And another way to say that is that the gradient descent may fail to converge and may even diverge.
- Below is the representation of the given process

![image](https://github.com/user-attachments/assets/a9fe83f2-4f0a-495d-a8b8-84b276053845)

---

- In case of a function which is not a square error cost funtion and it has two local minima. Let's suppose, our parameter W is at a point where there is no slope or say it is at one of the local minima. In this case since we don't have any slope, the derviative term turns to 0 and because of which the W doesn't get updated. So this means, that if we are already at a local minimum, gradient descent leaves W unchanged. Because it just updates the new value of w to be the exact same old value of W.
- So if your parameters have already brought you to a local minimum, then further gradient descent steps to absolutely nothing. It doesn't change the parameters which is what you want because it keeps the solution at that local minimum

![image](https://github.com/user-attachments/assets/5d44fda7-056b-40a5-b67a-994039eee30f)
---

- When we intialize a gradient descent, the change in the derivative term would decrease/ increase the position of W and we would reach the local minima with a fixed learning rate too.

![image](https://github.com/user-attachments/assets/63c168b5-6774-4279-90c1-d27d072b023a)

- So, that's the gradient descent algorithm, we can use to try to miimize the cost function J. Not just the mean squarred error cost function.


