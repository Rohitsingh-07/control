## Choosing the Learning rate

- The learning rate will run much better with an appropriate choice of learning rate. If it's too small, it will run very slowly and if it is too large, it may not even converge
- If we plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly
- This could mean that there's a bug in the code. Or sometimes it could mean that your learning rate is too large.
- One debugging tip for correct implementation of gradient descent is that with a small enough learning rate, the cost function should decrease on every single iteration.
- At times, it should work that alpha is too small and the cost should decrease with every iteration. However, if that is not working out and the cost increases that means there is a bug in the code

![image](https://github.com/user-attachments/assets/8de651ad-69ff-488b-a486-df9d98637ef1)

- So while running gradient descent, it is best to usually try a range of values for the learning rate Alpha. At start, the learning rate alpha should be kept at 0.001 and then try something which is 10 times large than the previous one and keep going till we find something close.

![image](https://github.com/user-attachments/assets/dd72c7d2-41d8-45ed-944a-5fba6b9bf485)

---

## Optional Lab: Feature scaling and Learning Rate (Multi-variable)

- Goals
  1. Utilize the multiple variables routines developed in the previous lab
  2. run Gradient Descent on a data set with multiple features
  3. explore the impact of the learning rate alpha on gradient descent
  4. improve performance of gradient descent by feature scaling using z-score normalization

```
import numpy as np
import matplotlib.pyplot as plt
from lab_utils_multi import  load_house_data, run_gradient_descent 
from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w
from lab_utils_common import dlc
np.set_printoptions(precision=2)
plt.style.use('./deeplearning.mplstyle')
```
- Similar library import for this lab too

![image](https://github.com/user-attachments/assets/cdafca96-7e77-45a7-a5f9-d7c4d6086d74)

![image](https://github.com/user-attachments/assets/ee01e5a8-5948-46ff-980d-7eb45b5f6627)

```
# load the dataset
X_train, y_train = load_house_data()
X_features = ['size(sqft)','bedrooms','floors','age']
```
- Loading the training and actual values

```
fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X_train[:,i],y_train)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("Price (1000's)")
plt.show()
```
- subplots(1,4) is used to create a 1 row and 4 column grid in the visualization
- figsize=(12, 3) sets the figure width to 12 inches and height to 3 inches.
- sharey=True means all plots will share the same y-axis (price) scale for better visual comparison.
- For each feature we get the y axis same and different x axis which makes 4 blocks of the scatter plots

![image](https://github.com/user-attachments/assets/163e3f0e-f8e6-4208-a579-ec3268e7b8df)

--- 

![image](https://github.com/user-attachments/assets/0a207ff6-a189-48fb-ac13-18c01c1e49a4)

![image](https://github.com/user-attachments/assets/2c6bcad5-0db3-40a7-8b07-133a6ff414d2)

```
#set alpha to 9.9e-7
_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)
```
![image](https://github.com/user-attachments/assets/ddec02b0-febb-4b18-8f34-828dc957551c)

![image](https://github.com/user-attachments/assets/ce6d1a21-0022-406f-960d-ee0ba9c98115)

- We repeat the same step with a slightly smaller alpha
```
#set alpha to 9e-7
_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)
```
![image](https://github.com/user-attachments/assets/72ad0380-7f7e-436e-9538-bde7a23f0fa6)

```
#set alpha to 1e-7
_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)
```
![image](https://github.com/user-attachments/assets/7ca0ff5a-69e8-48df-b806-d3fab7ffcdd3)
---
![image](https://github.com/user-attachments/assets/adae61b7-d48e-4ec4-85c3-89902fd06476)

