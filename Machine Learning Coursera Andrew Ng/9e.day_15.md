## Choosing the Learning rate

- The learning rate will run much better with an appropriate choice of learning rate. If it's too small, it will run very slowly and if it is too large, it may not even converge
- If we plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly
- This could mean that there's a bug in the code. Or sometimes it could mean that your learning rate is too large.
- One debugging tip for correct implementation of gradient descent is that with a small enough learning rate, the cost function should decrease on every single iteration.
- At times, it should work that alpha is too small and the cost should decrease with every iteration. However, if that is not working out and the cost increases that means there is a bug in the code

![image](https://github.com/user-attachments/assets/8de651ad-69ff-488b-a486-df9d98637ef1)

- So while running gradient descent, it is best to usually try a range of values for the learning rate Alpha. At start, the learning rate alpha should be kept at 0.001 and then try something which is 10 times large than the previous one and keep going till we find something close.

![image](https://github.com/user-attachments/assets/dd72c7d2-41d8-45ed-944a-5fba6b9bf485)

