## Choosing the Learning rate

- The learning rate will run much better with an appropriate choice of learning rate. If it's too small, it will run very slowly and if it is too large, it may not even converge
- If we plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is not working properly
- This could mean that there's a bug in the code. Or sometimes it could mean that your learning rate is too large.
- One debugging tip for correct implementation of gradient descent is that with a small enough learning rate, the cost function should decrease on every single iteration.
- At times, it should work that alpha is too small and the cost should decrease with every iteration. However, if that is not working out and the cost increases that means there is a bug in the code

![image](https://github.com/user-attachments/assets/8de651ad-69ff-488b-a486-df9d98637ef1)

- So while running gradient descent, it is best to usually try a range of values for the learning rate Alpha. At start, the learning rate alpha should be kept at 0.001 and then try something which is 10 times large than the previous one and keep going till we find something close.

![image](https://github.com/user-attachments/assets/dd72c7d2-41d8-45ed-944a-5fba6b9bf485)

---

## Optional Lab: Feature scaling and Learning Rate (Multi-variable)

- Goals
  1. Utilize the multiple variables routines developed in the previous lab
  2. run Gradient Descent on a data set with multiple features
  3. explore the impact of the learning rate alpha on gradient descent
  4. improve performance of gradient descent by feature scaling using z-score normalization

```
import numpy as np
import matplotlib.pyplot as plt
from lab_utils_multi import  load_house_data, run_gradient_descent 
from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w
from lab_utils_common import dlc
np.set_printoptions(precision=2)
plt.style.use('./deeplearning.mplstyle')
```
- Similar library import for this lab too

![image](https://github.com/user-attachments/assets/cdafca96-7e77-45a7-a5f9-d7c4d6086d74)

![image](https://github.com/user-attachments/assets/ee01e5a8-5948-46ff-980d-7eb45b5f6627)

```
# load the dataset
X_train, y_train = load_house_data()
X_features = ['size(sqft)','bedrooms','floors','age']
```
- Loading the training and actual values

```
fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X_train[:,i],y_train)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("Price (1000's)")
plt.show()
```
- subplots(1,4) is used to create a 1 row and 4 column grid in the visualization
- figsize=(12, 3) sets the figure width to 12 inches and height to 3 inches.
- sharey=True means all plots will share the same y-axis (price) scale for better visual comparison.
- For each feature we get the y axis same and different x axis which makes 4 blocks of the scatter plots

![image](https://github.com/user-attachments/assets/163e3f0e-f8e6-4208-a579-ec3268e7b8df)

--- 

![image](https://github.com/user-attachments/assets/0a207ff6-a189-48fb-ac13-18c01c1e49a4)

![image](https://github.com/user-attachments/assets/2c6bcad5-0db3-40a7-8b07-133a6ff414d2)

```
#set alpha to 9.9e-7
_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)
```
![image](https://github.com/user-attachments/assets/ddec02b0-febb-4b18-8f34-828dc957551c)

![image](https://github.com/user-attachments/assets/ce6d1a21-0022-406f-960d-ee0ba9c98115)

- We repeat the same step with a slightly smaller alpha
```
#set alpha to 9e-7
_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)
```
![image](https://github.com/user-attachments/assets/72ad0380-7f7e-436e-9538-bde7a23f0fa6)

```
#set alpha to 1e-7
_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)
```
![image](https://github.com/user-attachments/assets/7ca0ff5a-69e8-48df-b806-d3fab7ffcdd3)
---
![image](https://github.com/user-attachments/assets/adae61b7-d48e-4ec4-85c3-89902fd06476)
---
![image](https://github.com/user-attachments/assets/67f4802a-7155-4230-8f9d-5a7b5c39e6e2)

```
def zscore_normalize_features(X):
    """
    computes  X, zcore normalized by column
    
    Args:
      X (ndarray (m,n))     : input data, m examples, n features
      
    Returns:
      X_norm (ndarray (m,n)): input normalized by column
      mu (ndarray (n,))     : mean of each feature
      sigma (ndarray (n,))  : standard deviation of each feature
    """
    # find the mean of each column/feature
    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)
    # find the standard deviation of each column/feature
    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)
    # element-wise, subtract mu for that column from each example, divide by std for that column
    X_norm = (X - mu) / sigma      

    return (X_norm, mu, sigma)
 
#check our work
#from sklearn.preprocessing import scale
#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)
```
- This is the code for z score normalization

```
mu     = np.mean(X_train,axis=0)   
sigma  = np.std(X_train,axis=0) 
X_mean = (X_train - mu)
X_norm = (X_train - mu)/sigma      

fig,ax=plt.subplots(1, 3, figsize=(12, 3))
ax[0].scatter(X_train[:,0], X_train[:,3])
ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);
ax[0].set_title("unnormalized")
ax[0].axis('equal')

ax[1].scatter(X_mean[:,0], X_mean[:,3])
ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);
ax[1].set_title(r"X - $\mu$")
ax[1].axis('equal')

ax[2].scatter(X_norm[:,0], X_norm[:,3])
ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);
ax[2].set_title(r"Z-score normalized")
ax[2].axis('equal')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
fig.suptitle("distribution of features before, during, after normalization")
plt.show()
```
- This code helps in visualizing the data of the training example in unnormalized, during normalization and normalized stage

![image](https://github.com/user-attachments/assets/28696dc4-db20-4c68-bdab-6fee60fafc63)

```
# normalize the original features
X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)
print(f"X_mu = {X_mu}, \nX_sigma = {X_sigma}")
print(f"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}")   
print(f"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}")
```
- Peak to peak range means the difference between the largest value of a given feature and the smallest value. Here, we have 4 columns, hence we see 4 values in the ptp list
![image](https://github.com/user-attachments/assets/1fd292ba-f6c7-457a-9bc4-05810294109e)

```
fig,ax=plt.subplots(1, 4, figsize=(12, 3))
for i in range(len(ax)):
    norm_plot(ax[i],X_train[:,i],)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("count");
fig.suptitle("distribution of features before normalization")
plt.show()
fig,ax=plt.subplots(1,4,figsize=(12,3))
for i in range(len(ax)):
    norm_plot(ax[i],X_norm[:,i],)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("count"); 
fig.suptitle("distribution of features after normalization")

plt.show()
```
- This code is used to present the graphs for each feature before and after the normalization

![image](https://github.com/user-attachments/assets/a7a43273-5f9f-4920-bd21-9a8efdbd0e18)

- Notice, above, the range of the normalized data (x-axis) is centered around zero and roughly +/- 2. Most importantly, the range is similar for each feature.

![image](https://github.com/user-attachments/assets/3f6264a0-2c9b-478b-b63c-7ff8b56af069)

```
#predict target using normalized features
m = X_norm.shape[0]
yp = np.zeros(m)
for i in range(m):
    yp[i] = np.dot(X_norm[i], w_norm) + b_norm

    # plot predictions and targets versus original features    
fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X_train[:,i],y_train, label = 'target')
    ax[i].set_xlabel(X_features[i])
    ax[i].scatter(X_train[:,i],yp,color=dlc["dlorange"], label = 'predict')
ax[0].set_ylabel("Price"); ax[0].legend();
fig.suptitle("target versus prediction using z-score normalized model")
plt.show()
```

- Code we used to check the difference between the actual and predicted values post the z score normalization

![image](https://github.com/user-attachments/assets/819db900-076b-4e56-8dce-78338419ed9f)

```
# First, normalize out example.
x_house = np.array([1200, 3, 1, 40])
x_house_norm = (x_house - X_mu) / X_sigma
print(x_house_norm)
x_house_predict = np.dot(x_house_norm, w_norm) + b_norm
print(f" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}")

Output-

[-0.53  0.43 -0.79  0.06]
 predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709
```
![image](https://github.com/user-attachments/assets/bfdcfd4c-242f-4c15-b8ab-b72822746e93)

![image](https://github.com/user-attachments/assets/b68d8250-6b94-4c0c-82a2-b82000bdea00)

![image](https://github.com/user-attachments/assets/3caaa5b9-3589-4c96-a9a7-02504e519a3b)

- Housing Data Link: https://jse.amstat.org/v19n3/decock.pdf

---

## Feature Engineering

- The choice of features can have a huge impact on your learning algorithm's performance
- In fact, for many practical applications, choosing or entering the right features is a critical step to making the algorithm work well
- Say you have two features for each house. X_1 is the width of the lot size of the plots of land that the house is built on.
- This in real state is also called the frontage of the lot, and the second feature, x_2, is the depth of the lot size of, lets assume the rectangular plot of land that the house was built on.
- Given these two features, x_1 and x_2, you might build a model like this where f of x is w_1x_1 plus w_2x_2 plus b, where x_1 is the frontage or width, and x_2 is the depth. This model might work okay
- But here's another option for how you might choose a different way to use these features in the model that could be even more effective. You might notice that the area of the land can be calculated as the frontage or width times the depth. You may have an intuition that the area of the land is more predictive of the price, than the frontage and depth as separate features
- You might define a new feature, x_3, as x_1 times x_2. This new feature x_3 is equal to the area of the plot of land
- With this feature, you can then have a model f_w, b of x equals w_1x_1 plus w_2x_2 plus w_3x_3 plus b so that the model can now choose parameters w_1, w_2, and w_3, depending on whether the data shows that the frontage or the depth or the area x_3 of the lot turns out to be the most important thing for predicting the price of the house
- Creating a new feature is known as feature engineering in which you might use your knowledge or intuition about the problem to design new features usually by transforming or combining the original features of the problem in order to make it easier for the learning algorithm to make accurate predictions

![image](https://github.com/user-attachments/assets/f7b1b380-7748-4228-9c98-03daca57dab3)

--- 

## Polynomial Regression

- So far we've just been fitting straight lines to our data
- Let's take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called polynomial regression, which will let you fit curves, non-linear functions, to your data.
- Let's say we have the scatter plot of a data where the line is not the best fit for the data. We would need to plot a curve which would be the best fit for the data
- So to get a curve, we can use a quadratic function where we have the feature and the feature squared but the issue with the quadratic function is that its curve comes down over the period as the size would increase which is not appropriate
- We can use the cubic function where we have the size, it squared and size cubed. This function doesn't come down but it keeps going up and might even rise the prices pretty high which again seems inappropriate
- FYI, if we create a feature that are these powers like square of the original features, then feature scaling becomes increasingly important.
- Another reasonable alternative to taking the size squared and size cubed is to say use the square root of x. Your model may look like w_1 times x plus w_2 times the square root of x plus b.
- It becomes a bit less steep as x increases, but it doesn't ever completely flatten out, and it certainly never ever comes back down
- Later in the second course in this specialization, you see how you can choose different features and different models that include or don't include these features, and you have a process for measuring how well these different models perform to help you decide which features to include or not include.
- By using feature engineering and polynomial functions, you can potentially get a much better model for your data
  
![image](https://github.com/user-attachments/assets/f81b198f-f7b2-4cae-9a7d-511120fa41b0)

