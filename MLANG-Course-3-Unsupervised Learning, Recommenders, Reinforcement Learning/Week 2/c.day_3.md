## Collaborative filtering vs Content-based filtering

- In this video, we'll start to develop a second type of recommender system called a content-based filtering algorithm. To get started, let's compare and contrast the collaborative filtering approach that we'll be looking at so far with this new content-based filtering approach. Let's take a look. With collaborative filtering, the general approach is that we would recommend items to you based on ratings of users who gave similar ratings as you. We have some number of users give some ratings for some items, and the algorithm figures out how to use that to recommend new items to you. In contrast, content-based filtering takes a different approach to deciding what to recommend to you. A content-based filtering algorithm will recommend items to you based on the features of users and features of the items to find a good match.
- In other words, it requires having some features of each user, as well as some features of each item and it uses those features to try to decide which items and users might be a good match for each other. With a content-based filtering algorithm, you still have data where users have rated some items. Well, content-based filtering will continue to use r, i, j to denote whether or not user j has rated item i and will continue to use y i, j to denote the rating that user j is given item i if it's defined. But the key to content-based filtering is that we will be able to make good use of features of the user and of the items to find better matches than potentially a pure collaborative filtering approach might be able to.

<img width="1919" height="947" alt="image" src="https://github.com/user-attachments/assets/7691869d-72cd-49c5-864c-0761a8d1a310" />

---

- Let's take a look at how this works. In the case of movie recommendations, here are some examples of features. You may know the age of the user, or you may have the gender of the user. This could be a one-hot feature similar to what you saw when we were talking about decision trees where you could have a one-hot feature with the values based on whether the user's self-identified gender is male or female or unknown, and you may know the country of the user. If there are about 200 countries in the world then also be a one-hot feature with about 200 possible values. You can also look at past behaviors of the user to construct this feature vector. For example, if you look at the top thousand movies in your catalog, you might construct a thousand features that tells you of the thousand most popular movies in the world which of these has the user watch. In fact, you can also take ratings the user might have already given in order to construct new features. It turns out that if you have a set of movies and if you know what genre each movie is in, then the average rating per genre that the user has given. Of all the romance movies that the user has rated, what was the average rating?
- Of all the action movies that the user has rated, what was the average rating? And so on for all the other genres. This too can be a powerful feature to describe the user. One interesting thing about this feature is that it actually depends on the ratings that the user had given. But there's nothing wrong with that. Constructing a feature vector that depends on the user's ratings is a completely fine way to develop a feature vector to describe that user. With features like these you can then come up with a feature vector x subscript u, use as a user superscript j for user j.
- Similarly, you can also come up with a set of features for each movie of each item, such as what was the year of the movie? What's the genre or genres of the movie of known? If there are critic reviews of the movie, you can construct one or multiple features to capture something about what the critics are saying about the movie. Or once again, you can actually take user ratings of the movie to construct a feature of, say, the average rating of this movie. This feature again depends on the ratings that users are given but again, does nothing wrong with that. You can construct a feature for a given movie that depends on the ratings that movie had received, such as the average rating of the movie. Or if you wish, you can also have average rating per country or average rating per user demographic as they want to construct other types of features of the movies as well.
- With this, for each movie, you can then construct a feature vector, which I'm going to denote x subscript m, m stands for movie, and superscript i for movie i. Given features like this, the task is to try to figure out whether a given movie i is going to be good match for user j. Notice that the user features and movie features can be very different in size. For example, maybe the user features could be 1500 numbers and the movie features could be just 50 numbers. That's okay too.

<img width="1919" height="945" alt="image" src="https://github.com/user-attachments/assets/076d95fe-3fc7-447a-af5d-34b5ba10c172" />

- In content-based filtering, we're going to develop an algorithm that learns to match users and movies. Previously, we were predicting the rating of user j on movie i as wj dot products of xi plus bj. In order to develop content-based filtering, I'm going to get rid of bj. It turns out this won't hurt the performance of the content-based filtering at all. Instead of writing wj for a user j and xi for a movie i, I'm instead going to just replace this notation with vj_u. This v here stands for a vector. There'll be a list of numbers computed for user j and the u subscript here stands for user. Instead of xi, I'm going to compute a separate vector subscript m, to stand for the movie and for movie is what a superscript stands for. Vj_u as a vector as a list of numbers computed from the features of user j and vi_m is a list of numbers computed from the features like the ones you saw on the previous slide of movie i.
- If we're able to come up with an appropriate choice of these vectors, vj_u and vi_m, then hopefully the dot product between these two vectors will be a good prediction of the rating that user j gives movie i. Just illustrate what a learning algorithm could come up with. If v, u, that is a user vector, turns out to capture the user's preferences, say is 4.9, 0.1, and so on. Lists of numbers like that. The first number captures how much do they like romance movies. Then the second number captures how much do they like action movies and so on. Then v_m, the movie vector is 4.5, 0.2, and so on and so forth of these numbers capturing how much is this a romance movie, how much is this an action movie, and so on.
- Then the dot product, which multiplies these lists of numbers element-wise and then takes a sum, hopefully, will give a sense of how much this particular user will like this particular movie. The challenges given features of a user, say xj_u, how can we compute this vector vj_u that represents succinctly or compactly the user's preferences? Similarly given features of a movie, how can we compute vi_m? Notice that whereas x_u and x_m could be different in size, one could be very long lists of numbers, one could be much shorter list, v here have to be the same size. Because if you want to take a dot product between v_u and v_m, then both of them have to have the same dimensions such as maybe both of these are say 32 numbers. To summarize, in collaborative filtering, we had number of users give ratings of different items. In contrast, in content-based filtering, we have features of users and features of items and we want to find a way to find good matches between the users and the items.

<img width="1919" height="941" alt="image" src="https://github.com/user-attachments/assets/80606cde-516d-4b94-b969-3816ba362a7b" />

- The way we're going to do so is to compute these vectors, v_u for the users and v_m for the items over the movies, and then take dot products between them to try to find good matches. How do we compute the v_u and v_m? Let's take a look at that in the next video.

###########################################################################################################
###########################################################################################################
###########################################################################################################


## Deep learning for content-based filtering

- A good way to develop a content-based filtering algorithm is to use deep learning. The approach you see in this video is the way that many important commercial state-of-the-art content-based filtering algorithms are built today. Let's take a look. Recall that in our approach, given a feature vector describing a user, such as age and gender, and country, and so on, we have to compute the vector v_u, and similarly, given a vector describing a movie such as year of release, the stars in the movie, and so on, we have to compute a vector v_m. In order to do the former, we're going to use a neural network. The first neural network will be what we'll call the user network. Here's an example of user network, that takes as input the list of features of the user, x_u, so the age, the gender, the country of the user, and so on.
- Then using a few layers, say dense neural network layers, it will output this vector v_u that describes the user. Notice that in this neural network, the output layer has 32 units, and so v_u is actually a list of 32 numbers. Unlike most of the neural networks that we were using earlier, the final layer is not a layer with one unit, it's a layer with 32 units. Similarly, to compute v_m for a movie, we can have a movie network as follows, that takes as input features of the movie and through a few layers of a neural network is outputting v_m, that vector that describes the movie. Finally, we'll predict the rating of this user on that movie as v_ u dot product with v_m. Notice that the user network and the movie network can hypothetically have different numbers of hidden layers and different numbers of units per hidden layer. All the output layer needs to have the same size of the same dimension.

<img width="1919" height="935" alt="image" src="https://github.com/user-attachments/assets/95ed9ddb-c062-4499-a7cf-bc9a999deec0" />

- In the description you've seen so far, we were predicting the 1-5 or 0-5 star movie rating. If we had binary labels, if y was to the user like or favor an item, then you can also modify this algorithm to output. Instead of v_u.v_m, you can apply the sigmoid function to that and use this to predict the probability that's y^i,j is 1. To flesh out this notation, we can also add superscripts i and j here if we want to emphasize that this is the prediction by user j on movie i. I've drawn here the user network and the movie network as two separate neural networks. But it turns out that we can actually draw them together in a single diagram as if it was a single neural network. This is what it looks like.

<img width="1919" height="945" alt="image" src="https://github.com/user-attachments/assets/fa0134d3-17cc-4805-add0-6aa1b6b04f8a" />

- On the upper portion of this diagram, we have the user network which inputs x_u and ends up computing v_u. On the lower portion of this diagram, we have what was the movie network, the input is x_m and ends up computing v_m, and these two vectors are then dot-product together. This dot here represents dot product, and this gives us our prediction. Now, this model has a lot of parameters. Each of these layers of a neural network has a usual set of parameters of the neural network. How do you train all the parameters of both the user network and the movie network? What we're going to do is construct a cost function J, which is going to be very similar to the cost function that you saw in collaborative filtering, which is assuming that you do have some data of some users having rated some movies, we're going to sum over all pairs i and j of where you have labels, where i,j equals 1 of the difference between the prediction.
- That would be v_u^j dot product with v_m^i minus y^ij squared. The way we would train this model is depending on the parameters of the neural network, you end up with different vectors here for the users and for the movies. What we'd like to do is train the parameters of the neural network so that you end up with vectors for the users and for the movies that results in small squared error into predictions you get out here. To be clear, there's no separate training procedure for the user and movie networks. This expression down here, this is the cost function used to train all the parameters of the user and the movie networks. We're going to judge the two networks according to how well v_u and v_m predict y^ij, and with this cost function, we're going to use gradient descent or some other optimization algorithm to tune the parameters of the neural network to cause the cost function J to be as small as possible. If you want to regularize this model, we can also add the usual neural network regularization term to encourage the neural networks to keep the values of their parameters small.
- It turns out, after you've trained this model, you can also use this to find similar items. This is akin to what we have seen with collaborative filtering features, helping you find similar items as well. Let's take a look. V_u^j is a vector of length 32 that describes a user j that have features x_ u^j. Similarly, v^i_m is a vector of length 32 that describes a movie with these features over here. Given a specific movie, what if you want to find other movies similar to it? Well, this vector v^i_m describes the movie i.

<img width="1919" height="944" alt="image" src="https://github.com/user-attachments/assets/6ae756b2-2097-45ad-95d0-287353f785fe" />

- If you want to find other movies similar to it, you can then look for other movies k so that the distance between the vector describing movie k and the vector describing movie i, that the squared distance is small. This expression plays a role similar to what we had previously with collaborative filtering, where we talked about finding a movie with features x^k that was similar to the features x^i. Thus, with this approach, you can also find items similar to a given item. One final note, this can be pre-computed ahead of time. By that I mean, you can run a compute server overnight to go through the list of all your movies and for every movie, find similar movies to it, so that tomorrow, if a user comes to the website and they're browsing a specific movie, you can already have pre-computed to 10 or 20 most similar movies to show to the user at that time. The fact that you can pre-compute ahead of time what's similar to a given movie, will turn out to be important later when we talk about scaling up this approach to a very large catalog of movies. That's how you can use deep learning to build a content-based filtering algorithm.
- You might remember when we were talking about decision trees and the pros and cons of decision trees versus neural networks. I mentioned that one of the benefits of neural networks is that it's easier to take multiple neural networks and put them together to make them work in concert to build a larger system. What you just saw was actually an example of that, where we could take a user network and the movie network and put them together, and then take the inner product of the outputs. This ability to put two neural networks together this how we've managed to come up with a more complex architecture that turns out to be quite powerful. One notes, if you're implementing these algorithms in practice, I find that developers often end up spending a lot of time carefully designing the features needed to feed into these content-based filtering algorithms. If we end up building one of these systems commercially, it may be worth spending some time engineering good features for this application as well. In terms of these applications, one limitation that the algorithm as we've described it is it can be computationally very expensive to run if you have a large catalog of a lot of different movies you may want to recommend. 
In the next video, let's take a look at some of the practical issues and how you can modify this algorithm to make it scale that are working on even very large item catalogs. Let's go see that in the next video. 
