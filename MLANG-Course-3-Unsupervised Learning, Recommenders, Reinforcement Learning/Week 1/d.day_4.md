## Finding Unusual Events

- Anomaly detection algorithms look at an unlabeled dataset of normal events and thereby learns to detect or to raise a red flag for if there is an unusual or an anomalous event
- When a company makes an aircraft engine, you really want that aircraft engine to be reliable and function well because an aircraft engine failure has very negative consequences. So some of my friends were using anomaly detection to check if an aircraft engine after it was manufactured seemed anomalous or if there seemed to be anything wrong with it. Here's the idea, after an aircraft engine rolls off the assembly line, you can compute a number of different features of the aircraft engine.
- So, say feature x1 measures the heat generated by the engine. Feature x2 measures the vibration intensity and so on and so forth for additional features as well. But to simplify the slide a bit, I'm going to use just two features x1 and x2 corresponding to the heat and the vibrations of the engine. Now, it turns out that aircraft engine manufacturers don't make that many bad engines.
- And so the easier type of data to collect would be if you have manufactured m aircraft engines to collect the features x1 and x2 about how these m engines behave and probably most of them are just fine that normal engines rather than ones with a defect or flaw in them. And the anomaly detection problem is, after the learning algorithm has seen these m examples of how aircraft engines typically behave in terms of how much heat is generated and how much they vibrate
-  If a brand new aircraft engine were to roll off the assembly line and it had a new feature vector given by Xtest, we'd like to know does this engine look similar to ones that have been manufactured before?
-  So is this probably okay? Or is there something really weird about this engine which might cause this performance to be suspect, meaning that maybe we should inspect it even more carefully before we let it get shipped out and be installed in an airplane and then hopefully nothing will go wrong with it.
-  Here's how an anomaly detection algorithm works. Let me plot the examples x1 through xm over here via these crosses where each cross each data point in this plot corresponds to a specific engine with a specific amount of heat and specific amount of vibrations. If this new aircraft engine Xtest rolls off the assembly lin,e and if you were to plot these values of x1 and x2 and if it were here, you say, okay, that looks probably okay. Looks very similar to other aircraft engines. Maybe I don't need to worry about this one.

<img width="1893" height="935" alt="image" src="https://github.com/user-attachments/assets/3830efcd-8406-4e77-b139-41c89ac09a20" />

- But if this new aircraft engine has a heat and vibration signature that is say all the way down here, then this data point down here looks very different than ones we saw up on top. And so we will probably say, boy, this looks like an anomaly. This doesn't look like the examples I've seen before, we better inspect this more carefully before we let this engine get installed on an airplane.
- How can you have an algorithm address this problem? The most common way to carry out anomaly detection is through a technique called density estimation. And what that means is, when you're given your training sets of these m examples, the first thing you do is build a model for the probability of x. In other words, the learning algorithm will try to figure out what are the values of the features x1 and x2 that have high probability and what are the values that are less likely or have a lower chance or lower probability of being seen in the data set.
- In this example that we have here, I think it is quite likely to see examples in that little ellipse in the middle, so that region in the middle would have high probability maybe things in this ellipse have a little bit lower probability. Things in this ellipse of this oval have even lower probability and things outside have even lower probability. The details of how you decide from the training set what regions are higher versus lower probability is something we'll see in the next few videos.
- And having modeled or having learned to model for p of x When you are given the new test example Xtest. What you will do is then compute the probability of Xtest. And if it is small or more precisely, if it is less than some small number that I'm going to call epsilon, this is a greek alphabet epsilon. So what you should think of as a small number, which means that p of x is very small or in other words, the specific value of x that you saw for a certain user was very unlikely, relative to other usage that you have seen.

<img width="1891" height="892" alt="image" src="https://github.com/user-attachments/assets/7787777e-3095-4366-ac4c-74b23cacf309" />

- But the p of Xtest is less than some small threshold or some small number epsilon, we will raise a flag to say that this could be an anomaly. So for example, if Xtest was all the way down here, the probability of an example landing all the way out here is actually quite low. And so hopefully p of Xtest for this value of Xtest will be less than epsilon and so we would flag this as an anomaly.
- Whereas in contrast, if p of Xtest is not less than epsilon, if p of Xtest is greater than equal to epsilon, then we will say that it looks okay, this doesn't look like an anomaly. And that response to if you had an example in here say where our model p of x will say that examples near the middle here, they're actually quite high probability. There's a very high chance that the new airplane engine will have features close to these inner ellipses. And so p of Xtest will be large for those examples and we'll say it's okay and it's not an anomaly.

- Anomaly detection is used today in many applications. It is frequently used in fraud detection where for example if you are running a website with many different features. If you compute xi to be the features of user i's activities. And examples of features might include, how often does this user login and how many web pages do they visit? How many transactions are they making or how many posts on the discussion forum are they making to what is their typing speed? How many characters per second do they seem able to type. With data like this you can then model p of x from data to model what is the typical behavior of a given user.

- In the common workflow of fraud detection, you wouldn't automatically turn off an account just because it seemed anomalous. But instead you may ask the security team to take a closer look or put in some additional security checks such as ask the user to verify their identity with a cell phone number or ask them to pass a capture to prove that they're human and so on. But algorithms like this are routinely used today to try to find unusual or maybe slightly suspicious activity. So you can more carefully screen those accounts to make sure there isn't something fraudulent. And this type of fraud detection is used both to find fake accounts and this type of algorithm is also used frequently to try to identify financial fraud such as if there's a very unusual pattern of purchases. Then that may be something well worth a security team taking a more careful look at. Anomaly detection is also frequently used in manufacturing.

- You saw an example on the previous slide with aircraft engine manufacturing. But many manufacturers in multiple continents in many, many factories were routinely use anomaly detection to see if whatever they just manufactured. Anything from an airplane engine to a printed circuit board to a smartphone to a motor, to many, many things to see if you've just manufactured the unit that somehow behaves strangely. Because that may indicate that there's something wrong with your airplane engine or printed circuit boards or what have you that might cause you to want to take a more careful look before you ship that object to the customer. Is also used to monitor computers in clusters and in data centers where if X I are the features of a certain machine I such as if the features captured the memory users, the number of disk accesses per second. CPU load features can also be ratios, such as the ratio of CPU load to network traffic. Then if ever a specific computer behaves very differently than other computers, it might be worth taking a look at that computer to see if something is wrong with it. Such as if it has had a hard disk failure or network card failure or something's wrong with it or if maybe it has been hacked into.

<img width="1919" height="921" alt="image" src="https://github.com/user-attachments/assets/e31fc2db-cdbd-457b-b04d-88250304bb71" />

- Anomaly detection is one of those algorithms that is very widely used even though you don't seem to hear people talk about it that much. I remember the first time I worked on the commercial application of anomaly detection was when I was helping a telco company put in place anomaly detection to see when any one of the cell towers was behaving in an unusual way. Because that probably meant there was something wrong with the cell tower and so they want to get a technician to take so hopefully that helped more people get good cell phone coverage. And I've also used anomaly detection to find fraudulent financial transactions And these days I often use it to help manufacturing companies find anomalous parts that they may have manufactured but should inspect more often. So it is a very useful tool to have in your tool chest. And in the next few videos we'll talk about how you can build and get these algorithms to work for yourself.
- In order to get anonymous detection algorithms to work, we'll need to use a Gaussian distribution to model the data p of x. So, let's go on to the next video to talk about Gaussian distributions.

###########################################################################################################
###########################################################################################################
###########################################################################################################

## Gaussian (normal) distribution

- In order to apply anomaly detection, we're going to need to use the Gaussian distribution, which is also called the normal distribution. When you hear me say either Gaussian distribution or normal distribution, they mean exactly the same thing. If you've heard the bell-shaped distribution, that also refers to the same thing. But if you haven't heard of the bell-shaped distribution, that's fine too. But let's take a look at what is the Gaussian or the normal distribution. Say x is a number, and if x is a random number, sometimes called the random variable, x can take on random values. If the probability of x is given by a Gaussian or normal distribution with mean parameter Mu, and with variance Sigma squared.
- What that means is that the probability of x looks like a curve that goes like this. The center or the middle of the curve is given by the mean Mu, and the standard deviation or the width of this curve is given by that variance parameter Sigma. Technically, Sigma is called the standard deviation and the square of Sigma or Sigma squared is called the variance of the distribution. This curve here shows what is p of x or the probability of x.
- Here's one way to interpret it. It means that if you were to get, say, 100 numbers drawn from this probability distribution, and you were to plot a histogram of these 100 numbers drawn from this distribution, you might get a histogram that looks like this. It looks vaguely bell-shaped. What this curve on the left indicates is not if you have just 100 examples or 1,000 or a million or a billion. But if you had a practically infinite number of examples, and you were to draw a histogram of this practically infinite number of examples with a very fine histogram bin.
- Then you end up with essentially this bell-shaped curve here on the left. The formula for p of x is given by this expression; p of x equals 1 over square root 2 Pi. Pi here is that 3.14159 or it's about 22 over 7. Ratio of a circle's diameter circumference times Sigma times e to the negative x minus Mu, the mean parameter squared divided by 2 Sigma squared. For any given value of Mu and Sigma, if you were to plot this function as a function of x, you get this type of bell-shaped curve that is centered at Mu, and with the width of this bell-shaped curve being determined by the parameter Sigma. Now let's look at a few examples of how changing Mu and Sigma will affect the Gaussian distribution. First, let me set Mu equals 0 and Sigma equals 1.

<img width="1913" height="926" alt="image" src="https://github.com/user-attachments/assets/3e9a7959-f30c-4119-9d73-1dec045d81a6" />

- Here's my plot of the Gaussian distribution with mean 0, Mu equals 0, and standard deviation Sigma equals 1. You notice that this distribution is centered at zero and that is the standard deviation Sigma is equal to 1. Now, let's reduce the standard deviation Sigma to 0.5. If you plot the Gaussian distribution with Mu equals 0 and Sigma equals 0.5, it now it looks like this. Notice that it's still centered at zero because Mu is zero. But it's become a much thinner curve because Sigma is now 0.5. You might recall that Sigma is the standard deviation is 0.5, whereas Sigma squared is also called the variance.
- That's equal to 0.5 squared or 0.25. You may have heard that probabilities always have to sum up to one, so that's why the area under the curve is always equal to one, which is why when the Gaussian distribution becomes skinnier, it has to become taller as well. Let's look at another value of Mu and Sigma. Now, I'm going to increase Sigma to 2, so the standard deviation is 2 and the variance is 4. This now creates a much wider distribution because Sigma here is now much larger, and because it's now a wider distribution is become shorter as well because the area under the curve is still equals 1. Finally, let's try changing the mean parameter Mu, and I'll leave Sigma equals 0.5. In this case, the center of the distribution Mu moves over here to the right.
- But the width of the distribution is the same as the one on top because the standard deviation is 0.5 in both of these cases on the right. This is how different choices of Mu and Sigma affect the Gaussian distribution.

<img width="1888" height="919" alt="image" src="https://github.com/user-attachments/assets/0e731cf9-f99e-494d-8d31-8f1105259dc7" />

- When you're applying this to anomaly detection, here's what you have to do. You are given a dataset of m examples, and here x is just a number. Here, are plots of the training sets with 11 examples. What we have to do is try to estimate what a good choice is for the mean parameter Mu, as well as for the variance parameter Sigma squared. Given a dataset like this, it would seem that a Gaussian distribution maybe looking like that with a center here and a standard deviation like that.
- This might be a pretty good fit to the data. The way you would compute Mu and Sigma squared mathematically is our estimate for Mu will be just the average of all the training examples. It's 1 over m times sum from i equals 1 through m of the values of your training examples. The value we will use to estimate Sigma squared will be the average of the squared difference between two examples, and that Mu that you just estimated here on the left. It turns out that if you implement these two formulas in code with this value for Mu and this value for Sigma squared, then you pretty much get the Gaussian distribution that I hand drew on top. This will give you a choice of Mu and Sigma for a Gaussian distribution so that it looks like the 11 training samples might have been drawn from this Gaussian distribution. If you've taken an advanced statistics class, you may have heard that these formulas for Mu and Sigma squared are technically called the maximum likelihood estimates for Mu and Sigma.
- Some statistics classes will tell you to use the formula 1 over n minus 1 instead of 1 over m. In practice, using 1 over m or 1 over n minus 1 makes very little difference. I always use 1 over m, but just some other properties of dividing by m minus 1 that some statisticians prefer. But if you don't understand what they just said, don't worry about it. All you need to know is that if you set Mu according to this formula and Sigma squared according to this formula, you'd get a pretty good estimate of Mu and Sigma and in particular, you get a Gaussian distribution that will be a possible probability distribution in terms of what's the probability distribution that the training examples had come from. You can probably guess what comes next. If you were to get an example over here, then p of x is pretty high.
- Whereas if you were to get an example, we are here, then p of x is pretty low, which is why we would consider this example, okay, not really anomalous, not a lot like the other ones. Whereas an example we are here to be pretty unusual compared to the examples we've seen, and therefore more anomalous because p of x, which is the height of this curve, is much lower over here on the left compared to this point over here, closer to the middle. Now, we've done this only for when x is a number, as if you had only a single feature for your anomaly detection problem.

<img width="1903" height="942" alt="image" src="https://github.com/user-attachments/assets/44ac7dbc-c442-4bdb-9d8f-2d6d24d529bf" />

- You've now seen how the Gaussian distribution works. If x is a single number, this corresponds to if, say you had just one feature for your anomaly detection problem. But for practical anomaly detection applications, you will have many features, two or three or some even larger number n of features.
- Let's take what you saw for a single Gaussian and use it to build a more sophisticated anomaly detection algorithm. They can handle multiple features. Let's go do that in the next video

###########################################################################################################
###########################################################################################################
###########################################################################################################

## Anomaly detection algorithm

<img width="1908" height="926" alt="image" src="https://github.com/user-attachments/assets/d5e35d5b-4ee1-4194-810d-37b7703d87af" />

- Now that you've seen how the Gaussian or the normal distribution works for a single number, we're ready to build our anomaly detection algorithm. Let's dive in. You have a training set x1 through xm, where here each example x has n features. So, each example x is a vector with n numbers. In the case of the airplane engine example, we had two features corresponding to the heat and the vibrations. And so, each of these Xi's would be a two dimensional vector and n would be equal to 2. But for many practical applications n can be much larger and you might do this with dozens or even hundreds of features.
- Given this training set, what we would like to do is to carry out density estimation and all that means is, we will build a model or estimate the probability for p(x). What's the probability of any given feature vector? And our model for p(x) is going to be as follows, x is a feature vector with values x1, x2 and so on down to xn. And I'm going to model p(x) as the probability of x1, times the probability of x2, times the probability of x3 times the probability of xn, for the n th features in the feature vectors. If you've taken an advanced class in probably in statistics before, you may recognize that this equation corresponds to assuming that the features x1, x2 and so on up to xm are statistically independent. But it turns out this algorithm often works fine even that the features are not actually statistically independent.
- Now, to fill in this equation a little bit more, we are saying that the probability of all the features of this vector features x, is the product of p(x) 1 and p(x2) and so on up through p(xn). And in order to model the probability of x1, say the heat feature in this example we're going to have two parameters, mu 1 and sigma 1 or sigma squared is 1. And what that means is we're going to estimate, the mean of the feature x1 and also the variance of feature x1 and that will be new 1 and sigma 1.
- To model p(x2) x2 is a totally different feature measuring the vibrations of the airplane engine. We're going to have two different parameters, which I'm going to write as mu 2, sigma 2 squared. And it turns out this will correspond to the mean or the average of the vibration feature and the variance of the vibration feature and so on.
- If you have additional features mu 3 sigma 3 squared up through mu n and sigma n squared. In case you're wondering why we multiply probabilities, maybe here's 1 example that could build intuition. Suppose for an aircraft engine there's a 1/10 chance that it is really hot, unusually hot and maybe there is a 1 in 20 chance that it vibrates really hard. Then, what is the chance that it runs really hot and vibrates really hard. We're saying that the chance of that is 1/10 times 1/20 which is 1/200. So it's really unlikely to get an engine that both run really hot and vibrates really hard. It's the product of these two probabilities A somewhat more compact way to write this equation up here, is to say that this is equal to, the product from j =1 through n of p(xj).
- Would parameters mu j and sigma squared j. And this symbol here is a lot like the summation symbol except that whereas the summation symbol corresponds to addition, this symbol here corresponds to multiplying these terms over here for j =1 through n. So let's put it all together to see how you can build an anamoly detection system.

<img width="1904" height="941" alt="image" src="https://github.com/user-attachments/assets/76845b56-d8a8-460d-874c-e1d9c8482172" />

- The first step is to choose features xi that you think might be indicative of anomalous examples. Having come up with the features you want to use, you would then fit the parameters mu 1 through mu n and sigma square 1 through sigma squared n, for the n features in your data set. As you might guess, the parameter mu j will be just the average of xj of the feature j of all the examples in your training set. And sigma square j will be the average of the square difference between the feature and the value mu j, that you just computed.
- And by the way, if you have a vectorized implementation, you can also compute mu as the average of the training examples as follows, we're here, x and mu are both vectors. And so this would be the vectorized way of computing mu 1 through mu and all at the same time. And by estimating these parameters on your unlabeled training set, you've now computed all the parameters of your model. Finally, when you are given a new example, x test or I'm just going to write a new example as x here, what you would do is compute p(x) and see if it's large or small. So p(x) as you saw on the last slide is the product from j = 1 through n of the probability of the individual features. So p(x) j with parameters mu j and single square j. And if you substitute in, the formula for this probability, you end up with this expression 1 over root 2 pi sigma j of e to this expression over here.
- And so xj are the features, this is a j feature of your new example, mu j and sigma j are numbers or parameters you have computed in the previous step. And if you compute out this formula, you get some number for p(x). And the final step is to see a p(x) is less than epsilon. And if it is then you flag that it is an anomaly. 1 intuition behind what this algorithm is doing is that it will tend to flag an example as anomalous if 1 or more of the features are either very large or very small relative to what it has seen in the training set. So for each of the features x j, you're fitting a Gaussian distribution like this. And so if even 1 of the features of the new example was way out here, say, then P f xJ would be very small. And if just 1 of the terms in this product is very small, then this overall product, when you multiply together will tend to be very small and does p(x) will be small.
- And what anomaly detection is doing in this algorithm is a systematic way of quantifying whether or not this new example x has any features that are unusually large or unusually small.

<img width="1919" height="944" alt="image" src="https://github.com/user-attachments/assets/14fb3d58-9c32-4d5d-9c16-43741c83a007" />

- Now, let's take a look at what all this actually means on 1 example, Here's the data set with features x 1 and x 2. And you notice that the features x 1 take on a much larger range of values than the features x 2. If you were to compute the mean of the Features x 1, you end up with five, which is why you want is equal to 1. And it turns out that for this data said, if you compute sigma 1, it will be equal to about 2. And if you were to compute mu to the average of the features on next to the average is three and similarly is variance or standard deviation is much smaller, which is why Sigma 2 is equal to 1. So that corresponds to this Gaussian distribution for x 1 and this Gaussian distribution for x 2.
- If you were to actually multiply p(x) 1 and p(x) 2, then you end up with this three D surface plot for p(x) where any point, the height of this is the product of p(x) 1 times p(x) 2. For the corresponding values of x 1 and x 2. And this signifies that values where p(x) is higher or more likely. So, values near the middle kind of here are more likely. Whereas values far out here, values out here are much less likely. I have much lower chance. Now, let me pick two test examples, the first 1 here, I'm going to write this x test 1 and the second 1 down here as x test 2.
- And let's see which of these 2 examples the algorithm will flag as anomalous. I'm going to pick The Parameter Îµ to be equal to 0.02. And if you were to compute p(x) test 1, it turns out to be about 0.4 and this is much bigger than epsilon. And so the album will say this looks okay, doesn't look like an anomaly. Whereas in contrast, if you were to compute p(x) for this point down here corresponding to x 1 equals about eight and x 2 equals about 0.5. Kind of down here, then p(x) test 2 is 0.0021. So this is much smaller than epsilon.
- And so the album will flag this as a likely anomaly. So, pretty much as you might hope it decides that x test 1 looks pretty normal. Whereas excess to which is much further away than anything you see in the training set looks like it could be an anomaly.
- So you've seen the process of how to build an anomaly detection system. But how do you choose the parameter epsilon? And how do you know if your anomaly detection system is working well in the next video, let's dive a little bit more deeply into the process of developing and evaluating the performance of an anomaly detection system.
